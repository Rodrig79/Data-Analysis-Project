{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y2E1jbS4ZluC",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (0.23.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-learn) (0.16.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-learn) (1.19.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-learn) (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (1.1.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from pandas) (1.19.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (3.3.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib) (1.19.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib) (7.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: seaborn in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (0.10.1)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from seaborn) (1.5.2)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from seaborn) (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from seaborn) (1.19.1)\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from seaborn) (3.3.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from pandas>=0.22.0->seaborn) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from pandas>=0.22.0->seaborn) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=2.1.2->seaborn) (7.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=2.1.2->seaborn) (2020.6.20)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=2.1.2->seaborn) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=2.1.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=0.22.0->seaborn) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: graphviz in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (0.14.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-plot in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (0.3.7)\n",
      "Requirement already satisfied: joblib>=0.10 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-plot) (0.16.0)\n",
      "Requirement already satisfied: scipy>=0.9 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-plot) (1.5.2)\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-plot) (3.3.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-plot) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scipy>=0.9->scikit-plot) (1.19.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=1.4.0->scikit-plot) (2020.6.20)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=1.4.0->scikit-plot) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=1.4.0->scikit-plot) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=1.4.0->scikit-plot) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=1.4.0->scikit-plot) (7.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-learn>=0.18->scikit-plot) (2.1.0)\n",
      "Requirement already satisfied: six in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from cycler>=0.10->matplotlib>=1.4.0->scikit-plot) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: statsmodels in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (0.11.1)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from statsmodels) (1.5.2)\n",
      "Requirement already satisfied: pandas>=0.21 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from statsmodels) (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.14 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from statsmodels) (1.19.1)\n",
      "Requirement already satisfied: patsy>=0.5 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from statsmodels) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from pandas>=0.21->statsmodels) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from pandas>=0.21->statsmodels) (2020.1)\n",
      "Requirement already satisfied: six in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from patsy>=0.5->statsmodels) (1.15.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#If you need to install in modules in jupyter notebook \n",
    "%pip install scikit-learn\n",
    "%pip install pandas\n",
    "%pip install matplotlib \n",
    "%pip install seaborn \n",
    "%pip install graphviz\n",
    "%pip install scikit-plot   \n",
    "%pip install statsmodels   \n",
    "\n",
    "\n",
    "#Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "import datetime # Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import scikitplot as skplt\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.linear_model import LinearRegression\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"decision_trees\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IvsV2RSPapC6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#changes the output for the print statements\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vtW9LfInZ_x2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Pull \n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Rodrig79/Machine-Learning-Data-Analysis-Project/master/rawData/pokemon.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "#Data Cleaning\n",
    "df = df[~df.Name.str.contains('Mega')] #removed pokemon with \"Mega\" in it\n",
    "df = df.drop(columns = [\"Name\",\"Type 1\",\"Type 2\",\"#\",\"Generation\"]) #Removed columns with names\n",
    "\n",
    "#normalization\n",
    "# X = df.values #returns a numpy array\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# x_scaled = min_max_scaler.fit_transform(X)\n",
    "# df=pd.DataFrame(x_scaled, columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Splitting dataset to x and y\n",
    "y = df.Attack\n",
    "X = df[['Defense','HP','Sp. Atk','Sp. Def','Speed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Defense</th>\n",
       "      <th>HP</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>45</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>39</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>80</td>\n",
       "      <td>65</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>121</td>\n",
       "      <td>108</td>\n",
       "      <td>81</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>150</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>170</td>\n",
       "      <td>130</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>130</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>751 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Defense   HP  Sp. Atk  Sp. Def  Speed\n",
       "0    49       45   65       65       45   \n",
       "1    63       60   80       80       60   \n",
       "2    83       80   100      100      80   \n",
       "4    43       39   60       50       65   \n",
       "5    58       58   80       65       80   \n",
       "..   ..       ..   ..       ..       ..   \n",
       "794  121      108  81       95       95   \n",
       "795  150      50   100      150      50   \n",
       "797  60       80   150      130      70   \n",
       "798  60       80   170      130      80   \n",
       "799  120      80   130      90       70   \n",
       "\n",
       "[751 rows x 5 columns]"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiVariate  Regression with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.linear_model import LinearRegression\n",
    " from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-405-e1fc8a893142>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlinreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Wheat dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('Wheat dataset')\n",
    "print('linear model intercept: {}'\n",
    "     .format(linreg.intercept_))\n",
    "print('linear model coeff:\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "# make predictions on the testing set\n",
    "y_pred = linreg.predict(X_test)\n",
    "print('RMSE: {:.3f}'.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlrd in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "# excel_file = 'Folds5x2_pp.xlsx'\n",
    "# data = pd.read_excel(excel_file)\n",
    "# print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      49 \n",
       "1      62 \n",
       "2      82 \n",
       "4      52 \n",
       "5      64 \n",
       "       .. \n",
       "794    100\n",
       "795    100\n",
       "797    110\n",
       "798    160\n",
       "799    110\n",
       "Name: Attack, Length: 751, dtype: int64"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seperating data\n",
    "df = df.dropna() \n",
    "yexample = data.PE\n",
    "Xexample = data[['AT','V','AP','RH']] \n",
    "y = df.Attack\n",
    "X = df[['Defense','HP','Speed']] \n",
    "X.dropna()\n",
    "y.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X, Y, B):\n",
    "    m = len(Y)\n",
    "    J = np.sum((X.dot(B) - Y) ** 2)/(2 * m)\n",
    "    return J\n",
    "\n",
    "def batch_gradient_descent(X, Y, B, alpha, iterations):\n",
    "    cost_history = [0] * iterations\n",
    "    m = len(Y)\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        #print(iteration)\n",
    "        # Hypothesis Values\n",
    "        h = X.dot(B)\n",
    "        # Difference b/w Hypothesis and Actual Y\n",
    "        loss = h - Y\n",
    "        # Gradient Calculation\n",
    "        gradient = X.T.dot(loss) / m\n",
    "        # Changing Values of B using Gradient\n",
    "        B = B - alpha * gradient\n",
    "        # New Cost Value\n",
    "        cost = cost_function(X, Y, B)\n",
    "        cost_history[iteration] = cost \n",
    "    return B, cost_history \n",
    "def pred(x_test, newB):\n",
    "    return x_test.dot(newB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = 600\n",
    "# f = 2\n",
    "# X_train = X[:m,:f]\n",
    "# X_train = np.c_[np.ones(len(X_train),dtype='int64'),X_train]\n",
    "# y_train = y[:m]\n",
    "# X_test = X[m:,:f]\n",
    "# X_test = np.c_[np.ones(len(X_test),dtype='int64'),X_test]\n",
    "# y_test = y[m:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.76812684, -0.91287325, -0.77648137],\n",
       "       [-0.29531579, -0.33436756, -0.23743764],\n",
       "       [ 0.38012857,  0.43697336,  0.48128734],\n",
       "       ...,\n",
       "       [-0.39663244,  0.43697336,  0.12192485],\n",
       "       [-0.39663244,  0.43697336,  0.48128734],\n",
       "       [ 1.62970064,  0.43697336,  0.12192485]])"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Coefficients\n",
    "B = np.zeros(X_train.shape[1])\n",
    "alpha = 0.005\n",
    "iter_ = 2000\n",
    "newB, cost_history = batch_gradient_descent(X_train, y_train, B, alpha, iter_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x_test, newB):\n",
    "    return x_test.dot(newB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = pred(X_test,newB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.571688375255779"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def r2(y_,y):\n",
    "    sst = np.sum((y-y.mean())**2)\n",
    "    ssr = np.sum((y_-y)**2)\n",
    "    r2 = 1-(ssr/sst)\n",
    "    return(r2)\n",
    "#----------------\n",
    "r2(y_,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-23.155180614367016"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_ = pred(X_test[3],newB)\n",
    "ans_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Regression Model\n",
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge regression linear model intercept: 7.009233212820845\n",
      "ridge regression linear model coeff:\n",
      "[ 0.41417311  0.40321748  0.19917968 -0.29035188  0.26331594]\n",
      "R-squared score (training): 0.417\n",
      "R-squared score (test): 0.443\n",
      "Number of non-zero features: 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "linridge = Ridge(alpha=25.0).fit(X_train, y_train)\n",
    "\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test, y_test)))\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-352-d71bc87e92fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX_train_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX_test_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlinridge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRidge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         X = check_array(X, copy=self.copy, dtype=FLOAT_DTYPES,\n\u001b[0m\u001b[1;32m    408\u001b[0m                         force_all_finite=\"allow-nan\")\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m             raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n\u001b[0m\u001b[1;32m    651\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                              % (n_samples, array.shape, ensure_min_samples,\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "# Ridge regression with feature normalization\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linridge = Ridge(alpha=20.0).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Wheat dataset')\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test_scaled, y_test)))\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression: effect of alpha regularization parameter\n",
      "\n",
      "Alpha = 0.00\n",
      "num abs(coeff) > 1.0: 5, r-squared training: 0.42, r-squared test: 0.44\n",
      "\n",
      "Alpha = 1.00\n",
      "num abs(coeff) > 1.0: 5, r-squared training: 0.41, r-squared test: 0.44\n",
      "\n",
      "Alpha = 10.00\n",
      "num abs(coeff) > 1.0: 5, r-squared training: 0.32, r-squared test: 0.33\n",
      "\n",
      "Alpha = 20.00\n",
      "num abs(coeff) > 1.0: 5, r-squared training: 0.26, r-squared test: 0.27\n",
      "\n",
      "Alpha = 50.00\n",
      "num abs(coeff) > 1.0: 5, r-squared training: 0.18, r-squared test: 0.18\n",
      "\n",
      "Alpha = 100.00\n",
      "num abs(coeff) > 1.0: 5, r-squared training: 0.11, r-squared test: 0.11\n",
      "\n",
      "Alpha = 1000.00\n",
      "num abs(coeff) > 1.0: 2, r-squared training: 0.02, r-squared test: 0.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ridge regression with regularization parameter: alpha\n",
    "\n",
    "print('Ridge regression: effect of alpha regularization parameter\\n')\n",
    "for this_alpha in [0, 1, 10, 20, 50, 100, 1000]:\n",
    "    linridge = Ridge(alpha = this_alpha).fit(X_train_scaled, y_train)\n",
    "    r2_train = linridge.score(X_train_scaled, y_train)\n",
    "    r2_test = linridge.score(X_test_scaled, y_test)\n",
    "    num_coeff_bigger = np.sum(abs(linridge.coef_) > 1.0)\n",
    "    print('Alpha = {:.2f}\\nnum abs(coeff) > 1.0: {}, \\\n",
    "r-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n",
    "         .format(this_alpha, num_coeff_bigger, r2_train, r2_test))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso regression linear model intercept: 66.20384241687611\n",
      "lasso regression linear model coeff:\n",
      "[34.28143912]\n",
      "Non-zero features: 1\n",
      "R-squared score (training): 0.089\n",
      "R-squared score (test): 0.084\n",
      "\n",
      "Features with non-zero weight (sorted by absolute magnitude):\n",
      "\tHP, 34.281\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linlasso = Lasso(alpha=1, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('lasso regression linear model intercept: {}'\n",
    "     .format(linlasso.intercept_))\n",
    "print('lasso regression linear model coeff:\\n{}'\n",
    "     .format(linlasso.coef_))\n",
    "print('Non-zero features: {}'\n",
    "     .format(np.sum(linlasso.coef_ != 0)))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linlasso.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}\\n'\n",
    "     .format(linlasso.score(X_test_scaled, y_test)))\n",
    "print('Features with non-zero weight (sorted by absolute magnitude):')\n",
    "\n",
    "for e in sorted (list(zip(list(X), linlasso.coef_)),\n",
    "                key = lambda e: -abs(e[1])):\n",
    "    if e[1] != 0:\n",
    "        print('\\t{}, {:.3f}'.format(e[0], e[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso regression: effect of alpha regularization\n",
      "parameter on number of features kept in final model\n",
      "\n",
      "Alpha = 0.50\n",
      "Features kept: 1, r-squared training: 0.17, r-squared test: 0.14\n",
      "\n",
      "Alpha = 1.00\n",
      "Features kept: 1, r-squared training: 0.09, r-squared test: 0.08\n",
      "\n",
      "Alpha = 2.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n",
      "Alpha = 3.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n",
      "Alpha = 5.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n",
      "Alpha = 10.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n",
      "Alpha = 20.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n",
      "Alpha = 50.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Lasso regression: effect of alpha regularization\\n\\\n",
    "parameter on number of features kept in final model\\n')\n",
    "\n",
    "for alpha in [0.5, 1, 2, 3, 5, 10, 20, 50]:\n",
    "    linlasso = Lasso(alpha, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "    r2_train = linlasso.score(X_train_scaled, y_train)\n",
    "    r2_test = linlasso.score(X_test_scaled, y_test)\n",
    "    \n",
    "    print('Alpha = {:.2f}\\nFeatures kept: {}, r-squared training: {:.2f}, \\\n",
    "r-squared test: {:.2f}\\n'\n",
    "         .format(alpha, np.sum(linlasso.coef_ != 0), r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import KFold # import k-fold validation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLYNOMIAL DEGREE-2 W/ LINEAR REGRESSION\n",
      "(poly deg 2) linear model coeff (w):\n",
      "[ 0.00000000e+00  7.83632395e-01  5.81316973e-01  8.36628758e-02\n",
      " -3.03589224e-01  2.17935028e-01 -2.64092610e-03  4.12878549e-03\n",
      " -2.05705057e-03 -1.36610177e-03 -5.91249232e-06 -3.96493171e-03\n",
      " -2.46874919e-03  2.19326197e-03  4.71140219e-03  5.30018156e-03\n",
      " -4.63437500e-03 -2.32196045e-03  2.37022152e-03 -8.05809773e-04\n",
      " -6.17839096e-04]\n",
      "(poly deg 2) linear model intercept (b): -6.663\n",
      "(poly deg 2) R-squared score (training): 0.514\n",
      "(poly deg 2) R-squared score (test): 0.477\n",
      "(poly deg 2) RMSE score (train): 20.939\n",
      "(poly deg 2) RMSE score (test): 21.262\n",
      "\n",
      "POLYNOMIAL DEGREE-3 W/ LINEAR REGRESSION\n",
      "(poly deg 3) linear model coeff (w):\n",
      "[ 1.09582330e-10  9.63693242e-01 -1.68716025e+00  2.32220783e+00\n",
      " -1.47109972e+00 -7.42478680e-02 -9.34907237e-03  1.87006590e-02\n",
      " -2.78074030e-02  1.70931737e-02  4.16698973e-03  1.27650489e-02\n",
      " -5.55729820e-03  1.19546395e-02  1.94395528e-02 -8.89151315e-03\n",
      " -8.15594151e-04 -5.39787789e-03  2.30581295e-03 -6.37891267e-03\n",
      " -2.27424818e-03  1.09719466e-05 -3.46605342e-05  1.01164047e-04\n",
      "  4.54683442e-07 -2.38523633e-05 -4.23440735e-05  1.91696677e-04\n",
      " -1.53060933e-04 -4.38951706e-05 -2.40531362e-04  1.54108782e-04\n",
      "  2.85242718e-04 -7.13653918e-05 -8.80119423e-05 -7.63471778e-05\n",
      " -4.69359841e-05 -3.44315685e-05  3.61804724e-05 -6.76359005e-05\n",
      "  4.60722453e-05 -2.82363629e-04  1.90551047e-04  1.03285955e-04\n",
      " -9.94710864e-05 -6.23269954e-05  2.38384891e-05  1.66829933e-04\n",
      "  9.79938105e-05  4.91061040e-06 -3.34109916e-04 -1.44307008e-04\n",
      " -2.27800627e-05  9.08953659e-05  2.27827441e-04  1.89342535e-05]\n",
      "(poly deg 3) linear model intercept (b): 19.873\n",
      "(poly deg 3) R-squared score (training): 0.609\n",
      "(poly deg 3) R-squared score (test): -1.122\n",
      "(poly deg 3) RMSE score (train): 18.796\n",
      "(poly deg 3) RMSE score (test): 42.820\n",
      "\n",
      "POLYNOMIAL DEGREE-5 W/ LINEAR REGRESSION\n",
      "(poly deg 5) linear model coeff (w):\n",
      "[ 2.80817175e+03 -7.67431645e+00  3.53508781e+00  2.66288734e+01\n",
      "  3.66818142e+01 -2.00835710e+01 -2.15536924e-02  4.92799198e-01\n",
      "  3.76759674e-01 -5.11396640e-01  2.06149314e-01  1.02829334e-01\n",
      " -2.87443219e-01 -8.66152664e-01  1.06589272e-01 -5.97216748e-01\n",
      " -4.25401258e-01  9.17776323e-02 -3.37565700e-01 -4.58518810e-02\n",
      "  4.39097245e-01  1.63688866e-03 -7.24823623e-03 -1.86092436e-03\n",
      "  4.11333251e-03 -1.92415017e-04 -6.91686293e-03 -5.81807999e-03\n",
      "  1.22808831e-02 -1.65418982e-03  5.52265724e-04 -1.89868912e-03\n",
      " -5.29225647e-03  3.38378940e-03 -7.05089033e-04 -1.05955300e-03\n",
      " -2.03011707e-03 -1.37852717e-03  1.04703567e-02  3.64974667e-03\n",
      "  7.87859937e-03  1.00444448e-02 -8.44189611e-03 -1.01898818e-03\n",
      "  2.35885588e-04 -2.88834126e-04  4.97537647e-03 -6.08704291e-03\n",
      "  6.96644787e-03  9.98325217e-03  5.07435660e-03 -5.44667365e-03\n",
      "  9.43091358e-05  3.35471491e-03 -4.24236557e-03 -2.91354605e-03\n",
      " -1.70644357e-05  5.65695166e-05  4.46820391e-06 -1.32617236e-05\n",
      " -3.40772175e-06  1.01980315e-04 -8.26119227e-05 -7.70069435e-05\n",
      " -3.99653760e-05 -2.20168252e-05  1.24536126e-04  2.45151478e-05\n",
      " -4.31487769e-05 -3.28409601e-05  2.94287818e-05 -2.30506674e-05\n",
      "  2.25280530e-05  7.27764342e-06  3.91652322e-05  2.08142127e-05\n",
      "  1.05757355e-04  1.86024681e-04 -6.22085550e-05 -1.30953816e-04\n",
      "  1.14946773e-05  7.23401556e-05 -1.04375332e-04 -1.36618837e-04\n",
      " -1.39015504e-04  2.27100406e-04 -3.49000532e-05  5.32421571e-05\n",
      " -5.54313627e-05  6.88730153e-05 -1.66297358e-05  9.87042542e-06\n",
      "  5.34633709e-05 -3.37439191e-05  5.64895807e-06 -7.90749837e-05\n",
      "  5.54352155e-06 -3.81092536e-05 -7.59118790e-05 -7.97083346e-05\n",
      " -3.21835388e-05  1.83515800e-07 -5.36969488e-05  1.60032192e-05\n",
      " -1.19269443e-04 -5.77026460e-05  9.19486732e-05  5.12226678e-05\n",
      "  1.93648130e-04 -5.56698530e-05  6.87369038e-06 -3.45548176e-05\n",
      " -3.35646119e-05 -1.66768423e-05  1.89161033e-04  2.65581361e-05\n",
      " -2.83953137e-05 -6.61039284e-05 -2.52372973e-04  9.00796301e-05\n",
      "  2.29027219e-05 -1.27864538e-05  5.31083861e-05 -8.20124689e-05\n",
      "  6.03348378e-05  2.45345520e-06  6.04021153e-08 -2.91984579e-08\n",
      " -1.56104066e-07 -4.55378046e-08  3.49725626e-08 -5.72845449e-07\n",
      "  6.87841821e-07 -1.55999473e-07  1.98899713e-07 -4.35439249e-08\n",
      " -3.56385890e-07  4.19249886e-07  5.11648632e-07 -2.69485630e-07\n",
      " -1.72559155e-07 -7.19265485e-08 -3.35200149e-07  1.08832516e-06\n",
      " -2.45186935e-07  2.78258364e-07 -5.59163676e-07 -2.60742886e-07\n",
      " -2.48189168e-07  2.99646269e-07 -4.45244752e-08  6.90715289e-08\n",
      " -9.67132827e-08  8.97366235e-08  1.08507267e-06 -2.18597733e-06\n",
      "  3.76312288e-07 -8.30386007e-07  1.05540929e-06  4.34158152e-07\n",
      " -1.74188651e-07  3.92904204e-07  5.76573669e-08 -8.14428125e-07\n",
      " -5.29140200e-07  6.96156092e-07 -6.03597586e-07 -8.95918339e-07\n",
      " -6.37654079e-07  2.29582595e-06  2.65212198e-07 -1.51021843e-06\n",
      "  1.40835017e-06  1.17120723e-06 -2.57244917e-07 -1.16584688e-06\n",
      " -1.01530162e-06  9.39564599e-07 -5.79691676e-07 -3.00843558e-07\n",
      "  2.10347609e-07 -5.90901283e-09  1.21548016e-07  2.65694676e-07\n",
      " -4.44313923e-08 -1.05461992e-07 -5.67871368e-08 -5.58576429e-07\n",
      "  1.62310354e-06 -2.27250481e-07  3.61037141e-07  3.74171960e-07\n",
      " -7.77070272e-07  7.04077152e-09 -5.32933750e-07  1.11690268e-07\n",
      " -6.21778611e-08 -2.97650233e-08  2.92635551e-08 -1.25529501e-07\n",
      " -1.75146853e-07 -9.27235618e-07  8.17434434e-07  6.68213747e-07\n",
      "  6.78169158e-07 -2.80449921e-07  6.44585188e-07  4.50012316e-08\n",
      " -1.34283204e-06  1.83208754e-06 -1.47024466e-07  1.01680127e-06\n",
      " -1.61267701e-07 -1.73058216e-06  1.92833836e-07 -6.46719294e-08\n",
      " -2.26684145e-07  6.75944675e-07  5.55630146e-07 -1.90651093e-06\n",
      "  1.01546178e-06 -8.10527448e-07  4.01995205e-07  5.62451142e-07\n",
      " -5.98052346e-07  4.30256719e-10 -3.43544693e-07 -5.52374919e-08\n",
      "  5.87175651e-07  1.04381696e-07 -6.96546381e-08  1.51492376e-07\n",
      " -3.23784901e-08 -2.04684400e-07 -9.71172218e-08  4.51696112e-08\n",
      "  1.79548483e-07  1.31667855e-07 -6.87250472e-07  5.64855780e-08\n",
      "  1.73708856e-07  1.28824384e-07  4.93795492e-07  5.16889389e-07\n",
      " -5.38109483e-07 -1.05741331e-07 -3.31211627e-08 -5.75150505e-09\n",
      " -2.02596021e-07  2.33843242e-07 -9.04279784e-08  2.34385968e-08]\n",
      "(poly deg 5) linear model intercept (b): -3229.763\n",
      "(poly deg 5) R-squared score (training): 0.792\n",
      "(poly deg 5) R-squared score (test): -2495.770\n",
      "(poly deg 5) RMSE score (train): 13.716\n",
      "(poly deg 5) RMSE score (test): 1468.935\n",
      "\n",
      "POLYNOMIAL DEGREE-10 W/ LINEAR REGRESSION\n",
      "(poly deg 10) linear model coeff (w):\n",
      "[ 2.49200575e-12 -1.24200888e-11  1.24983611e-11 ...  4.48849851e-14\n",
      " -1.30415844e-13 -1.00614254e-13]\n",
      "(poly deg 10) linear model intercept (b): 23.903\n",
      "(poly deg 10) R-squared score (training): 1.000\n",
      "(poly deg 10) R-squared score (test): -12398793083561872.000\n",
      "(poly deg 10) RMSE score (train): 0.020\n",
      "(poly deg 10) RMSE score (test): 3273429738.344\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for degree in [2, 3, 5,10]:\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_F1_poly = poly.fit_transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y, random_state=0)\n",
    "    linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "    print('POLYNOMIAL DEGREE-{} W/ LINEAR REGRESSION'.format(degree))\n",
    "    print('(poly deg {}) linear model coeff (w):\\n{}'\n",
    "         .format(degree, linreg.coef_))\n",
    "    print('(poly deg {}) linear model intercept (b): {:.3f}'\n",
    "         .format(degree, linreg.intercept_))\n",
    "    print('(poly deg {}) R-squared score (training): {:.3f}'\n",
    "         .format(degree, linreg.score(X_train, y_train)))\n",
    "    print('(poly deg {}) R-squared score (test): {:.3f}'\n",
    "         .format(degree, linreg.score(X_test, y_test)))\n",
    "    print('(poly deg {}) RMSE score (train): {:.3f}'\n",
    "         .format(degree, sqrt(mean_squared_error(y_train, linreg.predict(X_train)))))\n",
    "    print('(poly deg {}) RMSE score (test): {:.3f}\\n'\n",
    "         .format(degree, sqrt(mean_squared_error(y_test, linreg.predict(X_test)))))\n",
    "\n",
    "plot_learning_curves(linreg,  X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLYNOMIAL DEGREE-2 W/ RIDGE REGRESSION\n",
      "(poly deg 2) linear model coeff (w):\n",
      "[ 0.00000000e+00  7.83561256e-01  5.81248959e-01  8.36606865e-02\n",
      " -3.03533540e-01  2.17897801e-01 -2.64075722e-03  4.12917773e-03\n",
      " -2.05708348e-03 -1.36616736e-03 -5.60359635e-06 -3.96475273e-03\n",
      " -2.46871137e-03  2.19313880e-03  4.71163080e-03  5.30016917e-03\n",
      " -4.63433789e-03 -2.32194092e-03  2.37009827e-03 -8.06078647e-04\n",
      " -6.17730005e-04]\n",
      "(poly deg 2) linear model intercept (b): -6.659\n",
      "(poly deg 2) R-squared score (training): 0.514\n",
      "(poly deg 2) R-squared score (test): 0.477\n",
      "(poly deg 2) RMSE score (train): 20.939\n",
      "(poly deg 2) RMSE score (test): 21.262\n",
      "\n",
      "POLYNOMIAL DEGREE-3 W/ RIDGE REGRESSION\n",
      "(poly deg 3) linear model coeff (w):\n",
      "[ 0.00000000e+00  9.62069563e-01 -1.68358847e+00  2.31876270e+00\n",
      " -1.46747506e+00 -7.31761141e-02 -9.33646847e-03  1.86938416e-02\n",
      " -2.77955470e-02  1.70822169e-02  4.19307280e-03  1.27382279e-02\n",
      " -5.53520793e-03  1.19199550e-02  1.94080912e-02 -8.88527363e-03\n",
      " -8.00337144e-04 -5.36569520e-03  2.28899218e-03 -6.41651992e-03\n",
      " -2.28464225e-03  1.09450454e-05 -3.46712910e-05  1.01175577e-04\n",
      "  4.60659942e-07 -2.39465657e-05 -4.22856630e-05  1.91555001e-04\n",
      " -1.52904925e-04 -4.39060630e-05 -2.40488300e-04  1.54002169e-04\n",
      "  2.85199945e-04 -7.13331527e-05 -8.79852738e-05 -7.64079188e-05\n",
      " -4.68869056e-05 -3.44658935e-05  3.62910481e-05 -6.75496007e-05\n",
      "  4.60506413e-05 -2.82316725e-04  1.90478717e-04  1.03239802e-04\n",
      " -9.93243463e-05 -6.22361387e-05  2.38197342e-05  1.66805199e-04\n",
      "  9.79920579e-05  4.93306394e-06 -3.34246577e-04 -1.44394795e-04\n",
      " -2.27501270e-05  9.10029938e-05  2.27954872e-04  1.89586572e-05]\n",
      "(poly deg 3) linear model intercept (b): 19.808\n",
      "(poly deg 3) R-squared score (training): 0.609\n",
      "(poly deg 3) R-squared score (test): -1.121\n",
      "(poly deg 3) RMSE score (train): 18.796\n",
      "(poly deg 3) RMSE score (test): 42.812\n",
      "\n",
      "POLYNOMIAL DEGREE-5 W/ RIDGE REGRESSION\n",
      "(poly deg 5) linear model coeff (w):\n",
      "[ 0.00000000e+00 -1.58495107e+00  4.63260318e+00  1.93542423e+01\n",
      "  2.16510859e+01 -1.28797807e+01 -1.26766698e-01  3.34272404e-01\n",
      "  4.46082208e-01 -3.88849402e-01 -9.33766889e-03 -1.26661523e-02\n",
      " -5.59569257e-02 -5.01149549e-01 -1.45898769e-01 -5.71348395e-01\n",
      " -3.82618428e-01  1.50323714e-01 -1.91978281e-01  7.18044283e-02\n",
      "  3.47331751e-01  2.23053148e-03 -4.77254402e-03 -2.75645051e-03\n",
      "  3.88030610e-03  1.76120317e-03 -5.68326478e-03 -5.55282995e-03\n",
      "  8.19347465e-03  1.09361934e-03  6.37856867e-04 -2.04577777e-03\n",
      " -6.54540851e-03  2.35273093e-03  4.46351560e-04  6.85755435e-04\n",
      " -6.20115525e-04 -3.90495704e-03  8.40083433e-03  7.73358740e-03\n",
      "  6.96962069e-03  6.71611281e-03 -9.04837188e-03 -2.42162689e-03\n",
      " -1.10416923e-03  8.65500610e-04  4.99270705e-03 -5.93010294e-03\n",
      "  6.41324174e-03  9.96118410e-03  5.81269558e-03 -5.58879885e-03\n",
      " -4.56803614e-04  7.78418344e-04 -4.54697659e-03 -2.21921108e-03\n",
      " -1.83425546e-05  4.76361611e-05  5.94811692e-06 -1.44695104e-05\n",
      " -8.56347276e-06  8.17152290e-05 -7.63700872e-05 -6.43629099e-05\n",
      " -6.03279668e-05 -2.17470652e-05  1.21435918e-04  4.24633345e-05\n",
      " -3.71514284e-05 -4.96953548e-05  1.70232185e-05 -2.21703902e-05\n",
      "  1.53630038e-05  2.99508060e-05  3.44796353e-05  2.25325771e-05\n",
      "  1.01071221e-04  1.76336741e-04 -4.10086473e-05 -1.17279145e-04\n",
      " -2.67288526e-06  7.36000864e-05 -1.07796456e-04 -1.37829364e-04\n",
      " -1.23005224e-04  2.03630455e-04 -1.72050361e-05  4.95180164e-05\n",
      " -5.22878332e-05  7.04628952e-05 -2.83184218e-05  3.91649879e-06\n",
      "  6.26284907e-05 -3.75747850e-05 -1.79976328e-05 -6.61569128e-05\n",
      "  3.50972335e-05 -3.76140715e-05 -6.36393542e-05 -9.50257198e-05\n",
      " -4.46677519e-05 -4.09270460e-07 -5.52249356e-05  1.90452252e-05\n",
      " -9.80410865e-05 -4.88791804e-05  9.50698675e-05  3.70554186e-05\n",
      "  2.06900121e-04 -4.77679182e-05  4.95436466e-06 -3.18524505e-05\n",
      " -4.64473143e-05 -1.74787147e-05  1.99669581e-04  4.46280269e-05\n",
      " -2.86745177e-05 -7.84495416e-05 -2.60534769e-04  7.58946999e-05\n",
      "  2.27037148e-05 -7.54592065e-06  7.29394118e-05 -7.77301023e-05\n",
      "  6.19004414e-05 -1.17642906e-07  5.81759638e-08 -3.45788211e-08\n",
      " -1.28195982e-07 -1.94604355e-08  2.74526012e-08 -4.52544424e-07\n",
      "  5.62243511e-07 -1.95172203e-07  2.39421001e-07 -2.23136447e-08\n",
      " -3.07137815e-07  2.98851944e-07  4.10959765e-07 -1.55311145e-07\n",
      " -1.27850769e-07 -9.14183716e-08 -2.46630621e-07  9.14478066e-07\n",
      " -2.02103809e-07  3.28782920e-07 -4.52741634e-07 -3.43545210e-07\n",
      " -9.60150647e-08  2.18210986e-07  9.36463296e-08 -5.06661830e-09\n",
      " -2.84835705e-08  1.08188502e-07  7.64763861e-07 -1.81126532e-06\n",
      "  2.80586430e-07 -6.84448844e-07  9.96029359e-07  2.66560817e-07\n",
      " -1.26988030e-07  3.88933030e-07  7.42276870e-08 -7.56889588e-07\n",
      " -5.83721361e-07  5.95963205e-07 -5.88920648e-07 -7.94457391e-07\n",
      " -7.01726200e-07  2.27291296e-06  2.61384990e-07 -1.43372881e-06\n",
      "  1.09805822e-06  1.29805402e-06 -4.56715614e-08 -1.17840532e-06\n",
      " -1.04707301e-06  6.76824957e-07 -4.71112359e-07 -4.40704557e-07\n",
      "  2.43841311e-07  7.77559926e-09  1.53803831e-07  2.61423553e-07\n",
      "  1.34345665e-08 -1.90476727e-07 -8.08831455e-08 -4.03444678e-07\n",
      "  1.31487405e-06 -3.33828819e-08  2.93696133e-07  3.24525433e-07\n",
      " -7.70215720e-07  1.37032037e-07 -5.00602904e-07  1.31801494e-07\n",
      " -4.99951454e-08 -4.68236930e-08  2.39468588e-08 -8.25121771e-08\n",
      " -1.51624162e-07 -9.65455805e-07  7.00046688e-07  6.52464303e-07\n",
      "  8.19853272e-07 -1.67120107e-07  6.27211786e-07 -1.27512411e-08\n",
      " -1.38503765e-06  1.62941904e-06  6.08263508e-08  1.07801014e-06\n",
      " -7.13242620e-08 -1.86838851e-06  1.40078497e-07 -1.27902784e-07\n",
      " -2.60933565e-07  7.07509532e-07  6.32875698e-07 -1.66262659e-06\n",
      "  8.84614346e-07 -8.99966940e-07  1.95748117e-07  5.73389723e-07\n",
      " -7.62303901e-07  6.94478952e-08 -2.09919628e-07 -1.17118695e-07\n",
      "  6.96910407e-07  1.31381736e-07 -6.12759510e-08  1.40236723e-07\n",
      "  1.37928843e-08 -1.96888498e-07 -9.12156324e-08 -8.57999311e-08\n",
      "  1.94013267e-07 -1.83969009e-08 -5.92088426e-07  1.37541467e-07\n",
      "  1.69555450e-07  2.09466068e-07  5.88214164e-07  4.66294584e-07\n",
      " -4.82323239e-07 -1.19078241e-07 -5.92587980e-08 -6.96252725e-08\n",
      " -2.12402196e-07  1.35560142e-07 -7.51561088e-08  2.56451267e-08]\n",
      "(poly deg 5) linear model intercept (b): -322.214\n",
      "(poly deg 5) R-squared score (training): 0.792\n",
      "(poly deg 5) R-squared score (test): -2601.783\n",
      "(poly deg 5) RMSE score (train): 13.713\n",
      "(poly deg 5) RMSE score (test): 1499.796\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=3.04438e-25): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLYNOMIAL DEGREE-10 W/ RIDGE REGRESSION\n",
      "(poly deg 10) linear model coeff (w):\n",
      "[ 0.00000000e+00  7.68787830e-30  8.38795854e-30 ...  2.72464543e-16\n",
      " -3.14148365e-17 -1.09156796e-17]\n",
      "(poly deg 10) linear model intercept (b): 55.069\n",
      "(poly deg 10) R-squared score (training): 0.846\n",
      "(poly deg 10) R-squared score (test): -11380044339.484\n",
      "(poly deg 10) RMSE score (train): 11.782\n",
      "(poly deg 10) RMSE score (test): 3136066.725\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n"
     ]
    }
   ],
   "source": [
    "for degree in [2, 3, 5, 10]:\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_F1_poly = poly.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y, random_state=0)\n",
    "    linreg = Ridge().fit(X_train, y_train)\n",
    "\n",
    "    print('POLYNOMIAL DEGREE-{} W/ RIDGE REGRESSION'.format(degree))\n",
    "    print('(poly deg {}) linear model coeff (w):\\n{}'\n",
    "         .format(degree, linreg.coef_))\n",
    "    print('(poly deg {}) linear model intercept (b): {:.3f}'\n",
    "         .format(degree, linreg.intercept_))\n",
    "    print('(poly deg {}) R-squared score (training): {:.3f}'\n",
    "         .format(degree, linreg.score(X_train, y_train)))\n",
    "    print('(poly deg {}) R-squared score (test): {:.3f}'\n",
    "         .format(degree, linreg.score(X_test, y_test)))\n",
    "    print('(poly deg {}) RMSE score (train): {:.3f}'\n",
    "         .format(degree, sqrt(mean_squared_error(y_train, linreg.predict(X_train)))))\n",
    "    print('(poly deg {}) RMSE score (test): {:.3f}\\n'\n",
    "         .format(degree, sqrt(mean_squared_error(y_test, linreg.predict(X_test)))))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DAP3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
