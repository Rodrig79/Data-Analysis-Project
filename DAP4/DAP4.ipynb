{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y2E1jbS4ZluC",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.23.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn) (1.16.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn) (0.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (1.16.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shaun\\appdata\\roaming\\python\\python37\\site-packages (from python-dateutil>=2.7.3->pandas) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (3.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (1.16.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (7.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shaun\\appdata\\roaming\\python\\python37\\site-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from seaborn) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from seaborn) (1.16.4)\n",
      "Requirement already satisfied: pandas>=0.22.0 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from seaborn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.0.1 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from seaborn) (1.5.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=2.1.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=2.1.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=2.1.2->seaborn) (1.2.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=2.1.2->seaborn) (7.2.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas>=0.22.0->seaborn) (2020.1)\n",
      "Requirement already satisfied: six in c:\\users\\shaun\\appdata\\roaming\\python\\python37\\site-packages (from cycler>=0.10->matplotlib>=2.1.2->seaborn) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.14.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-plot in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.3.7)\n",
      "Requirement already satisfied: joblib>=0.10 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-plot) (0.16.0)\n",
      "Requirement already satisfied: scipy>=0.9 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-plot) (1.5.2)\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-plot) (3.3.0)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-plot) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scipy>=0.9->scikit-plot) (1.16.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (7.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn>=0.18->scikit-plot) (2.1.0)\n",
      "Requirement already satisfied: six in c:\\users\\shaun\\appdata\\roaming\\python\\python37\\site-packages (from cycler>=0.10->matplotlib>=1.4.0->scikit-plot) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.11.1)\n",
      "Requirement already satisfied: numpy>=1.14 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from statsmodels) (1.16.4)\n",
      "Requirement already satisfied: pandas>=0.21 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from statsmodels) (1.1.0)\n",
      "Requirement already satisfied: patsy>=0.5 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from statsmodels) (0.5.1)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from statsmodels) (1.5.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas>=0.21->statsmodels) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas>=0.21->statsmodels) (2.8.1)\n",
      "Requirement already satisfied: six in c:\\users\\shaun\\appdata\\roaming\\python\\python37\\site-packages (from patsy>=0.5->statsmodels) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#If you need to install in modules in jupyter notebook \n",
    "%pip install scikit-learn\n",
    "%pip install pandas\n",
    "%pip install matplotlib \n",
    "%pip install seaborn \n",
    "%pip install graphviz\n",
    "%pip install scikit-plot   \n",
    "%pip install statsmodels   \n",
    "\n",
    "\n",
    "#Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "import datetime # Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import scikitplot as skplt\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.linear_model import LinearRegression\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"decision_trees\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IvsV2RSPapC6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#changes the output for the print statements\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vtW9LfInZ_x2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Pull \n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Rodrig79/Machine-Learning-Data-Analysis-Project/master/rawData/pokemon.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "#Data Cleaning\n",
    "df = df[~df.Name.str.contains('Mega')] #removed pokemon with \"Mega\" in it\n",
    "df = df.drop(columns = [\"Name\",\"Type 1\",\"Type 2\",\"#\",\"Generation\"]) #Removed columns with names\n",
    "\n",
    "#normalization\n",
    "# X = df.values #returns a numpy array\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# x_scaled = min_max_scaler.fit_transform(X)\n",
    "# df=pd.DataFrame(x_scaled, columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Splitting dataset to x and y\n",
    "y = df.Attack\n",
    "X = df[['Defense','HP','Sp. Atk','Sp. Def','Speed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Defense</th>\n",
       "      <th>HP</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>45</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>39</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>80</td>\n",
       "      <td>65</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>121</td>\n",
       "      <td>108</td>\n",
       "      <td>81</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>150</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>170</td>\n",
       "      <td>130</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>130</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>751 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Defense   HP  Sp. Atk  Sp. Def  Speed\n",
       "0    49       45   65       65       45   \n",
       "1    63       60   80       80       60   \n",
       "2    83       80   100      100      80   \n",
       "4    43       39   60       50       65   \n",
       "5    58       58   80       65       80   \n",
       "..   ..       ..   ..       ..       ..   \n",
       "794  121      108  81       95       95   \n",
       "795  150      50   100      150      50   \n",
       "797  60       80   150      130      70   \n",
       "798  60       80   170      130      80   \n",
       "799  120      80   130      90       70   \n",
       "\n",
       "[751 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiVariate  Regression with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.linear_model import LinearRegression\n",
    " from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wheat dataset\n",
      "linear model intercept: 7.006393306776701\n",
      "linear model coeff:\n",
      "[ 0.41421069  0.40325123  0.19919182 -0.29041143  0.26333349]\n",
      "R-squared score (training): 0.417\n",
      "R-squared score (test): 0.443\n",
      "RMSE: 21.947\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('Wheat dataset')\n",
    "print('linear model intercept: {}'\n",
    "     .format(linreg.intercept_))\n",
    "print('linear model coeff:\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "# make predictions on the testing set\n",
    "y_pred = linreg.predict(X_test)\n",
    "print('RMSE: {:.3f}'.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xlrd\n",
      "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
      "Installing collected packages: xlrd\n",
      "Successfully installed xlrd-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "# excel_file = 'Folds5x2_pp.xlsx'\n",
    "# data = pd.read_excel(excel_file)\n",
    "# print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-4511192d146c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#seperating data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0myexample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mXexample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'AT'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'V'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'AP'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'RH'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttack\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "#seperating data\n",
    "df = df.dropna() \n",
    "yexample = data.PE\n",
    "Xexample = data[['AT','V','AP','RH']] \n",
    "y = df.Attack\n",
    "X = df[['Defense','HP','Speed']] \n",
    "X.dropna()\n",
    "y.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X, Y, B):\n",
    "    m = len(Y)\n",
    "    J = np.sum((X.dot(B) - Y) ** 2)/(2 * m)\n",
    "    return J\n",
    "\n",
    "def batch_gradient_descent(X, Y, B, alpha, iterations):\n",
    "    cost_history = [0] * iterations\n",
    "    m = len(Y)\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        #print(iteration)\n",
    "        # Hypothesis Values\n",
    "        h = X.dot(B)\n",
    "        # Difference b/w Hypothesis and Actual Y\n",
    "        loss = h - Y\n",
    "        # Gradient Calculation\n",
    "        gradient = X.T.dot(loss) / m\n",
    "        # Changing Values of B using Gradient\n",
    "        B = B - alpha * gradient\n",
    "        # New Cost Value\n",
    "        cost = cost_function(X, Y, B)\n",
    "        cost_history[iteration] = cost \n",
    "    return B, cost_history \n",
    "def pred(x_test, newB):\n",
    "    return x_test.dot(newB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Coefficients\n",
    "B = np.zeros(X_train.shape[1])\n",
    "alpha = 0.005\n",
    "iter_ = 2000\n",
    "newB, cost_history = batch_gradient_descent(X_train, y_train, B, alpha, iter_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = pred(X_test,newB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.539602881241804"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def r2(y_,y):\n",
    "    sst = np.sum((y-y.mean())**2)\n",
    "    ssr = np.sum((y_-y)**2)\n",
    "    r2 = 1-(ssr/sst)\n",
    "    return(r2)\n",
    "#----------------\n",
    "r2(y_,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-23.35527806775007"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_ = pred(X_test[3],newB)\n",
    "ans_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3289.766637161347,\n",
       " 3286.94843440967,\n",
       " 3284.1879706036,\n",
       " 3281.483981221271,\n",
       " 3278.8352298337386,\n",
       " 3276.2405074789954,\n",
       " 3273.69863204994,\n",
       " 3271.208447695996,\n",
       " 3268.7688242380723,\n",
       " 3266.378656596565,\n",
       " 3264.0368642321214,\n",
       " 3261.742390598857,\n",
       " 3259.4942026097833,\n",
       " 3257.2912901141385,\n",
       " 3255.1326653863803,\n",
       " 3253.0173626265687,\n",
       " 3250.9444374718923,\n",
       " 3248.9129665190817,\n",
       " 3246.9220468574686,\n",
       " 3244.970795612465,\n",
       " 3243.058349499212,\n",
       " 3241.1838643861834,\n",
       " 3239.3465148685204,\n",
       " 3237.5454938508765,\n",
       " 3235.7800121395667,\n",
       " 3234.049298043802,\n",
       " 3232.3525969858256,\n",
       " 3230.689171119732,\n",
       " 3229.0582989587874,\n",
       " 3227.459275011062,\n",
       " 3225.89140942318,\n",
       " 3224.354027632017,\n",
       " 3222.84647002416,\n",
       " 3221.368091602962,\n",
       " 3219.9182616630133,\n",
       " 3218.4963634718792,\n",
       " 3217.101793958928,\n",
       " 3215.7339634110913,\n",
       " 3214.3922951754207,\n",
       " 3213.0762253682597,\n",
       " 3211.7852025909115,\n",
       " 3210.518687651638,\n",
       " 3209.276153293861,\n",
       " 3208.057083930421,\n",
       " 3206.8609753837545,\n",
       " 3205.6873346318794,\n",
       " 3204.5356795600223,\n",
       " 3203.405538717798,\n",
       " 3202.2964510817988,\n",
       " 3201.20796582347,\n",
       " 3200.139642082166,\n",
       " 3199.0910487432657,\n",
       " 3198.061764221231,\n",
       " 3197.051376247503,\n",
       " 3196.059481663125,\n",
       " 3195.085686215991,\n",
       " 3194.129604362608,\n",
       " 3193.1908590742887,\n",
       " 3192.2690816476484,\n",
       " 3191.363911519339,\n",
       " 3190.4749960849085,\n",
       " 3189.601990521696,\n",
       " 3188.7445576156756,\n",
       " 3187.902367592165,\n",
       " 3187.0750979502964,\n",
       " 3186.26243330119,\n",
       " 3185.464065209725,\n",
       " 3184.679692039841,\n",
       " 3183.909018803287,\n",
       " 3183.151757011742,\n",
       " 3182.4076245322326,\n",
       " 3181.6763454457687,\n",
       " 3180.957649909136,\n",
       " 3180.251274019766,\n",
       " 3179.556959683613,\n",
       " 3178.8744544859887,\n",
       " 3178.203511565259,\n",
       " 3177.5438894893673,\n",
       " 3176.89535213511,\n",
       " 3176.257668570093,\n",
       " 3175.630612937327,\n",
       " 3175.013964342394,\n",
       " 3174.4075067431268,\n",
       " 3173.811028841744,\n",
       " 3173.224323979396,\n",
       " 3172.647190033054,\n",
       " 3172.079429314702,\n",
       " 3171.520848472771,\n",
       " 3170.971258395774,\n",
       " 3170.430474118084,\n",
       " 3169.898314727813,\n",
       " 3169.374603276749,\n",
       " 3168.859166692288,\n",
       " 3168.351835691343,\n",
       " 3167.852444696159,\n",
       " 3167.3608317520093,\n",
       " 3166.876838446728,\n",
       " 3166.400309832028,\n",
       " 3165.9310943465807,\n",
       " 3165.4690437408035,\n",
       " 3165.0140130033246,\n",
       " 3164.565860289087,\n",
       " 3164.124446849053,\n",
       " 3163.6896369614783,\n",
       " 3163.2612978647103,\n",
       " 3162.839299691492,\n",
       " 3162.4235154047224,\n",
       " 3162.01382073465,\n",
       " 3161.6100941174695,\n",
       " 3161.2122166352824,\n",
       " 3160.8200719573933,\n",
       " 3160.4335462829167,\n",
       " 3160.052528284666,\n",
       " 3159.676909054284,\n",
       " 3159.3065820485986,\n",
       " 3158.941443037181,\n",
       " 3158.5813900510593,\n",
       " 3158.2263233325834,\n",
       " 3157.876145286403,\n",
       " 3157.53076043154,\n",
       " 3157.1900753545224,\n",
       " 3156.8539986635715,\n",
       " 3156.5224409437997,\n",
       " 3156.195314713412,\n",
       " 3155.8725343808806,\n",
       " 3155.5540162030748,\n",
       " 3155.2396782443207,\n",
       " 3154.9294403363756,\n",
       " 3154.6232240392906,\n",
       " 3154.3209526031465,\n",
       " 3154.0225509306415,\n",
       " 3153.7279455405137,\n",
       " 3153.437064531772,\n",
       " 3153.1498375487295,\n",
       " 3152.8661957468153,\n",
       " 3152.5860717591404,\n",
       " 3152.309399663819,\n",
       " 3152.0361149520045,\n",
       " 3151.7661544966495,\n",
       " 3151.499456521952,\n",
       " 3151.2359605734873,\n",
       " 3150.9756074889983,\n",
       " 3150.7183393698456,\n",
       " 3150.4640995530835,\n",
       " 3150.212832584166,\n",
       " 3149.9644841902486,\n",
       " 3149.719001254099,\n",
       " 3149.4763317885754,\n",
       " 3149.2364249116827,\n",
       " 3148.9992308221745,\n",
       " 3148.764700775706,\n",
       " 3148.5327870615133,\n",
       " 3148.3034429796126,\n",
       " 3148.0766228185057,\n",
       " 3147.8522818333845,\n",
       " 3147.630376224815,\n",
       " 3147.4108631179015,\n",
       " 3147.19370054191,\n",
       " 3146.9788474103484,\n",
       " 3146.7662635014844,\n",
       " 3146.5559094393034,\n",
       " 3146.3477466748845,\n",
       " 3146.141737468194,\n",
       " 3145.93784487028,\n",
       " 3145.7360327058695,\n",
       " 3145.536265556346,\n",
       " 3145.338508743107,\n",
       " 3145.142728311297,\n",
       " 3144.948891013892,\n",
       " 3144.756964296148,\n",
       " 3144.566916280388,\n",
       " 3144.378715751133,\n",
       " 3144.1923321405566,\n",
       " 3144.0077355142707,\n",
       " 3143.8248965574203,\n",
       " 3143.6437865610915,\n",
       " 3143.46437740902,\n",
       " 3143.2866415645967,\n",
       " 3143.110552058157,\n",
       " 3142.936082474559,\n",
       " 3142.7632069410347,\n",
       " 3142.59190011531,\n",
       " 3142.4221371739904,\n",
       " 3142.2538938012035,\n",
       " 3142.087146177495,\n",
       " 3141.9218709689694,\n",
       " 3141.758045316675,\n",
       " 3141.5956468262216,\n",
       " 3141.4346535576287,\n",
       " 3141.2750440154014,\n",
       " 3141.1167971388268,\n",
       " 3140.9598922924815,\n",
       " 3140.8043092569546,\n",
       " 3140.650028219774,\n",
       " 3140.4970297665373,\n",
       " 3140.345294872231,\n",
       " 3140.194804892757,\n",
       " 3140.0455415566307,\n",
       " 3139.8974869568747,\n",
       " 3139.750623543089,\n",
       " 3139.6049341136963,\n",
       " 3139.460401808359,\n",
       " 3139.3170101005658,\n",
       " 3139.174742790382,\n",
       " 3139.033583997361,\n",
       " 3138.8935181536117,\n",
       " 3138.754529997021,\n",
       " 3138.616604564627,\n",
       " 3138.479727186137,\n",
       " 3138.343883477588,\n",
       " 3138.209059335157,\n",
       " 3138.0752409290894,\n",
       " 3137.9424146977885,\n",
       " 3137.81056734201,\n",
       " 3137.6796858192,\n",
       " 3137.549757337957,\n",
       " 3137.4207693526096,\n",
       " 3137.2927095579234,\n",
       " 3137.165565883919,\n",
       " 3137.0393264908053,\n",
       " 3136.913979764027,\n",
       " 3136.7895143094206,\n",
       " 3136.6659189484776,\n",
       " 3136.543182713712,\n",
       " 3136.4212948441323,\n",
       " 3136.3002447808108,\n",
       " 3136.1800221625504,\n",
       " 3136.0606168216536,\n",
       " 3135.9420187797778,\n",
       " 3135.824218243884,\n",
       " 3135.707205602278,\n",
       " 3135.590971420735,\n",
       " 3135.4755064387127,\n",
       " 3135.3608015656487,\n",
       " 3135.246847877334,\n",
       " 3135.1336366123764,\n",
       " 3135.021159168732,\n",
       " 3134.909407100317,\n",
       " 3134.7983721136993,\n",
       " 3134.688046064853,\n",
       " 3134.5784209559965,\n",
       " 3134.469488932488,\n",
       " 3134.361242279797,\n",
       " 3134.2536734205455,\n",
       " 3134.1467749116023,\n",
       " 3134.0405394412564,\n",
       " 3133.9349598264394,\n",
       " 3133.8300290100174,\n",
       " 3133.7257400581434,\n",
       " 3133.622086157656,\n",
       " 3133.519060613555,\n",
       " 3133.416656846512,\n",
       " 3133.314868390451,\n",
       " 3133.213688890174,\n",
       " 3133.1131120990462,\n",
       " 3133.0131318767185,\n",
       " 3132.913742186919,\n",
       " 3132.8149370952738,\n",
       " 3132.7167107671926,\n",
       " 3132.619057465789,\n",
       " 3132.521971549849,\n",
       " 3132.4254474718505,\n",
       " 3132.3294797760177,\n",
       " 3132.234063096421,\n",
       " 3132.1391921551235,\n",
       " 3132.044861760361,\n",
       " 3131.951066804765,\n",
       " 3131.8578022636275,\n",
       " 3131.7650631931965,\n",
       " 3131.6728447290166,\n",
       " 3131.581142084301,\n",
       " 3131.489950548344,\n",
       " 3131.39926548496,\n",
       " 3131.3090823309644,\n",
       " 3131.219396594686,\n",
       " 3131.1302038545077,\n",
       " 3131.0414997574476,\n",
       " 3130.953280017759,\n",
       " 3130.865540415573,\n",
       " 3130.7782767955637,\n",
       " 3130.6914850656435,\n",
       " 3130.6051611956896,\n",
       " 3130.519301216294,\n",
       " 3130.433901217547,\n",
       " 3130.3489573478373,\n",
       " 3130.2644658126915,\n",
       " 3130.1804228736232,\n",
       " 3130.0968248470244,\n",
       " 3130.013668103066,\n",
       " 3129.9309490646297,\n",
       " 3129.848664206264,\n",
       " 3129.766810053158,\n",
       " 3129.685383180141,\n",
       " 3129.6043802107038,\n",
       " 3129.523797816039,\n",
       " 3129.4436327141057,\n",
       " 3129.363881668708,\n",
       " 3129.2845414886033,\n",
       " 3129.2056090266196,\n",
       " 3129.1270811788004,\n",
       " 3129.04895488356,\n",
       " 3128.971227120866,\n",
       " 3128.893894911431,\n",
       " 3128.816955315928,\n",
       " 3128.7404054342205,\n",
       " 3128.6642424046067,\n",
       " 3128.5884634030836,\n",
       " 3128.513065642626,\n",
       " 3128.4380463724815,\n",
       " 3128.363402877477,\n",
       " 3128.289132477346,\n",
       " 3128.215232526066,\n",
       " 3128.1417004112104,\n",
       " 3128.0685335533176,\n",
       " 3127.99572940527,\n",
       " 3127.9232854516877,\n",
       " 3127.851199208335,\n",
       " 3127.7794682215404,\n",
       " 3127.708090067629,\n",
       " 3127.637062352365,\n",
       " 3127.566382710408,\n",
       " 3127.4960488047795,\n",
       " 3127.426058326345,\n",
       " 3127.3564089932993,\n",
       " 3127.287098550671,\n",
       " 3127.218124769831,\n",
       " 3127.1494854480165,\n",
       " 3127.081178407862,\n",
       " 3127.0132014969404,\n",
       " 3126.9455525873163,\n",
       " 3126.8782295751057,\n",
       " 3126.811230380045,\n",
       " 3126.7445529450747,\n",
       " 3126.6781952359233,\n",
       " 3126.6121552407058,\n",
       " 3126.546430969531,\n",
       " 3126.4810204541113,\n",
       " 3126.415921747391,\n",
       " 3126.3511329231683,\n",
       " 3126.286652075738,\n",
       " 3126.222477319537,\n",
       " 3126.1586067887943,\n",
       " 3126.095038637192,\n",
       " 3126.0317710375334,\n",
       " 3125.9688021814172,\n",
       " 3125.906130278916,\n",
       " 3125.843753558265,\n",
       " 3125.7816702655564,\n",
       " 3125.7198786644403,\n",
       " 3125.65837703583,\n",
       " 3125.597163677616,\n",
       " 3125.536236904382,\n",
       " 3125.4755950471326,\n",
       " 3125.4152364530205,\n",
       " 3125.355159485086,\n",
       " 3125.295362521994,\n",
       " 3125.2358439577824,\n",
       " 3125.176602201615,\n",
       " 3125.1176356775354,\n",
       " 3125.0589428242315,\n",
       " 3125.0005220948005,\n",
       " 3124.942371956522,\n",
       " 3124.884490890631,\n",
       " 3124.8268773921013,\n",
       " 3124.769529969432,\n",
       " 3124.7124471444317,\n",
       " 3124.655627452018,\n",
       " 3124.599069440011,\n",
       " 3124.5427716689383,\n",
       " 3124.48673271184,\n",
       " 3124.4309511540787,\n",
       " 3124.3754255931517,\n",
       " 3124.3201546385117,\n",
       " 3124.2651369113864,\n",
       " 3124.210371044601,\n",
       " 3124.1558556824116,\n",
       " 3124.1015894803313,\n",
       " 3124.0475711049685,\n",
       " 3123.993799233867,\n",
       " 3123.94027255534,\n",
       " 3123.8869897683253,\n",
       " 3123.833949582224,\n",
       " 3123.781150716755,\n",
       " 3123.72859190181,\n",
       " 3123.6762718773084,\n",
       " 3123.6241893930555,\n",
       " 3123.572343208607,\n",
       " 3123.5207320931318,\n",
       " 3123.469354825282,\n",
       " 3123.4182101930596,\n",
       " 3123.367296993691,\n",
       " 3123.316614033504,\n",
       " 3123.2661601277987,\n",
       " 3123.2159341007364,\n",
       " 3123.165934785213,\n",
       " 3123.1161610227487,\n",
       " 3123.0666116633747,\n",
       " 3123.0172855655183,\n",
       " 3122.9681815958957,\n",
       " 3122.9192986294065,\n",
       " 3122.8706355490253,\n",
       " 3122.822191245703,\n",
       " 3122.7739646182613,\n",
       " 3122.7259545732963,\n",
       " 3122.6781600250797,\n",
       " 3122.630579895465,\n",
       " 3122.5832131137895,\n",
       " 3122.53605861679,\n",
       " 3122.4891153485037,\n",
       " 3122.442382260186,\n",
       " 3122.395858310221,\n",
       " 3122.349542464036,\n",
       " 3122.3034336940173,\n",
       " 3122.2575309794297,\n",
       " 3122.2118333063336,\n",
       " 3122.166339667508,\n",
       " 3122.1210490623694,\n",
       " 3122.075960496899,\n",
       " 3122.0310729835637,\n",
       " 3121.9863855412464,\n",
       " 3121.94189719517,\n",
       " 3121.897606976829,\n",
       " 3121.853513923916,\n",
       " 3121.8096170802587,\n",
       " 3121.765915495746,\n",
       " 3121.722408226266,\n",
       " 3121.679094333641,\n",
       " 3121.6359728855596,\n",
       " 3121.5930429555187,\n",
       " 3121.5503036227583,\n",
       " 3121.5077539722024,\n",
       " 3121.4653930943987,\n",
       " 3121.4232200854613,\n",
       " 3121.38123404701,\n",
       " 3121.3394340861164,\n",
       " 3121.297819315248,\n",
       " 3121.256388852212,\n",
       " 3121.215141820103,\n",
       " 3121.1740773472475,\n",
       " 3121.133194567157,\n",
       " 3121.092492618469,\n",
       " 3121.0519706449054,\n",
       " 3121.0116277952156,\n",
       " 3120.9714632231326,\n",
       " 3120.9314760873217,\n",
       " 3120.891665551337,\n",
       " 3120.8520307835715,\n",
       " 3120.8125709572128,\n",
       " 3120.7732852501995,\n",
       " 3120.734172845175,\n",
       " 3120.6952329294454,\n",
       " 3120.656464694936,\n",
       " 3120.6178673381487,\n",
       " 3120.579440060121,\n",
       " 3120.541182066389,\n",
       " 3120.5030925669375,\n",
       " 3120.4651707761714,\n",
       " 3120.4274159128695,\n",
       " 3120.3898272001497,\n",
       " 3120.352403865428,\n",
       " 3120.315145140387,\n",
       " 3120.2780502609316,\n",
       " 3120.2411184671596,\n",
       " 3120.204349003321,\n",
       " 3120.1677411177875,\n",
       " 3120.1312940630155,\n",
       " 3120.0950070955114,\n",
       " 3120.0588794757996,\n",
       " 3120.022910468389,\n",
       " 3119.987099341741,\n",
       " 3119.951445368237,\n",
       " 3119.9159478241436,\n",
       " 3119.880605989589,\n",
       " 3119.845419148525,\n",
       " 3119.810386588699,\n",
       " 3119.775507601628,\n",
       " 3119.7407814825606,\n",
       " 3119.706207530457,\n",
       " 3119.671785047955,\n",
       " 3119.6375133413426,\n",
       " 3119.6033917205305,\n",
       " 3119.569419499027,\n",
       " 3119.535595993905,\n",
       " 3119.5019205257818,\n",
       " 3119.468392418788,\n",
       " 3119.4350110005435,\n",
       " 3119.401775602132,\n",
       " 3119.368685558074,\n",
       " 3119.3357402063034,\n",
       " 3119.3029388881414,\n",
       " 3119.270280948273,\n",
       " 3119.237765734723,\n",
       " 3119.205392598832,\n",
       " 3119.1731608952314,\n",
       " 3119.141069981823,\n",
       " 3119.109119219753,\n",
       " 3119.0773079733926,\n",
       " 3119.045635610313,\n",
       " 3119.0141015012637,\n",
       " 3118.9827050201516,\n",
       " 3118.9514455440203,\n",
       " 3118.9203224530256,\n",
       " 3118.889335130417,\n",
       " 3118.858482962517,\n",
       " 3118.827765338698,\n",
       " 3118.797181651367,\n",
       " 3118.766731295938,\n",
       " 3118.73641367082,\n",
       " 3118.706228177392,\n",
       " 3118.676174219985,\n",
       " 3118.6462512058642,\n",
       " 3118.616458545209,\n",
       " 3118.586795651094,\n",
       " 3118.557261939469,\n",
       " 3118.527856829145,\n",
       " 3118.498579741772,\n",
       " 3118.469430101822,\n",
       " 3118.4404073365727,\n",
       " 3118.411510876087,\n",
       " 3118.382740153199,\n",
       " 3118.354094603496,\n",
       " 3118.325573665298,\n",
       " 3118.2971767796466,\n",
       " 3118.268903390282,\n",
       " 3118.2407529436323,\n",
       " 3118.212724888795,\n",
       " 3118.1848186775173,\n",
       " 3118.1570337641874,\n",
       " 3118.1293696058106,\n",
       " 3118.1018256620005,\n",
       " 3118.0744013949584,\n",
       " 3118.0470962694617,\n",
       " 3118.0199097528457,\n",
       " 3117.9928413149905,\n",
       " 3117.9658904283046,\n",
       " 3117.939056567712,\n",
       " 3117.9123392106358,\n",
       " 3117.8857378369835,\n",
       " 3117.8592519291337,\n",
       " 3117.8328809719233,\n",
       " 3117.806624452629,\n",
       " 3117.780481860957,\n",
       " 3117.7544526890288,\n",
       " 3117.7285364313652,\n",
       " 3117.7027325848762,\n",
       " 3117.6770406488436,\n",
       " 3117.6514601249096,\n",
       " 3117.6259905170655,\n",
       " 3117.6006313316343,\n",
       " 3117.5753820772607,\n",
       " 3117.5502422648988,\n",
       " 3117.525211407795,\n",
       " 3117.50028902148,\n",
       " 3117.4754746237554,\n",
       " 3117.4507677346787,\n",
       " 3117.4261678765515,\n",
       " 3117.401674573912,\n",
       " 3117.377287353514,\n",
       " 3117.353005744325,\n",
       " 3117.328829277505,\n",
       " 3117.3047574864004,\n",
       " 3117.2807899065315,\n",
       " 3117.256926075578,\n",
       " 3117.233165533371,\n",
       " 3117.209507821878,\n",
       " 3117.185952485195,\n",
       " 3117.1624990695327,\n",
       " 3117.1391471232055,\n",
       " 3117.115896196622,\n",
       " 3117.092745842272,\n",
       " 3117.0696956147167,\n",
       " 3117.0467450705773,\n",
       " 3117.023893768525,\n",
       " 3117.001141269269,\n",
       " 3116.978487135546,\n",
       " 3116.9559309321116,\n",
       " 3116.9334722257277,\n",
       " 3116.9111105851516,\n",
       " 3116.8888455811298,\n",
       " 3116.86667678638,\n",
       " 3116.844603775591,\n",
       " 3116.822626125403,\n",
       " 3116.800743414404,\n",
       " 3116.7789552231184,\n",
       " 3116.757261133993,\n",
       " 3116.7356607313945,\n",
       " 3116.714153601592,\n",
       " 3116.6927393327546,\n",
       " 3116.6714175149364,\n",
       " 3116.6501877400683,\n",
       " 3116.6290496019496,\n",
       " 3116.608002696239,\n",
       " 3116.5870466204433,\n",
       " 3116.5661809739076,\n",
       " 3116.5454053578096,\n",
       " 3116.524719375148,\n",
       " 3116.5041226307326,\n",
       " 3116.483614731178,\n",
       " 3116.4631952848904,\n",
       " 3116.442863902064,\n",
       " 3116.4226201946663,\n",
       " 3116.4024637764355,\n",
       " 3116.3823942628655,\n",
       " 3116.362411271204,\n",
       " 3116.342514420436,\n",
       " 3116.3227033312814,\n",
       " 3116.3029776261837,\n",
       " 3116.2833369293035,\n",
       " 3116.2637808665063,\n",
       " 3116.244309065359,\n",
       " 3116.2249211551175,\n",
       " 3116.205616766721,\n",
       " 3116.1863955327813,\n",
       " 3116.1672570875776,\n",
       " 3116.148201067048,\n",
       " 3116.1292271087764,\n",
       " 3116.110334851992,\n",
       " 3116.0915239375568,\n",
       " 3116.072794007958,\n",
       " 3116.0541447073006,\n",
       " 3116.0355756813,\n",
       " 3116.0170865772757,\n",
       " 3115.9986770441383,\n",
       " 3115.980346732388,\n",
       " 3115.962095294103,\n",
       " 3115.943922382933,\n",
       " 3115.925827654093,\n",
       " 3115.907810764353,\n",
       " 3115.889871372033,\n",
       " 3115.8720091369946,\n",
       " 3115.854223720632,\n",
       " 3115.83651478587,\n",
       " 3115.818881997148,\n",
       " 3115.801325020423,\n",
       " 3115.783843523153,\n",
       " 3115.7664371742967,\n",
       " 3115.7491056443014,\n",
       " 3115.7318486050995,\n",
       " 3115.7146657301005,\n",
       " 3115.697556694182,\n",
       " 3115.680521173685,\n",
       " 3115.6635588464073,\n",
       " 3115.6466693915936,\n",
       " 3115.629852489931,\n",
       " 3115.6131078235453,\n",
       " 3115.596435075984,\n",
       " 3115.579833932222,\n",
       " 3115.5633040786456,\n",
       " 3115.5468452030523,\n",
       " 3115.5304569946397,\n",
       " 3115.514139143999,\n",
       " 3115.4978913431114,\n",
       " 3115.481713285341,\n",
       " 3115.4656046654254,\n",
       " 3115.4495651794723,\n",
       " 3115.4335945249504,\n",
       " 3115.417692400687,\n",
       " 3115.4018585068575,\n",
       " 3115.3860925449803,\n",
       " 3115.3703942179127,\n",
       " 3115.3547632298414,\n",
       " 3115.3391992862794,\n",
       " 3115.323702094056,\n",
       " 3115.3082713613144,\n",
       " 3115.2929067975037,\n",
       " 3115.2776081133743,\n",
       " 3115.2623750209673,\n",
       " 3115.247207233616,\n",
       " 3115.232104465933,\n",
       " 3115.2170664338073,\n",
       " 3115.2020928544,\n",
       " 3115.187183446133,\n",
       " 3115.17233792869,\n",
       " 3115.1575560230053,\n",
       " 3115.1428374512616,\n",
       " 3115.128181936879,\n",
       " 3115.113589204517,\n",
       " 3115.099058980062,\n",
       " 3115.084590990626,\n",
       " 3115.070184964536,\n",
       " 3115.0558406313357,\n",
       " 3115.0415577217714,\n",
       " 3115.0273359677935,\n",
       " 3115.013175102548,\n",
       " 3114.999074860369,\n",
       " 3114.985034976778,\n",
       " 3114.971055188474,\n",
       " 3114.95713523333,\n",
       " 3114.9432748503887,\n",
       " 3114.929473779853,\n",
       " 3114.9157317630875,\n",
       " 3114.902048542605,\n",
       " 3114.8884238620685,\n",
       " 3114.874857466281,\n",
       " 3114.8613491011815,\n",
       " 3114.847898513842,\n",
       " 3114.834505452461,\n",
       " 3114.8211696663548,\n",
       " 3114.8078909059573,\n",
       " 3114.794668922814,\n",
       " 3114.781503469575,\n",
       " 3114.7683942999893,\n",
       " 3114.7553411689046,\n",
       " 3114.7423438322567,\n",
       " 3114.7294020470663,\n",
       " 3114.7165155714365,\n",
       " 3114.7036841645445,\n",
       " 3114.6909075866383,\n",
       " 3114.678185599032,\n",
       " 3114.6655179641,\n",
       " 3114.652904445273,\n",
       " 3114.640344807032,\n",
       " 3114.627838814905,\n",
       " 3114.615386235461,\n",
       " 3114.602986836306,\n",
       " 3114.59064038608,\n",
       " 3114.578346654446,\n",
       " 3114.566105412093,\n",
       " 3114.553916430726,\n",
       " 3114.5417794830655,\n",
       " 3114.529694342839,\n",
       " 3114.5176607847784,\n",
       " 3114.5056785846145,\n",
       " 3114.493747519076,\n",
       " 3114.481867365877,\n",
       " 3114.4700379037217,\n",
       " 3114.458258912294,\n",
       " 3114.4465301722557,\n",
       " 3114.4348514652397,\n",
       " 3114.4232225738474,\n",
       " 3114.4116432816454,\n",
       " 3114.400113373157,\n",
       " 3114.3886326338634,\n",
       " 3114.3772008501937,\n",
       " 3114.3658178095247,\n",
       " 3114.3544833001747,\n",
       " 3114.3431971114005,\n",
       " 3114.3319590333917,\n",
       " 3114.3207688572656,\n",
       " 3114.3096263750676,\n",
       " 3114.2985313797626,\n",
       " 3114.28748366523,\n",
       " 3114.2764830262645,\n",
       " 3114.2655292585678,\n",
       " 3114.254622158747,\n",
       " 3114.2437615243075,\n",
       " 3114.2329471536514,\n",
       " 3114.2221788460724,\n",
       " 3114.2114564017543,\n",
       " 3114.200779621762,\n",
       " 3114.1901483080414,\n",
       " 3114.1795622634145,\n",
       " 3114.169021291575,\n",
       " 3114.1585251970846,\n",
       " 3114.1480737853694,\n",
       " 3114.137666862715,\n",
       " 3114.127304236263,\n",
       " 3114.1169857140103,\n",
       " 3114.106711104797,\n",
       " 3114.0964802183125,\n",
       " 3114.0862928650863,\n",
       " 3114.0761488564826,\n",
       " 3114.0660480047018,\n",
       " 3114.055990122772,\n",
       " 3114.0459750245473,\n",
       " 3114.036002524705,\n",
       " 3114.0260724387394,\n",
       " 3114.01618458296,\n",
       " 3114.006338774487,\n",
       " 3113.996534831249,\n",
       " 3113.986772571978,\n",
       " 3113.977051816203,\n",
       " 3113.967372384254,\n",
       " 3113.9577340972514,\n",
       " 3113.948136777105,\n",
       " 3113.938580246511,\n",
       " 3113.9290643289464,\n",
       " 3113.919588848669,\n",
       " 3113.9101536307094,\n",
       " 3113.9007585008717,\n",
       " 3113.891403285727,\n",
       " 3113.8820878126116,\n",
       " 3113.8728119096227,\n",
       " 3113.863575405616,\n",
       " 3113.8543781302014,\n",
       " 3113.845219913739,\n",
       " 3113.8361005873376,\n",
       " 3113.82701998285,\n",
       " 3113.8179779328684,\n",
       " 3113.808974270726,\n",
       " 3113.800008830487,\n",
       " 3113.7910814469483,\n",
       " 3113.782191955633,\n",
       " 3113.7733401927912,\n",
       " 3113.764525995391,\n",
       " 3113.755749201121,\n",
       " 3113.7470096483826,\n",
       " 3113.7383071762906,\n",
       " 3113.729641624666,\n",
       " 3113.721012834036,\n",
       " 3113.7124206456297,\n",
       " 3113.7038649013753,\n",
       " 3113.695345443895,\n",
       " 3113.686862116506,\n",
       " 3113.678414763213,\n",
       " 3113.6700032287076,\n",
       " 3113.661627358364,\n",
       " 3113.653286998238,\n",
       " 3113.6449819950617,\n",
       " 3113.63671219624,\n",
       " 3113.628477449851,\n",
       " 3113.62027760464,\n",
       " 3113.6121125100167,\n",
       " 3113.603982016053,\n",
       " 3113.595885973481,\n",
       " 3113.5878242336876,\n",
       " 3113.579796648714,\n",
       " 3113.57180307125,\n",
       " 3113.5638433546346,\n",
       " 3113.5559173528495,\n",
       " 3113.5480249205193,\n",
       " 3113.5401659129075,\n",
       " 3113.5323401859114,\n",
       " 3113.5245475960633,\n",
       " 3113.516788000525,\n",
       " 3113.5090612570843,\n",
       " 3113.501367224156,\n",
       " 3113.4937057607754,\n",
       " 3113.4860767265945,\n",
       " 3113.478479981886,\n",
       " 3113.470915387532,\n",
       " 3113.463382805027,\n",
       " 3113.455882096472,\n",
       " 3113.448413124576,\n",
       " 3113.440975752648,\n",
       " 3113.433569844596,\n",
       " 3113.4261952649294,\n",
       " 3113.4188518787473,\n",
       " 3113.411539551743,\n",
       " 3113.4042581501976,\n",
       " 3113.3970075409807,\n",
       " 3113.3897875915436,\n",
       " 3113.3825981699188,\n",
       " 3113.3754391447205,\n",
       " 3113.3683103851354,\n",
       " 3113.3612117609255,\n",
       " 3113.3541431424233,\n",
       " 3113.3471044005287,\n",
       " 3113.3400954067097,\n",
       " 3113.3331160329963,\n",
       " 3113.326166151978,\n",
       " 3113.319245636805,\n",
       " 3113.312354361182,\n",
       " 3113.3054921993667,\n",
       " 3113.2986590261685,\n",
       " 3113.2918547169443,\n",
       " 3113.285079147597,\n",
       " 3113.2783321945744,\n",
       " 3113.2716137348634,\n",
       " 3113.2649236459906,\n",
       " 3113.2582618060187,\n",
       " 3113.251628093545,\n",
       " 3113.2450223876967,\n",
       " 3113.2384445681305,\n",
       " 3113.2318945150305,\n",
       " 3113.225372109105,\n",
       " 3113.2188772315844,\n",
       " 3113.212409764218,\n",
       " 3113.2059695892735,\n",
       " 3113.1995565895336,\n",
       " 3113.1931706482933,\n",
       " 3113.1868116493574,\n",
       " 3113.1804794770405,\n",
       " 3113.174174016163,\n",
       " 3113.167895152048,\n",
       " 3113.1616427705203,\n",
       " 3113.155416757904,\n",
       " 3113.149217001021,\n",
       " 3113.1430433871874,\n",
       " 3113.1368958042112,\n",
       " 3113.130774140392,\n",
       " 3113.1246782845164,\n",
       " 3113.1186081258584,\n",
       " 3113.1125635541744,\n",
       " 3113.1065444597043,\n",
       " 3113.100550733165,\n",
       " 3113.094582265753,\n",
       " 3113.08863894914,\n",
       " 3113.0827206754702,\n",
       " 3113.0768273373583,\n",
       " 3113.0709588278887,\n",
       " 3113.0651150406134,\n",
       " 3113.059295869548,\n",
       " 3113.0535012091714,\n",
       " 3113.047730954423,\n",
       " 3113.041985000702,\n",
       " 3113.0362632438623,\n",
       " 3113.0305655802126,\n",
       " 3113.024891906516,\n",
       " 3113.019242119985,\n",
       " 3113.01361611828,\n",
       " 3113.0080137995083,\n",
       " 3113.0024350622225,\n",
       " 3112.996879805417,\n",
       " 3112.9913479285274,\n",
       " 3112.9858393314275,\n",
       " 3112.9803539144277,\n",
       " 3112.9748915782734,\n",
       " 3112.969452224144,\n",
       " 3112.9640357536464,\n",
       " 3112.9586420688206,\n",
       " 3112.9532710721305,\n",
       " 3112.947922666468,\n",
       " 3112.9425967551456,\n",
       " 3112.9372932418983,\n",
       " 3112.9320120308817,\n",
       " 3112.926753026668,\n",
       " 3112.921516134245,\n",
       " 3112.9163012590143,\n",
       " 3112.9111083067914,\n",
       " 3112.9059371838002,\n",
       " 3112.9007877966746,\n",
       " 3112.8956600524534,\n",
       " 3112.890553858582,\n",
       " 3112.885469122908,\n",
       " 3112.8804057536804,\n",
       " 3112.875363659549,\n",
       " 3112.870342749557,\n",
       " 3112.8653429331503,\n",
       " 3112.8603641201635,\n",
       " 3112.8554062208254,\n",
       " 3112.8504691457574,\n",
       " 3112.845552805966,\n",
       " 3112.8406571128494,\n",
       " 3112.8357819781872,\n",
       " 3112.8309273141467,\n",
       " 3112.8260930332735,\n",
       " 3112.8212790484977,\n",
       " 3112.8164852731243,\n",
       " 3112.8117116208387,\n",
       " 3112.806958005699,\n",
       " 3112.8022243421387,\n",
       " 3112.797510544963,\n",
       " 3112.7928165293474,\n",
       " 3112.788142210836,\n",
       " 3112.7834875053413,\n",
       " 3112.778852329139,\n",
       " 3112.7742365988715,\n",
       " 3112.7696402315414,\n",
       " 3112.7650631445135,\n",
       " 3112.7605052555105,\n",
       " 3112.755966482612,\n",
       " 3112.7514467442575,\n",
       " 3112.7469459592353,\n",
       " 3112.74246404669,\n",
       " 3112.7380009261183,\n",
       " 3112.733556517363,\n",
       " 3112.7291307406176,\n",
       " 3112.7247235164214,\n",
       " 3112.720334765659,\n",
       " 3112.71596440956,\n",
       " 3112.711612369692,\n",
       " 3112.707278567967,\n",
       " 3112.7029629266353,\n",
       " 3112.698665368282,\n",
       " 3112.694385815832,\n",
       " 3112.690124192542,\n",
       " 3112.6858804220037,\n",
       " 3112.6816544281387,\n",
       " 3112.677446135199,\n",
       " 3112.6732554677656,\n",
       " 3112.6690823507474,\n",
       " 3112.664926709378,\n",
       " 3112.6607884692153,\n",
       " 3112.656667556141,\n",
       " 3112.6525638963576,\n",
       " 3112.6484774163873,\n",
       " 3112.644408043072,\n",
       " 3112.640355703569,\n",
       " 3112.6363203253536,\n",
       " 3112.632301836214,\n",
       " 3112.628300164252,\n",
       " 3112.62431523788,\n",
       " 3112.6203469858237,\n",
       " 3112.6163953371133,\n",
       " 3112.6124602210893,\n",
       " 3112.608541567399,\n",
       " 3112.6046393059924,\n",
       " 3112.600753367124,\n",
       " 3112.5968836813513,\n",
       " 3112.5930301795297,\n",
       " 3112.5891927928183,\n",
       " 3112.5853714526706,\n",
       " 3112.5815660908397,\n",
       " 3112.5777766393717,\n",
       " 3112.5740030306097,\n",
       " 3112.5702451971865,\n",
       " 3112.56650307203,\n",
       " 3112.5627765883555,\n",
       " 3112.559065679669,\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.76812684, -0.91287325, -0.17236808, -0.18658351, -0.77648137],\n",
       "       [-0.29531579, -0.33436756,  0.32360291,  0.36120467, -0.23743764],\n",
       "       [ 0.38012857,  0.43697336,  0.98489757,  1.09158892,  0.48128734],\n",
       "       ...,\n",
       "       [-0.39663244,  0.43697336,  2.63813422,  2.18716529,  0.12192485],\n",
       "       [-0.39663244,  0.43697336,  3.29942887,  2.18716529,  0.48128734],\n",
       "       [ 1.62970064,  0.43697336,  1.97683956,  0.7263968 ,  0.12192485]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Regression Model\n",
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge regression linear model intercept: 7.009233212820845\n",
      "ridge regression linear model coeff:\n",
      "[ 0.41417311  0.40321748  0.19917968 -0.29035188  0.26331594]\n",
      "R-squared score (training): 0.417\n",
      "R-squared score (test): 0.443\n",
      "Number of non-zero features: 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "linridge = Ridge(alpha=25.0).fit(X_train, y_train)\n",
    "\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test, y_test)))\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-352-d71bc87e92fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX_train_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX_test_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlinridge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRidge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         X = check_array(X, copy=self.copy, dtype=FLOAT_DTYPES,\n\u001b[0m\u001b[1;32m    408\u001b[0m                         force_all_finite=\"allow-nan\")\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m             raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n\u001b[0m\u001b[1;32m    651\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                              % (n_samples, array.shape, ensure_min_samples,\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "# Ridge regression with feature normalization\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linridge = Ridge(alpha=20.0).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Wheat dataset')\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test_scaled, y_test)))\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression: effect of alpha regularization parameter\n",
      "\n",
      "Alpha = 0.00\n",
      "num abs(coeff) > 1.0: 5, r-squared training: 0.42, r-squared test: 0.44\n",
      "\n",
      "Alpha = 1.00\n",
      "num abs(coeff) > 1.0: 5, r-squared training: 0.41, r-squared test: 0.44\n",
      "\n",
      "Alpha = 10.00\n",
      "num abs(coeff) > 1.0: 5, r-squared training: 0.32, r-squared test: 0.33\n",
      "\n",
      "Alpha = 20.00\n",
      "num abs(coeff) > 1.0: 5, r-squared training: 0.26, r-squared test: 0.27\n",
      "\n",
      "Alpha = 50.00\n",
      "num abs(coeff) > 1.0: 5, r-squared training: 0.18, r-squared test: 0.18\n",
      "\n",
      "Alpha = 100.00\n",
      "num abs(coeff) > 1.0: 5, r-squared training: 0.11, r-squared test: 0.11\n",
      "\n",
      "Alpha = 1000.00\n",
      "num abs(coeff) > 1.0: 2, r-squared training: 0.02, r-squared test: 0.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ridge regression with regularization parameter: alpha\n",
    "\n",
    "print('Ridge regression: effect of alpha regularization parameter\\n')\n",
    "for this_alpha in [0, 1, 10, 20, 50, 100, 1000]:\n",
    "    linridge = Ridge(alpha = this_alpha).fit(X_train_scaled, y_train)\n",
    "    r2_train = linridge.score(X_train_scaled, y_train)\n",
    "    r2_test = linridge.score(X_test_scaled, y_test)\n",
    "    num_coeff_bigger = np.sum(abs(linridge.coef_) > 1.0)\n",
    "    print('Alpha = {:.2f}\\nnum abs(coeff) > 1.0: {}, \\\n",
    "r-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n",
    "         .format(this_alpha, num_coeff_bigger, r2_train, r2_test))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso regression linear model intercept: 66.20384241687611\n",
      "lasso regression linear model coeff:\n",
      "[34.28143912]\n",
      "Non-zero features: 1\n",
      "R-squared score (training): 0.089\n",
      "R-squared score (test): 0.084\n",
      "\n",
      "Features with non-zero weight (sorted by absolute magnitude):\n",
      "\tHP, 34.281\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linlasso = Lasso(alpha=1, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('lasso regression linear model intercept: {}'\n",
    "     .format(linlasso.intercept_))\n",
    "print('lasso regression linear model coeff:\\n{}'\n",
    "     .format(linlasso.coef_))\n",
    "print('Non-zero features: {}'\n",
    "     .format(np.sum(linlasso.coef_ != 0)))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linlasso.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}\\n'\n",
    "     .format(linlasso.score(X_test_scaled, y_test)))\n",
    "print('Features with non-zero weight (sorted by absolute magnitude):')\n",
    "\n",
    "for e in sorted (list(zip(list(X), linlasso.coef_)),\n",
    "                key = lambda e: -abs(e[1])):\n",
    "    if e[1] != 0:\n",
    "        print('\\t{}, {:.3f}'.format(e[0], e[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso regression: effect of alpha regularization\n",
      "parameter on number of features kept in final model\n",
      "\n",
      "Alpha = 0.50\n",
      "Features kept: 1, r-squared training: 0.17, r-squared test: 0.14\n",
      "\n",
      "Alpha = 1.00\n",
      "Features kept: 1, r-squared training: 0.09, r-squared test: 0.08\n",
      "\n",
      "Alpha = 2.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n",
      "Alpha = 3.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n",
      "Alpha = 5.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n",
      "Alpha = 10.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n",
      "Alpha = 20.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n",
      "Alpha = 50.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Lasso regression: effect of alpha regularization\\n\\\n",
    "parameter on number of features kept in final model\\n')\n",
    "\n",
    "for alpha in [0.5, 1, 2, 3, 5, 10, 20, 50]:\n",
    "    linlasso = Lasso(alpha, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "    r2_train = linlasso.score(X_train_scaled, y_train)\n",
    "    r2_test = linlasso.score(X_test_scaled, y_test)\n",
    "    \n",
    "    print('Alpha = {:.2f}\\nFeatures kept: {}, r-squared training: {:.2f}, \\\n",
    "r-squared test: {:.2f}\\n'\n",
    "         .format(alpha, np.sum(linlasso.coef_ != 0), r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import KFold # import k-fold validation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLYNOMIAL DEGREE-2 W/ LINEAR REGRESSION\n",
      "(poly deg 2) linear model coeff (w):\n",
      "[ 0.00000000e+00  7.83632395e-01  5.81316973e-01  8.36628758e-02\n",
      " -3.03589224e-01  2.17935028e-01 -2.64092610e-03  4.12878549e-03\n",
      " -2.05705057e-03 -1.36610177e-03 -5.91249232e-06 -3.96493171e-03\n",
      " -2.46874919e-03  2.19326197e-03  4.71140219e-03  5.30018156e-03\n",
      " -4.63437500e-03 -2.32196045e-03  2.37022152e-03 -8.05809773e-04\n",
      " -6.17839096e-04]\n",
      "(poly deg 2) linear model intercept (b): -6.663\n",
      "(poly deg 2) R-squared score (training): 0.514\n",
      "(poly deg 2) R-squared score (test): 0.477\n",
      "(poly deg 2) RMSE score (train): 20.939\n",
      "(poly deg 2) RMSE score (test): 21.262\n",
      "\n",
      "POLYNOMIAL DEGREE-3 W/ LINEAR REGRESSION\n",
      "(poly deg 3) linear model coeff (w):\n",
      "[ 1.09582330e-10  9.63693242e-01 -1.68716025e+00  2.32220783e+00\n",
      " -1.47109972e+00 -7.42478680e-02 -9.34907237e-03  1.87006590e-02\n",
      " -2.78074030e-02  1.70931737e-02  4.16698973e-03  1.27650489e-02\n",
      " -5.55729820e-03  1.19546395e-02  1.94395528e-02 -8.89151315e-03\n",
      " -8.15594151e-04 -5.39787789e-03  2.30581295e-03 -6.37891267e-03\n",
      " -2.27424818e-03  1.09719466e-05 -3.46605342e-05  1.01164047e-04\n",
      "  4.54683442e-07 -2.38523633e-05 -4.23440735e-05  1.91696677e-04\n",
      " -1.53060933e-04 -4.38951706e-05 -2.40531362e-04  1.54108782e-04\n",
      "  2.85242718e-04 -7.13653918e-05 -8.80119423e-05 -7.63471778e-05\n",
      " -4.69359841e-05 -3.44315685e-05  3.61804724e-05 -6.76359005e-05\n",
      "  4.60722453e-05 -2.82363629e-04  1.90551047e-04  1.03285955e-04\n",
      " -9.94710864e-05 -6.23269954e-05  2.38384891e-05  1.66829933e-04\n",
      "  9.79938105e-05  4.91061040e-06 -3.34109916e-04 -1.44307008e-04\n",
      " -2.27800627e-05  9.08953659e-05  2.27827441e-04  1.89342535e-05]\n",
      "(poly deg 3) linear model intercept (b): 19.873\n",
      "(poly deg 3) R-squared score (training): 0.609\n",
      "(poly deg 3) R-squared score (test): -1.122\n",
      "(poly deg 3) RMSE score (train): 18.796\n",
      "(poly deg 3) RMSE score (test): 42.820\n",
      "\n",
      "POLYNOMIAL DEGREE-5 W/ LINEAR REGRESSION\n",
      "(poly deg 5) linear model coeff (w):\n",
      "[ 2.80817175e+03 -7.67431645e+00  3.53508781e+00  2.66288734e+01\n",
      "  3.66818142e+01 -2.00835710e+01 -2.15536924e-02  4.92799198e-01\n",
      "  3.76759674e-01 -5.11396640e-01  2.06149314e-01  1.02829334e-01\n",
      " -2.87443219e-01 -8.66152664e-01  1.06589272e-01 -5.97216748e-01\n",
      " -4.25401258e-01  9.17776323e-02 -3.37565700e-01 -4.58518810e-02\n",
      "  4.39097245e-01  1.63688866e-03 -7.24823623e-03 -1.86092436e-03\n",
      "  4.11333251e-03 -1.92415017e-04 -6.91686293e-03 -5.81807999e-03\n",
      "  1.22808831e-02 -1.65418982e-03  5.52265724e-04 -1.89868912e-03\n",
      " -5.29225647e-03  3.38378940e-03 -7.05089033e-04 -1.05955300e-03\n",
      " -2.03011707e-03 -1.37852717e-03  1.04703567e-02  3.64974667e-03\n",
      "  7.87859937e-03  1.00444448e-02 -8.44189611e-03 -1.01898818e-03\n",
      "  2.35885588e-04 -2.88834126e-04  4.97537647e-03 -6.08704291e-03\n",
      "  6.96644787e-03  9.98325217e-03  5.07435660e-03 -5.44667365e-03\n",
      "  9.43091358e-05  3.35471491e-03 -4.24236557e-03 -2.91354605e-03\n",
      " -1.70644357e-05  5.65695166e-05  4.46820391e-06 -1.32617236e-05\n",
      " -3.40772175e-06  1.01980315e-04 -8.26119227e-05 -7.70069435e-05\n",
      " -3.99653760e-05 -2.20168252e-05  1.24536126e-04  2.45151478e-05\n",
      " -4.31487769e-05 -3.28409601e-05  2.94287818e-05 -2.30506674e-05\n",
      "  2.25280530e-05  7.27764342e-06  3.91652322e-05  2.08142127e-05\n",
      "  1.05757355e-04  1.86024681e-04 -6.22085550e-05 -1.30953816e-04\n",
      "  1.14946773e-05  7.23401556e-05 -1.04375332e-04 -1.36618837e-04\n",
      " -1.39015504e-04  2.27100406e-04 -3.49000532e-05  5.32421571e-05\n",
      " -5.54313627e-05  6.88730153e-05 -1.66297358e-05  9.87042542e-06\n",
      "  5.34633709e-05 -3.37439191e-05  5.64895807e-06 -7.90749837e-05\n",
      "  5.54352155e-06 -3.81092536e-05 -7.59118790e-05 -7.97083346e-05\n",
      " -3.21835388e-05  1.83515800e-07 -5.36969488e-05  1.60032192e-05\n",
      " -1.19269443e-04 -5.77026460e-05  9.19486732e-05  5.12226678e-05\n",
      "  1.93648130e-04 -5.56698530e-05  6.87369038e-06 -3.45548176e-05\n",
      " -3.35646119e-05 -1.66768423e-05  1.89161033e-04  2.65581361e-05\n",
      " -2.83953137e-05 -6.61039284e-05 -2.52372973e-04  9.00796301e-05\n",
      "  2.29027219e-05 -1.27864538e-05  5.31083861e-05 -8.20124689e-05\n",
      "  6.03348378e-05  2.45345520e-06  6.04021153e-08 -2.91984579e-08\n",
      " -1.56104066e-07 -4.55378046e-08  3.49725626e-08 -5.72845449e-07\n",
      "  6.87841821e-07 -1.55999473e-07  1.98899713e-07 -4.35439249e-08\n",
      " -3.56385890e-07  4.19249886e-07  5.11648632e-07 -2.69485630e-07\n",
      " -1.72559155e-07 -7.19265485e-08 -3.35200149e-07  1.08832516e-06\n",
      " -2.45186935e-07  2.78258364e-07 -5.59163676e-07 -2.60742886e-07\n",
      " -2.48189168e-07  2.99646269e-07 -4.45244752e-08  6.90715289e-08\n",
      " -9.67132827e-08  8.97366235e-08  1.08507267e-06 -2.18597733e-06\n",
      "  3.76312288e-07 -8.30386007e-07  1.05540929e-06  4.34158152e-07\n",
      " -1.74188651e-07  3.92904204e-07  5.76573669e-08 -8.14428125e-07\n",
      " -5.29140200e-07  6.96156092e-07 -6.03597586e-07 -8.95918339e-07\n",
      " -6.37654079e-07  2.29582595e-06  2.65212198e-07 -1.51021843e-06\n",
      "  1.40835017e-06  1.17120723e-06 -2.57244917e-07 -1.16584688e-06\n",
      " -1.01530162e-06  9.39564599e-07 -5.79691676e-07 -3.00843558e-07\n",
      "  2.10347609e-07 -5.90901283e-09  1.21548016e-07  2.65694676e-07\n",
      " -4.44313923e-08 -1.05461992e-07 -5.67871368e-08 -5.58576429e-07\n",
      "  1.62310354e-06 -2.27250481e-07  3.61037141e-07  3.74171960e-07\n",
      " -7.77070272e-07  7.04077152e-09 -5.32933750e-07  1.11690268e-07\n",
      " -6.21778611e-08 -2.97650233e-08  2.92635551e-08 -1.25529501e-07\n",
      " -1.75146853e-07 -9.27235618e-07  8.17434434e-07  6.68213747e-07\n",
      "  6.78169158e-07 -2.80449921e-07  6.44585188e-07  4.50012316e-08\n",
      " -1.34283204e-06  1.83208754e-06 -1.47024466e-07  1.01680127e-06\n",
      " -1.61267701e-07 -1.73058216e-06  1.92833836e-07 -6.46719294e-08\n",
      " -2.26684145e-07  6.75944675e-07  5.55630146e-07 -1.90651093e-06\n",
      "  1.01546178e-06 -8.10527448e-07  4.01995205e-07  5.62451142e-07\n",
      " -5.98052346e-07  4.30256719e-10 -3.43544693e-07 -5.52374919e-08\n",
      "  5.87175651e-07  1.04381696e-07 -6.96546381e-08  1.51492376e-07\n",
      " -3.23784901e-08 -2.04684400e-07 -9.71172218e-08  4.51696112e-08\n",
      "  1.79548483e-07  1.31667855e-07 -6.87250472e-07  5.64855780e-08\n",
      "  1.73708856e-07  1.28824384e-07  4.93795492e-07  5.16889389e-07\n",
      " -5.38109483e-07 -1.05741331e-07 -3.31211627e-08 -5.75150505e-09\n",
      " -2.02596021e-07  2.33843242e-07 -9.04279784e-08  2.34385968e-08]\n",
      "(poly deg 5) linear model intercept (b): -3229.763\n",
      "(poly deg 5) R-squared score (training): 0.792\n",
      "(poly deg 5) R-squared score (test): -2495.770\n",
      "(poly deg 5) RMSE score (train): 13.716\n",
      "(poly deg 5) RMSE score (test): 1468.935\n",
      "\n",
      "POLYNOMIAL DEGREE-10 W/ LINEAR REGRESSION\n",
      "(poly deg 10) linear model coeff (w):\n",
      "[ 2.49200575e-12 -1.24200888e-11  1.24983611e-11 ...  4.48849851e-14\n",
      " -1.30415844e-13 -1.00614254e-13]\n",
      "(poly deg 10) linear model intercept (b): 23.903\n",
      "(poly deg 10) R-squared score (training): 1.000\n",
      "(poly deg 10) R-squared score (test): -12398793083561872.000\n",
      "(poly deg 10) RMSE score (train): 0.020\n",
      "(poly deg 10) RMSE score (test): 3273429738.344\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for degree in [2, 3, 5,10]:\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_F1_poly = poly.fit_transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y, random_state=0)\n",
    "    linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "    print('POLYNOMIAL DEGREE-{} W/ LINEAR REGRESSION'.format(degree))\n",
    "    print('(poly deg {}) linear model coeff (w):\\n{}'\n",
    "         .format(degree, linreg.coef_))\n",
    "    print('(poly deg {}) linear model intercept (b): {:.3f}'\n",
    "         .format(degree, linreg.intercept_))\n",
    "    print('(poly deg {}) R-squared score (training): {:.3f}'\n",
    "         .format(degree, linreg.score(X_train, y_train)))\n",
    "    print('(poly deg {}) R-squared score (test): {:.3f}'\n",
    "         .format(degree, linreg.score(X_test, y_test)))\n",
    "    print('(poly deg {}) RMSE score (train): {:.3f}'\n",
    "         .format(degree, sqrt(mean_squared_error(y_train, linreg.predict(X_train)))))\n",
    "    print('(poly deg {}) RMSE score (test): {:.3f}\\n'\n",
    "         .format(degree, sqrt(mean_squared_error(y_test, linreg.predict(X_test)))))\n",
    "\n",
    "plot_learning_curves(linreg,  X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLYNOMIAL DEGREE-2 W/ RIDGE REGRESSION\n",
      "(poly deg 2) linear model coeff (w):\n",
      "[ 0.00000000e+00  7.83561256e-01  5.81248959e-01  8.36606865e-02\n",
      " -3.03533540e-01  2.17897801e-01 -2.64075722e-03  4.12917773e-03\n",
      " -2.05708348e-03 -1.36616736e-03 -5.60359635e-06 -3.96475273e-03\n",
      " -2.46871137e-03  2.19313880e-03  4.71163080e-03  5.30016917e-03\n",
      " -4.63433789e-03 -2.32194092e-03  2.37009827e-03 -8.06078647e-04\n",
      " -6.17730005e-04]\n",
      "(poly deg 2) linear model intercept (b): -6.659\n",
      "(poly deg 2) R-squared score (training): 0.514\n",
      "(poly deg 2) R-squared score (test): 0.477\n",
      "(poly deg 2) RMSE score (train): 20.939\n",
      "(poly deg 2) RMSE score (test): 21.262\n",
      "\n",
      "POLYNOMIAL DEGREE-3 W/ RIDGE REGRESSION\n",
      "(poly deg 3) linear model coeff (w):\n",
      "[ 0.00000000e+00  9.62069563e-01 -1.68358847e+00  2.31876270e+00\n",
      " -1.46747506e+00 -7.31761141e-02 -9.33646847e-03  1.86938416e-02\n",
      " -2.77955470e-02  1.70822169e-02  4.19307280e-03  1.27382279e-02\n",
      " -5.53520793e-03  1.19199550e-02  1.94080912e-02 -8.88527363e-03\n",
      " -8.00337144e-04 -5.36569520e-03  2.28899218e-03 -6.41651992e-03\n",
      " -2.28464225e-03  1.09450454e-05 -3.46712910e-05  1.01175577e-04\n",
      "  4.60659942e-07 -2.39465657e-05 -4.22856630e-05  1.91555001e-04\n",
      " -1.52904925e-04 -4.39060630e-05 -2.40488300e-04  1.54002169e-04\n",
      "  2.85199945e-04 -7.13331527e-05 -8.79852738e-05 -7.64079188e-05\n",
      " -4.68869056e-05 -3.44658935e-05  3.62910481e-05 -6.75496007e-05\n",
      "  4.60506413e-05 -2.82316725e-04  1.90478717e-04  1.03239802e-04\n",
      " -9.93243463e-05 -6.22361387e-05  2.38197342e-05  1.66805199e-04\n",
      "  9.79920579e-05  4.93306394e-06 -3.34246577e-04 -1.44394795e-04\n",
      " -2.27501270e-05  9.10029938e-05  2.27954872e-04  1.89586572e-05]\n",
      "(poly deg 3) linear model intercept (b): 19.808\n",
      "(poly deg 3) R-squared score (training): 0.609\n",
      "(poly deg 3) R-squared score (test): -1.121\n",
      "(poly deg 3) RMSE score (train): 18.796\n",
      "(poly deg 3) RMSE score (test): 42.812\n",
      "\n",
      "POLYNOMIAL DEGREE-5 W/ RIDGE REGRESSION\n",
      "(poly deg 5) linear model coeff (w):\n",
      "[ 0.00000000e+00 -1.58495107e+00  4.63260318e+00  1.93542423e+01\n",
      "  2.16510859e+01 -1.28797807e+01 -1.26766698e-01  3.34272404e-01\n",
      "  4.46082208e-01 -3.88849402e-01 -9.33766889e-03 -1.26661523e-02\n",
      " -5.59569257e-02 -5.01149549e-01 -1.45898769e-01 -5.71348395e-01\n",
      " -3.82618428e-01  1.50323714e-01 -1.91978281e-01  7.18044283e-02\n",
      "  3.47331751e-01  2.23053148e-03 -4.77254402e-03 -2.75645051e-03\n",
      "  3.88030610e-03  1.76120317e-03 -5.68326478e-03 -5.55282995e-03\n",
      "  8.19347465e-03  1.09361934e-03  6.37856867e-04 -2.04577777e-03\n",
      " -6.54540851e-03  2.35273093e-03  4.46351560e-04  6.85755435e-04\n",
      " -6.20115525e-04 -3.90495704e-03  8.40083433e-03  7.73358740e-03\n",
      "  6.96962069e-03  6.71611281e-03 -9.04837188e-03 -2.42162689e-03\n",
      " -1.10416923e-03  8.65500610e-04  4.99270705e-03 -5.93010294e-03\n",
      "  6.41324174e-03  9.96118410e-03  5.81269558e-03 -5.58879885e-03\n",
      " -4.56803614e-04  7.78418344e-04 -4.54697659e-03 -2.21921108e-03\n",
      " -1.83425546e-05  4.76361611e-05  5.94811692e-06 -1.44695104e-05\n",
      " -8.56347276e-06  8.17152290e-05 -7.63700872e-05 -6.43629099e-05\n",
      " -6.03279668e-05 -2.17470652e-05  1.21435918e-04  4.24633345e-05\n",
      " -3.71514284e-05 -4.96953548e-05  1.70232185e-05 -2.21703902e-05\n",
      "  1.53630038e-05  2.99508060e-05  3.44796353e-05  2.25325771e-05\n",
      "  1.01071221e-04  1.76336741e-04 -4.10086473e-05 -1.17279145e-04\n",
      " -2.67288526e-06  7.36000864e-05 -1.07796456e-04 -1.37829364e-04\n",
      " -1.23005224e-04  2.03630455e-04 -1.72050361e-05  4.95180164e-05\n",
      " -5.22878332e-05  7.04628952e-05 -2.83184218e-05  3.91649879e-06\n",
      "  6.26284907e-05 -3.75747850e-05 -1.79976328e-05 -6.61569128e-05\n",
      "  3.50972335e-05 -3.76140715e-05 -6.36393542e-05 -9.50257198e-05\n",
      " -4.46677519e-05 -4.09270460e-07 -5.52249356e-05  1.90452252e-05\n",
      " -9.80410865e-05 -4.88791804e-05  9.50698675e-05  3.70554186e-05\n",
      "  2.06900121e-04 -4.77679182e-05  4.95436466e-06 -3.18524505e-05\n",
      " -4.64473143e-05 -1.74787147e-05  1.99669581e-04  4.46280269e-05\n",
      " -2.86745177e-05 -7.84495416e-05 -2.60534769e-04  7.58946999e-05\n",
      "  2.27037148e-05 -7.54592065e-06  7.29394118e-05 -7.77301023e-05\n",
      "  6.19004414e-05 -1.17642906e-07  5.81759638e-08 -3.45788211e-08\n",
      " -1.28195982e-07 -1.94604355e-08  2.74526012e-08 -4.52544424e-07\n",
      "  5.62243511e-07 -1.95172203e-07  2.39421001e-07 -2.23136447e-08\n",
      " -3.07137815e-07  2.98851944e-07  4.10959765e-07 -1.55311145e-07\n",
      " -1.27850769e-07 -9.14183716e-08 -2.46630621e-07  9.14478066e-07\n",
      " -2.02103809e-07  3.28782920e-07 -4.52741634e-07 -3.43545210e-07\n",
      " -9.60150647e-08  2.18210986e-07  9.36463296e-08 -5.06661830e-09\n",
      " -2.84835705e-08  1.08188502e-07  7.64763861e-07 -1.81126532e-06\n",
      "  2.80586430e-07 -6.84448844e-07  9.96029359e-07  2.66560817e-07\n",
      " -1.26988030e-07  3.88933030e-07  7.42276870e-08 -7.56889588e-07\n",
      " -5.83721361e-07  5.95963205e-07 -5.88920648e-07 -7.94457391e-07\n",
      " -7.01726200e-07  2.27291296e-06  2.61384990e-07 -1.43372881e-06\n",
      "  1.09805822e-06  1.29805402e-06 -4.56715614e-08 -1.17840532e-06\n",
      " -1.04707301e-06  6.76824957e-07 -4.71112359e-07 -4.40704557e-07\n",
      "  2.43841311e-07  7.77559926e-09  1.53803831e-07  2.61423553e-07\n",
      "  1.34345665e-08 -1.90476727e-07 -8.08831455e-08 -4.03444678e-07\n",
      "  1.31487405e-06 -3.33828819e-08  2.93696133e-07  3.24525433e-07\n",
      " -7.70215720e-07  1.37032037e-07 -5.00602904e-07  1.31801494e-07\n",
      " -4.99951454e-08 -4.68236930e-08  2.39468588e-08 -8.25121771e-08\n",
      " -1.51624162e-07 -9.65455805e-07  7.00046688e-07  6.52464303e-07\n",
      "  8.19853272e-07 -1.67120107e-07  6.27211786e-07 -1.27512411e-08\n",
      " -1.38503765e-06  1.62941904e-06  6.08263508e-08  1.07801014e-06\n",
      " -7.13242620e-08 -1.86838851e-06  1.40078497e-07 -1.27902784e-07\n",
      " -2.60933565e-07  7.07509532e-07  6.32875698e-07 -1.66262659e-06\n",
      "  8.84614346e-07 -8.99966940e-07  1.95748117e-07  5.73389723e-07\n",
      " -7.62303901e-07  6.94478952e-08 -2.09919628e-07 -1.17118695e-07\n",
      "  6.96910407e-07  1.31381736e-07 -6.12759510e-08  1.40236723e-07\n",
      "  1.37928843e-08 -1.96888498e-07 -9.12156324e-08 -8.57999311e-08\n",
      "  1.94013267e-07 -1.83969009e-08 -5.92088426e-07  1.37541467e-07\n",
      "  1.69555450e-07  2.09466068e-07  5.88214164e-07  4.66294584e-07\n",
      " -4.82323239e-07 -1.19078241e-07 -5.92587980e-08 -6.96252725e-08\n",
      " -2.12402196e-07  1.35560142e-07 -7.51561088e-08  2.56451267e-08]\n",
      "(poly deg 5) linear model intercept (b): -322.214\n",
      "(poly deg 5) R-squared score (training): 0.792\n",
      "(poly deg 5) R-squared score (test): -2601.783\n",
      "(poly deg 5) RMSE score (train): 13.713\n",
      "(poly deg 5) RMSE score (test): 1499.796\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=3.04438e-25): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLYNOMIAL DEGREE-10 W/ RIDGE REGRESSION\n",
      "(poly deg 10) linear model coeff (w):\n",
      "[ 0.00000000e+00  7.68787830e-30  8.38795854e-30 ...  2.72464543e-16\n",
      " -3.14148365e-17 -1.09156796e-17]\n",
      "(poly deg 10) linear model intercept (b): 55.069\n",
      "(poly deg 10) R-squared score (training): 0.846\n",
      "(poly deg 10) R-squared score (test): -11380044339.484\n",
      "(poly deg 10) RMSE score (train): 11.782\n",
      "(poly deg 10) RMSE score (test): 3136066.725\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n"
     ]
    }
   ],
   "source": [
    "for degree in [2, 3, 5, 10]:\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_F1_poly = poly.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y, random_state=0)\n",
    "    linreg = Ridge().fit(X_train, y_train)\n",
    "\n",
    "    print('POLYNOMIAL DEGREE-{} W/ RIDGE REGRESSION'.format(degree))\n",
    "    print('(poly deg {}) linear model coeff (w):\\n{}'\n",
    "         .format(degree, linreg.coef_))\n",
    "    print('(poly deg {}) linear model intercept (b): {:.3f}'\n",
    "         .format(degree, linreg.intercept_))\n",
    "    print('(poly deg {}) R-squared score (training): {:.3f}'\n",
    "         .format(degree, linreg.score(X_train, y_train)))\n",
    "    print('(poly deg {}) R-squared score (test): {:.3f}'\n",
    "         .format(degree, linreg.score(X_test, y_test)))\n",
    "    print('(poly deg {}) RMSE score (train): {:.3f}'\n",
    "         .format(degree, sqrt(mean_squared_error(y_train, linreg.predict(X_train)))))\n",
    "    print('(poly deg {}) RMSE score (test): {:.3f}\\n'\n",
    "         .format(degree, sqrt(mean_squared_error(y_test, linreg.predict(X_test)))))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DAP3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
