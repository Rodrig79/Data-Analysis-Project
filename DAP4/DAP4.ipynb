{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y2E1jbS4ZluC",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (0.23.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-learn) (1.19.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-learn) (0.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from pandas) (1.19.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (3.3.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib) (7.2.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib) (1.19.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: six in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: seaborn in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from seaborn) (1.19.1)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from seaborn) (1.1.1)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from seaborn) (1.5.2)\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from seaborn) (3.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from pandas>=0.22.0->seaborn) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from pandas>=0.22.0->seaborn) (2020.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=2.1.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=2.1.2->seaborn) (1.2.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=2.1.2->seaborn) (2020.6.20)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=2.1.2->seaborn) (7.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=0.22.0->seaborn) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: graphviz in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (0.14.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-plot in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (0.3.7)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-plot) (0.23.2)\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-plot) (3.3.1)\n",
      "Requirement already satisfied: joblib>=0.10 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-plot) (0.16.0)\n",
      "Requirement already satisfied: scipy>=0.9 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-plot) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-learn>=0.18->scikit-plot) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-learn>=0.18->scikit-plot) (1.19.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=1.4.0->scikit-plot) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=1.4.0->scikit-plot) (2020.6.20)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=1.4.0->scikit-plot) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=1.4.0->scikit-plot) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=1.4.0->scikit-plot) (7.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from python-dateutil>=2.1->matplotlib>=1.4.0->scikit-plot) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: statsmodels in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (0.11.1)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from statsmodels) (1.5.2)\n",
      "Requirement already satisfied: pandas>=0.21 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from statsmodels) (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.14 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from statsmodels) (1.19.1)\n",
      "Requirement already satisfied: patsy>=0.5 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from statsmodels) (0.5.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from pandas>=0.21->statsmodels) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from pandas>=0.21->statsmodels) (2.8.1)\n",
      "Requirement already satisfied: six in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from patsy>=0.5->statsmodels) (1.15.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#If you need to install in modules in jupyter notebook \n",
    "%pip install scikit-learn\n",
    "%pip install pandas\n",
    "%pip install matplotlib \n",
    "%pip install seaborn \n",
    "%pip install graphviz\n",
    "%pip install scikit-plot   \n",
    "%pip install statsmodels   \n",
    "\n",
    "\n",
    "#Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "import datetime # Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import scikitplot as skplt\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.linear_model import LinearRegression\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"decision_trees\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IvsV2RSPapC6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#changes the output for the print statements\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vtW9LfInZ_x2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Pull \n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Rodrig79/Machine-Learning-Data-Analysis-Project/master/rawData/pokemon.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "#Data Cleaning\n",
    "df = df[~df.Name.str.contains('Mega')] #removed pokemon with \"Mega\" in it\n",
    "df = df.drop(columns = [\"Name\",\"Type 1\",\"Type 2\",\"#\",\"Generation\"]) #Removed columns with names\n",
    "\n",
    "#normalization\n",
    "# X = df.values #returns a numpy array\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# x_scaled = min_max_scaler.fit_transform(X)\n",
    "# df=pd.DataFrame(x_scaled, columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      49 \n",
       "1      62 \n",
       "2      82 \n",
       "4      52 \n",
       "5      64 \n",
       "       .. \n",
       "794    100\n",
       "795    100\n",
       "797    110\n",
       "798    160\n",
       "799    110\n",
       "Name: Attack, Length: 751, dtype: int64"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #Splitting dataset to x and y\n",
    "# y = df.Attack\n",
    "# X = df[['Defense','HP','Sp. Atk','Sp. Def','Speed']]\n",
    "#seperating data\n",
    "df = df.dropna() \n",
    "y = df['Attack']\n",
    "X = df[['Defense','HP','Speed']] \n",
    "X.dropna()\n",
    "y.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Defense</th>\n",
       "      <th>HP</th>\n",
       "      <th>Speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>39</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>121</td>\n",
       "      <td>108</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>751 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Defense   HP  Speed\n",
       "0    49       45   45   \n",
       "1    63       60   60   \n",
       "2    83       80   80   \n",
       "4    43       39   65   \n",
       "5    58       58   80   \n",
       "..   ..       ..   ..   \n",
       "794  121      108  95   \n",
       "795  150      50   50   \n",
       "797  60       80   70   \n",
       "798  60       80   80   \n",
       "799  120      80   70   \n",
       "\n",
       "[751 rows x 3 columns]"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiVariate  Regression with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.linear_model import LinearRegression\n",
    " from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wheat dataset\n",
      "linear model intercept: 5.305692824017925\n",
      "linear model coeff:\n",
      "[0.33084547 0.39032057 0.29685746]\n",
      "R-squared score (training): 0.370\n",
      "R-squared score (test): 0.374\n",
      "RMSE: 23.266\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('Wheat dataset')\n",
    "print('linear model intercept: {}'\n",
    "     .format(linreg.intercept_))\n",
    "print('linear model coeff:\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "# make predictions on the testing set\n",
    "y_pred = linreg.predict(X_test)\n",
    "print('RMSE: {:.3f}'.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlrd in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "# excel_file = 'Folds5x2_pp.xlsx'\n",
    "# data = pd.read_excel(excel_file)\n",
    "# print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X, Y, B):\n",
    "    m = len(Y)\n",
    "    J = np.sum((X.dot(B) - Y) ** 2)/(2 * m)\n",
    "    return J\n",
    "\n",
    "def batch_gradient_descent(X, Y, B, alpha, iterations):\n",
    "    cost_history = [0] * iterations\n",
    "    m = len(Y)\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        #print(iteration)\n",
    "        # Hypothesis Values\n",
    "        h = X.dot(B)\n",
    "        # Difference b/w Hypothesis and Actual Y\n",
    "        loss = h - Y\n",
    "        # Gradient Calculation\n",
    "        gradient = X.T.dot(loss) / m\n",
    "        # Changing Values of B using Gradient\n",
    "        B = B - alpha * gradient\n",
    "        # New Cost Value\n",
    "        cost = cost_function(X, Y, B)\n",
    "        cost_history[iteration] = cost \n",
    "    return B, cost_history \n",
    "def pred(x_test, newB):\n",
    "    return x_test.dot(newB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 42)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Coefficients\n",
    "B = np.zeros(X_train.shape[1])\n",
    "alpha = .02\n",
    "iter_ = 5000\n",
    "newB, cost_history = batch_gradient_descent(X_train, y_train, B, alpha, iter_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = pred(X_test,newB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: -7.466936664319855\n"
     ]
    }
   ],
   "source": [
    "def r2(y_,y):\n",
    "    sst = np.sum((y-y.mean())**2)\n",
    "    ssr = np.sum((y_-y)**2)\n",
    "    r2 = 1-(ssr/sst)\n",
    "    return(r2)\n",
    "#----------------\n",
    "# r2 = abs(r2(y_,y_test))\n",
    "r2 = r2(y_,y_test)\n",
    "print(\"r2:\",r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-23.170235462156274"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_ = abs(pred(X_test[3],newB))\n",
    "ans_ = pred(X_test[3],newB)\n",
    "ans_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3325.8717355671743,\n",
       " 3325.626575677521,\n",
       " 3325.3817490067427,\n",
       " 3325.137255101492,\n",
       " 3324.893093509043,\n",
       " 3324.6492637772844,\n",
       " 3324.40576545472,\n",
       " 3324.1625980904687,\n",
       " 3323.919761234263,\n",
       " 3323.6772544364485,\n",
       " 3323.4350772479834,\n",
       " 3323.1932292204347,\n",
       " 3322.951709905982,\n",
       " 3322.7105188574155,\n",
       " 3322.4696556281306,\n",
       " 3322.229119772134,\n",
       " 3321.9889108440375,\n",
       " 3321.7490283990605,\n",
       " 3321.5094719930266,\n",
       " 3321.270241182365,\n",
       " 3321.031335524108,\n",
       " 3320.7927545758907,\n",
       " 3320.554497895952,\n",
       " 3320.3165650431292,\n",
       " 3320.078955576863,\n",
       " 3319.8416690571917,\n",
       " 3319.604705044754,\n",
       " 3319.3680631007865,\n",
       " 3319.1317427871213,\n",
       " 3318.8957436661885,\n",
       " 3318.660065301015,\n",
       " 3318.4247072552193,\n",
       " 3318.189669093016,\n",
       " 3317.9549503792123,\n",
       " 3317.7205506792084,\n",
       " 3317.486469558996,\n",
       " 3317.252706585156,\n",
       " 3317.019261324862,\n",
       " 3316.786133345874,\n",
       " 3316.5533222165413,\n",
       " 3316.3208275058028,\n",
       " 3316.088648783181,\n",
       " 3315.856785618786,\n",
       " 3315.625237583313,\n",
       " 3315.394004248041,\n",
       " 3315.1630851848336,\n",
       " 3314.9324799661363,\n",
       " 3314.7021881649766,\n",
       " 3314.4722093549635,\n",
       " 3314.2425431102865,\n",
       " 3314.013189005714,\n",
       " 3313.7841466165937,\n",
       " 3313.5554155188524,\n",
       " 3313.326995288992,\n",
       " 3313.0988855040923,\n",
       " 3312.8710857418077,\n",
       " 3312.64359558037,\n",
       " 3312.416414598581,\n",
       " 3312.18954237582,\n",
       " 3311.962978492036,\n",
       " 3311.7367225277508,\n",
       " 3311.5107740640583,\n",
       " 3311.2851326826194,\n",
       " 3311.0597979656673,\n",
       " 3310.8347694960025,\n",
       " 3310.610046856996,\n",
       " 3310.385629632581,\n",
       " 3310.161517407262,\n",
       " 3309.937709766106,\n",
       " 3309.7142062947455,\n",
       " 3309.4910065793765,\n",
       " 3309.268110206761,\n",
       " 3309.04551676422,\n",
       " 3308.823225839637,\n",
       " 3308.6012370214594,\n",
       " 3308.37954989869,\n",
       " 3308.158164060896,\n",
       " 3307.937079098199,\n",
       " 3307.71629460128,\n",
       " 3307.4958101613793,\n",
       " 3307.275625370291,\n",
       " 3307.0557398203655,\n",
       " 3306.8361531045075,\n",
       " 3306.6168648161783,\n",
       " 3306.3978745493896,\n",
       " 3306.179181898706,\n",
       " 3305.9607864592463,\n",
       " 3305.7426878266792,\n",
       " 3305.524885597222,\n",
       " 3305.307379367645,\n",
       " 3305.090168735264,\n",
       " 3304.8732532979443,\n",
       " 3304.6566326541,\n",
       " 3304.44030640269,\n",
       " 3304.224274143219,\n",
       " 3304.0085354757384,\n",
       " 3303.7930900008423,\n",
       " 3303.5779373196697,\n",
       " 3303.3630770339014,\n",
       " 3303.148508745761,\n",
       " 3302.9342320580145,\n",
       " 3302.720246573966,\n",
       " 3302.506551897462,\n",
       " 3302.2931476328877,\n",
       " 3302.0800333851666,\n",
       " 3301.867208759759,\n",
       " 3301.6546733626637,\n",
       " 3301.4424268004145,\n",
       " 3301.230468680082,\n",
       " 3301.018798609271,\n",
       " 3300.8074161961204,\n",
       " 3300.5963210493032,\n",
       " 3300.385512778023,\n",
       " 3300.1749909920177,\n",
       " 3299.9647553015548,\n",
       " 3299.7548053174337,\n",
       " 3299.545140650983,\n",
       " 3299.335760914059,\n",
       " 3299.1266657190477,\n",
       " 3298.9178546788635,\n",
       " 3298.7093274069452,\n",
       " 3298.50108351726,\n",
       " 3298.2931226243,\n",
       " 3298.0854443430812,\n",
       " 3297.8780482891443,\n",
       " 3297.6709340785524,\n",
       " 3297.464101327894,\n",
       " 3297.2575496542754,\n",
       " 3297.0512786753266,\n",
       " 3296.845288009199,\n",
       " 3296.6395772745605,\n",
       " 3296.4341460906007,\n",
       " 3296.228994077027,\n",
       " 3296.0241208540633,\n",
       " 3295.819526042452,\n",
       " 3295.6152092634507,\n",
       " 3295.411170138833,\n",
       " 3295.207408290887,\n",
       " 3295.003923342415,\n",
       " 3294.800714916732,\n",
       " 3294.5977826376675,\n",
       " 3294.3951261295624,\n",
       " 3294.1927450172675,\n",
       " 3293.9906389261455,\n",
       " 3293.7888074820685,\n",
       " 3293.587250311419,\n",
       " 3293.385967041086,\n",
       " 3293.1849572984684,\n",
       " 3292.984220711471,\n",
       " 3292.783756908506,\n",
       " 3292.58356551849,\n",
       " 3292.383646170845,\n",
       " 3292.1839984954995,\n",
       " 3291.9846221228827,\n",
       " 3291.7855166839295,\n",
       " 3291.5866818100753,\n",
       " 3291.388117133257,\n",
       " 3291.189822285914,\n",
       " 3290.9917969009857,\n",
       " 3290.7940406119096,\n",
       " 3290.596553052623,\n",
       " 3290.3993338575638,\n",
       " 3290.202382661663,\n",
       " 3290.0056991003507,\n",
       " 3289.809282809555,\n",
       " 3289.613133425697,\n",
       " 3289.4172505856936,\n",
       " 3289.221633926956,\n",
       " 3289.0262830873894,\n",
       " 3288.831197705389,\n",
       " 3288.6363774198467,\n",
       " 3288.4418218701426,\n",
       " 3288.2475306961487,\n",
       " 3288.0535035382277,\n",
       " 3287.8597400372305,\n",
       " 3287.666239834498,\n",
       " 3287.4730025718586,\n",
       " 3287.2800278916293,\n",
       " 3287.087315436612,\n",
       " 3286.894864850096,\n",
       " 3286.702675775857,\n",
       " 3286.510747858154,\n",
       " 3286.319080741731,\n",
       " 3286.1276740718145,\n",
       " 3285.9365274941156,\n",
       " 3285.745640654827,\n",
       " 3285.555013200622,\n",
       " 3285.364644778656,\n",
       " 3285.174535036563,\n",
       " 3284.9846836224597,\n",
       " 3284.795090184938,\n",
       " 3284.6057543730717,\n",
       " 3284.4166758364095,\n",
       " 3284.227854224978,\n",
       " 3284.03928918928,\n",
       " 3283.8509803802954,\n",
       " 3283.6629274494767,\n",
       " 3283.475130048753,\n",
       " 3283.2875878305254,\n",
       " 3283.1003004476706,\n",
       " 3282.9132675535343,\n",
       " 3282.7264888019367,\n",
       " 3282.5399638471695,\n",
       " 3282.353692343991,\n",
       " 3282.1676739476343,\n",
       " 3281.9819083137995,\n",
       " 3281.796395098655,\n",
       " 3281.6111339588374,\n",
       " 3281.4261245514526,\n",
       " 3281.2413665340696,\n",
       " 3281.056859564726,\n",
       " 3280.8726033019248,\n",
       " 3280.688597404633,\n",
       " 3280.5048415322817,\n",
       " 3280.321335344766,\n",
       " 3280.1380785024444,\n",
       " 3279.955070666136,\n",
       " 3279.772311497124,\n",
       " 3279.58980065715,\n",
       " 3279.4075378084185,\n",
       " 3279.2255226135912,\n",
       " 3279.043754735792,\n",
       " 3278.862233838599,\n",
       " 3278.680959586054,\n",
       " 3278.49993164265,\n",
       " 3278.3191496733416,\n",
       " 3278.138613343536,\n",
       " 3277.9583223190975,\n",
       " 3277.778276266345,\n",
       " 3277.598474852052,\n",
       " 3277.4189177434437,\n",
       " 3277.2396046082,\n",
       " 3277.060535114453,\n",
       " 3276.8817089307868,\n",
       " 3276.7031257262333,\n",
       " 3276.52478517028,\n",
       " 3276.3466869328604,\n",
       " 3276.168830684359,\n",
       " 3275.9912160956083,\n",
       " 3275.8138428378898,\n",
       " 3275.63671058293,\n",
       " 3275.459819002905,\n",
       " 3275.283167770436,\n",
       " 3275.106756558589,\n",
       " 3274.930585040876,\n",
       " 3274.754652891253,\n",
       " 3274.5789597841203,\n",
       " 3274.4035053943203,\n",
       " 3274.2282893971387,\n",
       " 3274.0533114683035,\n",
       " 3273.8785712839826,\n",
       " 3273.704068520786,\n",
       " 3273.529802855764,\n",
       " 3273.3557739664056,\n",
       " 3273.181981530639,\n",
       " 3273.0084252268302,\n",
       " 3272.8351047337856,\n",
       " 3272.6620197307448,\n",
       " 3272.4891698973865,\n",
       " 3272.316554913825,\n",
       " 3272.1441744606104,\n",
       " 3271.972028218726,\n",
       " 3271.8001158695906,\n",
       " 3271.628437095056,\n",
       " 3271.4569915774086,\n",
       " 3271.285778999365,\n",
       " 3271.1147990440745,\n",
       " 3270.944051395118,\n",
       " 3270.773535736507,\n",
       " 3270.603251752683,\n",
       " 3270.433199128516,\n",
       " 3270.263377549307,\n",
       " 3270.0937867007833,\n",
       " 3269.924426269101,\n",
       " 3269.755295940843,\n",
       " 3269.586395403019,\n",
       " 3269.4177243430654,\n",
       " 3269.249282448842,\n",
       " 3269.0810694086354,\n",
       " 3268.9130849111552,\n",
       " 3268.745328645535,\n",
       " 3268.5778003013334,\n",
       " 3268.410499568528,\n",
       " 3268.24342613752,\n",
       " 3268.076579699134,\n",
       " 3267.9099599446117,\n",
       " 3267.743566565618,\n",
       " 3267.5773992542363,\n",
       " 3267.411457702968,\n",
       " 3267.245741604736,\n",
       " 3267.080250652878,\n",
       " 3266.914984541151,\n",
       " 3266.7499429637273,\n",
       " 3266.5851256151973,\n",
       " 3266.420532190566,\n",
       " 3266.256162385253,\n",
       " 3266.092015895094,\n",
       " 3265.928092416337,\n",
       " 3265.7643916456445,\n",
       " 3265.6009132800914,\n",
       " 3265.437657017165,\n",
       " 3265.274622554764,\n",
       " 3265.1118095911993,\n",
       " 3264.9492178251917,\n",
       " 3264.786846955872,\n",
       " 3264.6246966827803,\n",
       " 3264.4627667058658,\n",
       " 3264.3010567254864,\n",
       " 3264.139566442408,\n",
       " 3263.978295557803,\n",
       " 3263.8172437732505,\n",
       " 3263.6564107907375,\n",
       " 3263.4957963126535,\n",
       " 3263.3354000417958,\n",
       " 3263.1752216813657,\n",
       " 3263.0152609349666,\n",
       " 3262.8555175066076,\n",
       " 3262.6959911006998,\n",
       " 3262.5366814220556,\n",
       " 3262.3775881758916,\n",
       " 3262.2187110678224,\n",
       " 3262.060049803867,\n",
       " 3261.9016040904403,\n",
       " 3261.74337363436,\n",
       " 3261.585358142842,\n",
       " 3261.4275573235,\n",
       " 3261.2699708843465,\n",
       " 3261.112598533791,\n",
       " 3260.9554399806393,\n",
       " 3260.798494934095,\n",
       " 3260.6417631037552,\n",
       " 3260.4852441996154,\n",
       " 3260.3289379320627,\n",
       " 3260.1728440118795,\n",
       " 3260.0169621502437,\n",
       " 3259.861292058723,\n",
       " 3259.7058334492795,\n",
       " 3259.550586034268,\n",
       " 3259.395549526432,\n",
       " 3259.24072363891,\n",
       " 3259.0861080852274,\n",
       " 3258.9317025793016,\n",
       " 3258.7775068354385,\n",
       " 3258.6235205683324,\n",
       " 3258.4697434930667,\n",
       " 3258.316175325113,\n",
       " 3258.1628157803284,\n",
       " 3258.009664574959,\n",
       " 3257.856721425635,\n",
       " 3257.703986049373,\n",
       " 3257.5514581635753,\n",
       " 3257.399137486029,\n",
       " 3257.2470237349025,\n",
       " 3257.0951166287527,\n",
       " 3256.9434158865156,\n",
       " 3256.791921227511,\n",
       " 3256.6406323714414,\n",
       " 3256.489549038389,\n",
       " 3256.3386709488173,\n",
       " 3256.1879978235734,\n",
       " 3256.0375293838797,\n",
       " 3255.8872653513417,\n",
       " 3255.7372054479415,\n",
       " 3255.5873493960403,\n",
       " 3255.4376969183772,\n",
       " 3255.288247738069,\n",
       " 3255.13900157861,\n",
       " 3254.989958163869,\n",
       " 3254.8411172180895,\n",
       " 3254.6924784658954,\n",
       " 3254.5440416322813,\n",
       " 3254.395806442616,\n",
       " 3254.247772622645,\n",
       " 3254.0999398984827,\n",
       " 3253.9523079966207,\n",
       " 3253.80487664392,\n",
       " 3253.657645567615,\n",
       " 3253.5106144953097,\n",
       " 3253.3637831549804,\n",
       " 3253.2171512749715,\n",
       " 3253.070718584,\n",
       " 3252.9244848111503,\n",
       " 3252.778449685875,\n",
       " 3252.632612937997,\n",
       " 3252.4869742977044,\n",
       " 3252.341533495555,\n",
       " 3252.196290262471,\n",
       " 3252.051244329742,\n",
       " 3251.906395429024,\n",
       " 3251.761743292337,\n",
       " 3251.617287652066,\n",
       " 3251.4730282409605,\n",
       " 3251.3289647921324,\n",
       " 3251.18509703906,\n",
       " 3251.0414247155813,\n",
       " 3250.897947555897,\n",
       " 3250.7546652945707,\n",
       " 3250.611577666527,\n",
       " 3250.46868440705,\n",
       " 3250.3259852517863,\n",
       " 3250.1834799367393,\n",
       " 3250.0411681982746,\n",
       " 3249.8990497731156,\n",
       " 3249.757124398342,\n",
       " 3249.6153918113946,\n",
       " 3249.4738517500705,\n",
       " 3249.332503952521,\n",
       " 3249.1913481572583,\n",
       " 3249.050384103147,\n",
       " 3248.909611529408,\n",
       " 3248.7690301756174,\n",
       " 3248.628639781706,\n",
       " 3248.4884400879587,\n",
       " 3248.3484308350126,\n",
       " 3248.208611763859,\n",
       " 3248.068982615841,\n",
       " 3247.9295431326536,\n",
       " 3247.790293056344,\n",
       " 3247.6512321293103,\n",
       " 3247.512360094302,\n",
       " 3247.373676694416,\n",
       " 3247.2351816731016,\n",
       " 3247.0968747741563,\n",
       " 3246.9587557417253,\n",
       " 3246.8208243203044,\n",
       " 3246.6830802547356,\n",
       " 3246.5455232902063,\n",
       " 3246.408153172255,\n",
       " 3246.2709696467628,\n",
       " 3246.1339724599584,\n",
       " 3245.9971613584153,\n",
       " 3245.860536089052,\n",
       " 3245.724096399132,\n",
       " 3245.587842036262,\n",
       " 3245.451772748392,\n",
       " 3245.3158882838156,\n",
       " 3245.1801883911694,\n",
       " 3245.044672819432,\n",
       " 3244.909341317921,\n",
       " 3244.774193636299,\n",
       " 3244.639229524568,\n",
       " 3244.5044487330683,\n",
       " 3244.3698510124827,\n",
       " 3244.2354361138314,\n",
       " 3244.1012037884752,\n",
       " 3243.967153788112,\n",
       " 3243.8332858647764,\n",
       " 3243.699599770843,\n",
       " 3243.5660952590238,\n",
       " 3243.4327720823635,\n",
       " 3243.2996299942465,\n",
       " 3243.166668748392,\n",
       " 3243.033888098853,\n",
       " 3242.9012878000185,\n",
       " 3242.7688676066123,\n",
       " 3242.6366272736905,\n",
       " 3242.5045665566427,\n",
       " 3242.3726852111927,\n",
       " 3242.240982993396,\n",
       " 3242.1094596596395,\n",
       " 3241.9781149666433,\n",
       " 3241.846948671456,\n",
       " 3241.7159605314596,\n",
       " 3241.585150304364,\n",
       " 3241.454517748211,\n",
       " 3241.3240626213696,\n",
       " 3241.193784682539,\n",
       " 3241.0636836907465,\n",
       " 3240.9337594053472,\n",
       " 3240.8040115860235,\n",
       " 3240.6744399927848,\n",
       " 3240.5450443859677,\n",
       " 3240.4158245262356,\n",
       " 3240.286780174575,\n",
       " 3240.1579110923008,\n",
       " 3240.0292170410507,\n",
       " 3239.900697782787,\n",
       " 3239.772353079797,\n",
       " 3239.644182694691,\n",
       " 3239.5161863904013,\n",
       " 3239.3883639301844,\n",
       " 3239.260715077618,\n",
       " 3239.133239596602,\n",
       " 3239.0059372513574,\n",
       " 3238.8788078064267,\n",
       " 3238.751851026672,\n",
       " 3238.6250666772753,\n",
       " 3238.4984545237394,\n",
       " 3238.3720143318847,\n",
       " 3238.2457458678514,\n",
       " 3238.1196488980963,\n",
       " 3237.9937231893973,\n",
       " 3237.8679685088473,\n",
       " 3237.7423846238553,\n",
       " 3237.616971302149,\n",
       " 3237.491728311772,\n",
       " 3237.366655421081,\n",
       " 3237.241752398751,\n",
       " 3237.1170190137705,\n",
       " 3236.992455035443,\n",
       " 3236.8680602333848,\n",
       " 3236.7438343775266,\n",
       " 3236.6197772381115,\n",
       " 3236.4958885856972,\n",
       " 3236.3721681911497,\n",
       " 3236.248615825652,\n",
       " 3236.1252312606953,\n",
       " 3236.002014268081,\n",
       " 3235.8789646199234,\n",
       " 3235.756082088646,\n",
       " 3235.6333664469807,\n",
       " 3235.510817467972,\n",
       " 3235.3884349249693,\n",
       " 3235.266218591634,\n",
       " 3235.144168241932,\n",
       " 3235.02228365014,\n",
       " 3234.9005645908396,\n",
       " 3234.779010838921,\n",
       " 3234.657622169579,\n",
       " 3234.5363983583165,\n",
       " 3234.415339180939,\n",
       " 3234.2944444135605,\n",
       " 3234.173713832596,\n",
       " 3234.0531472147686,\n",
       " 3233.932744337101,\n",
       " 3233.8125049769233,\n",
       " 3233.692428911867,\n",
       " 3233.572515919866,\n",
       " 3233.4527657791564,\n",
       " 3233.3331782682767,\n",
       " 3233.213753166066,\n",
       " 3233.094490251665,\n",
       " 3232.9753893045145,\n",
       " 3232.8564501043566,\n",
       " 3232.737672431231,\n",
       " 3232.6190560654786,\n",
       " 3232.500600787738,\n",
       " 3232.382306378949,\n",
       " 3232.264172620345,\n",
       " 3232.1461992934605,\n",
       " 3232.028386180127,\n",
       " 3231.910733062471,\n",
       " 3231.7932397229183,\n",
       " 3231.675905944188,\n",
       " 3231.5587315092966,\n",
       " 3231.441716201555,\n",
       " 3231.32485980457,\n",
       " 3231.208162102243,\n",
       " 3231.0916228787673,\n",
       " 3230.975241918632,\n",
       " 3230.859019006619,\n",
       " 3230.742953927803,\n",
       " 3230.6270464675513,\n",
       " 3230.511296411522,\n",
       " 3230.3957035456674,\n",
       " 3230.280267656229,\n",
       " 3230.16498852974,\n",
       " 3230.049865953024,\n",
       " 3229.9348997131947,\n",
       " 3229.8200895976556,\n",
       " 3229.7054353940994,\n",
       " 3229.5909368905072,\n",
       " 3229.4765938751502,\n",
       " 3229.3624061365863,\n",
       " 3229.248373463661,\n",
       " 3229.134495645508,\n",
       " 3229.0207724715483,\n",
       " 3228.907203731488,\n",
       " 3228.793789215321,\n",
       " 3228.6805287133247,\n",
       " 3228.567422016065,\n",
       " 3228.454468914391,\n",
       " 3228.3416691994357,\n",
       " 3228.229022662618,\n",
       " 3228.116529095639,\n",
       " 3228.004188290485,\n",
       " 3227.8920000394255,\n",
       " 3227.7799641350102,\n",
       " 3227.6680803700724,\n",
       " 3227.5563485377297,\n",
       " 3227.4447684313764,\n",
       " 3227.3333398446925,\n",
       " 3227.222062571637,\n",
       " 3227.1109364064478,\n",
       " 3226.9999611436456,\n",
       " 3226.8891365780282,\n",
       " 3226.778462504675,\n",
       " 3226.667938718943,\n",
       " 3226.557565016467,\n",
       " 3226.4473411931604,\n",
       " 3226.337267045216,\n",
       " 3226.2273423691013,\n",
       " 3226.1175669615623,\n",
       " 3226.0079406196214,\n",
       " 3225.8984631405774,\n",
       " 3225.789134322005,\n",
       " 3225.679953961753,\n",
       " 3225.5709218579473,\n",
       " 3225.462037808987,\n",
       " 3225.353301613546,\n",
       " 3225.2447130705723,\n",
       " 3225.1362719792887,\n",
       " 3225.0279781391887,\n",
       " 3224.9198313500397,\n",
       " 3224.8118314118833,\n",
       " 3224.7039781250305,\n",
       " 3224.596271290066,\n",
       " 3224.488710707844,\n",
       " 3224.381296179492,\n",
       " 3224.2740275064066,\n",
       " 3224.1669044902555,\n",
       " 3224.0599269329755,\n",
       " 3223.9530946367727,\n",
       " 3223.846407404124,\n",
       " 3223.7398650377736,\n",
       " 3223.6334673407346,\n",
       " 3223.5272141162877,\n",
       " 3223.421105167982,\n",
       " 3223.3151402996336,\n",
       " 3223.2093193153264,\n",
       " 3223.1036420194096,\n",
       " 3222.9981082164986,\n",
       " 3222.892717711476,\n",
       " 3222.7874703094894,\n",
       " 3222.68236581595,\n",
       " 3222.577404036537,\n",
       " 3222.4725847771906,\n",
       " 3222.3679078441173,\n",
       " 3222.2633730437874,\n",
       " 3222.1589801829327,\n",
       " 3222.054729068549,\n",
       " 3221.9506195078966,\n",
       " 3221.846651308494,\n",
       " 3221.742824278126,\n",
       " 3221.6391382248357,\n",
       " 3221.5355929569296,\n",
       " 3221.432188282973,\n",
       " 3221.328924011794,\n",
       " 3221.225799952479,\n",
       " 3221.1228159143748,\n",
       " 3221.019971707088,\n",
       " 3220.917267140484,\n",
       " 3220.8147020246874,\n",
       " 3220.71227617008,\n",
       " 3220.6099893873024,\n",
       " 3220.507841487253,\n",
       " 3220.4058322810874,\n",
       " 3220.303961580218,\n",
       " 3220.202229196314,\n",
       " 3220.100634941301,\n",
       " 3219.999178627361,\n",
       " 3219.8978600669298,\n",
       " 3219.7966790726996,\n",
       " 3219.6956354576187,\n",
       " 3219.5947290348877,\n",
       " 3219.493959617963,\n",
       " 3219.393327020554,\n",
       " 3219.292831056624,\n",
       " 3219.1924715403893,\n",
       " 3219.0922482863184,\n",
       " 3218.9921611091336,\n",
       " 3218.8922098238086,\n",
       " 3218.7923942455686,\n",
       " 3218.6927141898914,\n",
       " 3218.5931694725036,\n",
       " 3218.493759909385,\n",
       " 3218.394485316765,\n",
       " 3218.2953455111224,\n",
       " 3218.1963403091863,\n",
       " 3218.0974695279347,\n",
       " 3217.998732984596,\n",
       " 3217.9001304966455,\n",
       " 3217.8016618818083,\n",
       " 3217.703326958057,\n",
       " 3217.6051255436114,\n",
       " 3217.5070574569395,\n",
       " 3217.409122516756,\n",
       " 3217.3113205420223,\n",
       " 3217.213651351946,\n",
       " 3217.1161147659795,\n",
       " 3217.0187106038247,\n",
       " 3216.921438685425,\n",
       " 3216.8242988309703,\n",
       " 3216.727290860895,\n",
       " 3216.630414595878,\n",
       " 3216.5336698568417,\n",
       " 3216.4370564649535,\n",
       " 3216.3405742416226,\n",
       " 3216.2442230085016,\n",
       " 3216.1480025874866,\n",
       " 3216.0519128007154,\n",
       " 3215.955953470568,\n",
       " 3215.8601244196657,\n",
       " 3215.7644254708716,\n",
       " 3215.6688564472906,\n",
       " 3215.573417172266,\n",
       " 3215.4781074693838,\n",
       " 3215.382927162469,\n",
       " 3215.2878760755866,\n",
       " 3215.1929540330407,\n",
       " 3215.098160859375,\n",
       " 3215.0034963793723,\n",
       " 3214.908960418052,\n",
       " 3214.814552800673,\n",
       " 3214.7202733527324,\n",
       " 3214.6261218999643,\n",
       " 3214.532098268339,\n",
       " 3214.438202284064,\n",
       " 3214.344433773585,\n",
       " 3214.250792563582,\n",
       " 3214.157278480971,\n",
       " 3214.0638913529046,\n",
       " 3213.9706310067686,\n",
       " 3213.8774972701863,\n",
       " 3213.784489971014,\n",
       " 3213.6916089373417,\n",
       " 3213.5988539974956,\n",
       " 3213.506224980032,\n",
       " 3213.4137217137445,\n",
       " 3213.3213440276563,\n",
       " 3213.2290917510254,\n",
       " 3213.1369647133406,\n",
       " 3213.0449627443245,\n",
       " 3212.9530856739293,\n",
       " 3212.861333332341,\n",
       " 3212.7697055499743,\n",
       " 3212.6782021574754,\n",
       " 3212.586822985723,\n",
       " 3212.4955678658225,\n",
       " 3212.404436629111,\n",
       " 3212.3134291071556,\n",
       " 3212.2225451317513,\n",
       " 3212.131784534923,\n",
       " 3212.0411471489238,\n",
       " 3211.950632806235,\n",
       " 3211.8602413395647,\n",
       " 3211.7699725818516,\n",
       " 3211.6798263662586,\n",
       " 3211.589802526177,\n",
       " 3211.4999008952263,\n",
       " 3211.4101213072495,\n",
       " 3211.3204635963166,\n",
       " 3211.2309275967245,\n",
       " 3211.1415131429962,\n",
       " 3211.052220069876,\n",
       " 3210.963048212337,\n",
       " 3210.873997405575,\n",
       " 3210.785067485012,\n",
       " 3210.6962582862907,\n",
       " 3210.60756964528,\n",
       " 3210.519001398071,\n",
       " 3210.430553380979,\n",
       " 3210.3422254305406,\n",
       " 3210.2540173835155,\n",
       " 3210.1659290768853,\n",
       " 3210.0779603478545,\n",
       " 3209.9901110338465,\n",
       " 3209.9023809725095,\n",
       " 3209.81477000171,\n",
       " 3209.727277959535,\n",
       " 3209.639904684294,\n",
       " 3209.5526500145143,\n",
       " 3209.4655137889445,\n",
       " 3209.3784958465517,\n",
       " 3209.2915960265213,\n",
       " 3209.2048141682603,\n",
       " 3209.118150111391,\n",
       " 3209.031603695757,\n",
       " 3208.9451747614166,\n",
       " 3208.8588631486487,\n",
       " 3208.7726686979468,\n",
       " 3208.686591250024,\n",
       " 3208.600630645808,\n",
       " 3208.5147867264454,\n",
       " 3208.429059333296,\n",
       " 3208.3434483079377,\n",
       " 3208.257953492163,\n",
       " 3208.1725747279797,\n",
       " 3208.0873118576105,\n",
       " 3208.002164723494,\n",
       " 3207.917133168282,\n",
       " 3207.83221703484,\n",
       " 3207.747416166248,\n",
       " 3207.6627304057997,\n",
       " 3207.578159597002,\n",
       " 3207.4937035835746,\n",
       " 3207.4093622094497,\n",
       " 3207.3251353187716,\n",
       " 3207.241022755896,\n",
       " 3207.157024365394,\n",
       " 3207.0731399920437,\n",
       " 3206.9893694808366,\n",
       " 3206.9057126769753,\n",
       " 3206.822169425872,\n",
       " 3206.7387395731494,\n",
       " 3206.6554229646417,\n",
       " 3206.5722194463906,\n",
       " 3206.48912886465,\n",
       " 3206.40615106588,\n",
       " 3206.323285896752,\n",
       " 3206.2405332041444,\n",
       " 3206.157892835146,\n",
       " 3206.0753646370504,\n",
       " 3205.992948457363,\n",
       " 3205.9106441437934,\n",
       " 3205.828451544259,\n",
       " 3205.746370506887,\n",
       " 3205.664400880007,\n",
       " 3205.582542512157,\n",
       " 3205.500795252083,\n",
       " 3205.4191589487336,\n",
       " 3205.337633451263,\n",
       " 3205.2562186090345,\n",
       " 3205.1749142716117,\n",
       " 3205.093720288766,\n",
       " 3205.0126365104716,\n",
       " 3204.9316627869066,\n",
       " 3204.850798968456,\n",
       " 3204.7700449057033,\n",
       " 3204.689400449441,\n",
       " 3204.6088654506593,\n",
       " 3204.528439760555,\n",
       " 3204.448123230525,\n",
       " 3204.3679157121705,\n",
       " 3204.2878170572926,\n",
       " 3204.2078271178952,\n",
       " 3204.1279457461833,\n",
       " 3204.048172794562,\n",
       " 3203.9685081156395,\n",
       " 3203.8889515622236,\n",
       " 3203.8095029873207,\n",
       " 3203.730162244139,\n",
       " 3203.650929186086,\n",
       " 3203.5718036667695,\n",
       " 3203.4927855399937,\n",
       " 3203.4138746597655,\n",
       " 3203.3350708802873,\n",
       " 3203.256374055962,\n",
       " 3203.1777840413893,\n",
       " 3203.0993006913673,\n",
       " 3203.020923860892,\n",
       " 3202.9426534051554,\n",
       " 3202.864489179548,\n",
       " 3202.7864310396562,\n",
       " 3202.708478841264,\n",
       " 3202.6306324403495,\n",
       " 3202.552891693089,\n",
       " 3202.475256455852,\n",
       " 3202.3977265852077,\n",
       " 3202.320301937915,\n",
       " 3202.2429823709317,\n",
       " 3202.1657677414087,\n",
       " 3202.088657906692,\n",
       " 3202.01165272432,\n",
       " 3201.934752052027,\n",
       " 3201.8579557477406,\n",
       " 3201.78126366958,\n",
       " 3201.7046756758587,\n",
       " 3201.628191625083,\n",
       " 3201.551811375952,\n",
       " 3201.4755347873565,\n",
       " 3201.399361718379,\n",
       " 3201.3232920282944,\n",
       " 3201.247325576569,\n",
       " 3201.171462222859,\n",
       " 3201.0957018270146,\n",
       " 3201.0200442490736,\n",
       " 3200.9444893492655,\n",
       " 3200.8690369880105,\n",
       " 3200.793687025917,\n",
       " 3200.7184393237844,\n",
       " 3200.6432937426025,\n",
       " 3200.5682501435476,\n",
       " 3200.493308387987,\n",
       " 3200.418468337476,\n",
       " 3200.343729853757,\n",
       " 3200.2690927987633,\n",
       " 3200.1945570346134,\n",
       " 3200.1201224236147,\n",
       " 3200.045788828262,\n",
       " 3199.9715561112366,\n",
       " 3199.8974241354067,\n",
       " 3199.8233927638275,\n",
       " 3199.7494618597407,\n",
       " 3199.675631286574,\n",
       " 3199.6019009079387,\n",
       " 3199.5282705876357,\n",
       " 3199.454740189648,\n",
       " 3199.3813095781456,\n",
       " 3199.307978617481,\n",
       " 3199.234747172194,\n",
       " 3199.1616151070075,\n",
       " 3199.088582286828,\n",
       " 3199.015648576745,\n",
       " 3198.9428138420353,\n",
       " 3198.8700779481546,\n",
       " 3198.7974407607444,\n",
       " 3198.7249021456273,\n",
       " 3198.6524619688107,\n",
       " 3198.580120096481,\n",
       " 3198.5078763950105,\n",
       " 3198.4357307309497,\n",
       " 3198.363682971033,\n",
       " 3198.2917329821757,\n",
       " 3198.219880631474,\n",
       " 3198.1481257862047,\n",
       " 3198.0764683138254,\n",
       " 3198.0049080819736,\n",
       " 3197.9334449584676,\n",
       " 3197.862078811305,\n",
       " 3197.790809508664,\n",
       " 3197.7196369189,\n",
       " 3197.6485609105503,\n",
       " 3197.5775813523283,\n",
       " 3197.5066981131286,\n",
       " 3197.4359110620217,\n",
       " 3197.3652200682586,\n",
       " 3197.294625001267,\n",
       " 3197.2241257306514,\n",
       " 3197.153722126195,\n",
       " 3197.0834140578586,\n",
       " 3197.013201395778,\n",
       " 3196.9430840102677,\n",
       " 3196.8730617718174,\n",
       " 3196.8031345510935,\n",
       " 3196.733302218938,\n",
       " 3196.663564646369,\n",
       " 3196.59392170458,\n",
       " 3196.52437326494,\n",
       " 3196.4549191989922,\n",
       " 3196.3855593784556,\n",
       " 3196.3162936752224,\n",
       " 3196.2471219613603,\n",
       " 3196.178044109112,\n",
       " 3196.1090599908907,\n",
       " 3196.040169479286,\n",
       " 3195.9713724470603,\n",
       " 3195.902668767148,\n",
       " 3195.8340583126587,\n",
       " 3195.765540956872,\n",
       " 3195.6971165732407,\n",
       " 3195.628785035392,\n",
       " 3195.560546217121,\n",
       " 3195.492399992398,\n",
       " 3195.424346235364,\n",
       " 3195.3563848203294,\n",
       " 3195.2885156217767,\n",
       " 3195.2207385143606,\n",
       " 3195.1530533729047,\n",
       " 3195.0854600724037,\n",
       " 3195.0179584880207,\n",
       " 3194.9505484950914,\n",
       " 3194.883229969119,\n",
       " 3194.8160027857766,\n",
       " 3194.7488668209066,\n",
       " 3194.6818219505208,\n",
       " 3194.6148680507986,\n",
       " 3194.5480049980883,\n",
       " 3194.4812326689075,\n",
       " 3194.4145509399405,\n",
       " 3194.3479596880397,\n",
       " 3194.281458790225,\n",
       " 3194.2150481236836,\n",
       " 3194.1487275657705,\n",
       " 3194.0824969940068,\n",
       " 3194.01635628608,\n",
       " 3193.950305319845,\n",
       " 3193.8843439733214,\n",
       " 3193.8184721246967,\n",
       " 3193.7526896523223,\n",
       " 3193.686996434717,\n",
       " 3193.6213923505625,\n",
       " 3193.555877278707,\n",
       " 3193.490451098164,\n",
       " 3193.4251136881117,\n",
       " 3193.35986492789,\n",
       " 3193.2947046970057,\n",
       " 3193.22963287513,\n",
       " 3193.164649342094,\n",
       " 3193.0997539778973,\n",
       " 3193.0349466626994,\n",
       " 3192.970227276823,\n",
       " 3192.905595700756,\n",
       " 3192.8410518151445,\n",
       " 3192.776595500803,\n",
       " 3192.712226638703,\n",
       " 3192.647945109981,\n",
       " 3192.583750795932,\n",
       " 3192.5196435780167,\n",
       " 3192.455623337854,\n",
       " 3192.3916899572246,\n",
       " 3192.3278433180703,\n",
       " 3192.2640833024943,\n",
       " 3192.2004097927584,\n",
       " 3192.136822671286,\n",
       " 3192.073321820659,\n",
       " 3192.009907123622,\n",
       " 3191.9465784630747,\n",
       " 3191.88333572208,\n",
       " 3191.820178783859]"
      ]
     },
     "execution_count": 690,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Regression Model\n",
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge regression linear model intercept: 75.61029319590861\n",
      "ridge regression linear model coeff:\n",
      "[9.45954091 9.77904476 7.9729408 ]\n",
      "R-squared score (training): 0.369\n",
      "R-squared score (test): 0.375\n",
      "Number of non-zero features: 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "linridge = Ridge(alpha=25.0).fit(X_train, y_train)\n",
    "\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test, y_test)))\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wheat dataset\n",
      "ridge regression linear model intercept: 49.33056850867053\n",
      "ridge regression linear model coeff:\n",
      "[30.092738   25.32929225 25.27089312]\n",
      "R-squared score (training): 0.231\n",
      "R-squared score (test): 0.247\n",
      "Number of non-zero features: 3\n"
     ]
    }
   ],
   "source": [
    "# Ridge regression with feature normalization\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linridge = Ridge(alpha=20.0).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Wheat dataset')\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test_scaled, y_test)))\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression: effect of alpha regularization parameter\n",
      "\n",
      "Alpha = 0.00\n",
      "num abs(coeff) > 1.0: 3, r-squared training: 0.37, r-squared test: 0.37\n",
      "\n",
      "Alpha = 1.00\n",
      "num abs(coeff) > 1.0: 3, r-squared training: 0.37, r-squared test: 0.38\n",
      "\n",
      "Alpha = 10.00\n",
      "num abs(coeff) > 1.0: 3, r-squared training: 0.29, r-squared test: 0.32\n",
      "\n",
      "Alpha = 20.00\n",
      "num abs(coeff) > 1.0: 3, r-squared training: 0.23, r-squared test: 0.25\n",
      "\n",
      "Alpha = 50.00\n",
      "num abs(coeff) > 1.0: 3, r-squared training: 0.14, r-squared test: 0.14\n",
      "\n",
      "Alpha = 100.00\n",
      "num abs(coeff) > 1.0: 3, r-squared training: 0.08, r-squared test: 0.08\n",
      "\n",
      "Alpha = 1000.00\n",
      "num abs(coeff) > 1.0: 1, r-squared training: 0.01, r-squared test: 0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ridge regression with regularization parameter: alpha\n",
    "\n",
    "print('Ridge regression: effect of alpha regularization parameter\\n')\n",
    "for this_alpha in [0, 1, 10, 20, 50, 100, 1000]:\n",
    "    linridge = Ridge(alpha = this_alpha).fit(X_train_scaled, y_train)\n",
    "    r2_train = linridge.score(X_train_scaled, y_train)\n",
    "    r2_test = linridge.score(X_test_scaled, y_test)\n",
    "    num_coeff_bigger = np.sum(abs(linridge.coef_) > 1.0)\n",
    "    print('Alpha = {:.2f}\\nnum abs(coeff) > 1.0: {}, \\\n",
    "r-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n",
    "         .format(this_alpha, num_coeff_bigger, r2_train, r2_test))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso regression linear model intercept: 51.691515991159186\n",
      "lasso regression linear model coeff:\n",
      "[34.93234036 13.83541656 22.61395754]\n",
      "Non-zero features: 3\n",
      "R-squared score (training): 0.212\n",
      "R-squared score (test): 0.228\n",
      "\n",
      "Features with non-zero weight (sorted by absolute magnitude):\n",
      "\t[-0.76812684 -0.91287325 -0.77648137], 34.932\n",
      "\t[0.38012857 0.43697336 0.48128734], 22.614\n",
      "\t[-0.29531579 -0.33436756 -0.23743764], 13.835\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linlasso = Lasso(alpha=1, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('lasso regression linear model intercept: {}'\n",
    "     .format(linlasso.intercept_))\n",
    "print('lasso regression linear model coeff:\\n{}'\n",
    "     .format(linlasso.coef_))\n",
    "print('Non-zero features: {}'\n",
    "     .format(np.sum(linlasso.coef_ != 0)))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linlasso.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}\\n'\n",
    "     .format(linlasso.score(X_test_scaled, y_test)))\n",
    "print('Features with non-zero weight (sorted by absolute magnitude):')\n",
    "\n",
    "for e in sorted (list(zip(list(X), linlasso.coef_)),\n",
    "                key = lambda e: -abs(e[1])):\n",
    "    if e[1] != 0:\n",
    "        print('\\t{}, {:.3f}'.format(e[0], e[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso regression: effect of alpha regularization\n",
      "parameter on number of features kept in final model\n",
      "\n",
      "Alpha = 0.50\n",
      "Features kept: 3, r-squared training: 0.33, r-squared test: 0.35\n",
      "\n",
      "Alpha = 1.00\n",
      "Features kept: 3, r-squared training: 0.21, r-squared test: 0.23\n",
      "\n",
      "Alpha = 2.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n",
      "Alpha = 3.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n",
      "Alpha = 5.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n",
      "Alpha = 10.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n",
      "Alpha = 20.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n",
      "Alpha = 50.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Lasso regression: effect of alpha regularization\\n\\\n",
    "parameter on number of features kept in final model\\n')\n",
    "\n",
    "for alpha in [0.5, 1, 2, 3, 5, 10, 20, 50]:\n",
    "    linlasso = Lasso(alpha, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "    r2_train = linlasso.score(X_train_scaled, y_train)\n",
    "    r2_test = linlasso.score(X_test_scaled, y_test)\n",
    "    \n",
    "    print('Alpha = {:.2f}\\nFeatures kept: {}, r-squared training: {:.2f}, \\\n",
    "r-squared test: {:.2f}\\n'\n",
    "         .format(alpha, np.sum(linlasso.coef_ != 0), r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import KFold # import k-fold validation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLYNOMIAL DEGREE-2 W/ LINEAR REGRESSION\n",
      "(poly deg 2) linear model coeff (w):\n",
      "[ 0.          9.27494063 14.88858744  6.2633744  -1.71614443  3.46508052\n",
      " -2.36960433 -2.0656223   2.2869867  -0.0486266 ]\n",
      "(poly deg 2) linear model intercept (b): 78.221\n",
      "(poly deg 2) R-squared score (training): 0.446\n",
      "(poly deg 2) R-squared score (test): 0.468\n",
      "(poly deg 2) RMSE score (train): 22.361\n",
      "(poly deg 2) RMSE score (test): 21.449\n",
      "\n",
      "POLYNOMIAL DEGREE-3 W/ LINEAR REGRESSION\n",
      "(poly deg 3) linear model coeff (w):\n",
      "[ 0.         10.04600121 16.51410576  3.8567487  -1.95735901  4.63335918\n",
      " -3.56010278 -2.93175914  3.60510299 -0.61092097  0.31269739  0.67298507\n",
      "  1.71459649 -1.15030386  0.82323137 -0.75006686 -0.24894993 -0.50930664\n",
      " -0.50459191  0.37132156]\n",
      "(poly deg 3) linear model intercept (b): 79.322\n",
      "(poly deg 3) R-squared score (training): 0.460\n",
      "(poly deg 3) R-squared score (test): 0.422\n",
      "(poly deg 3) RMSE score (train): 22.073\n",
      "(poly deg 3) RMSE score (test): 22.357\n",
      "\n",
      "POLYNOMIAL DEGREE-5 W/ LINEAR REGRESSION\n",
      "(poly deg 5) linear model coeff (w):\n",
      "[-2.76549167e+12  6.08520888e+00  1.25062638e+01  8.91976260e+00\n",
      " -4.74497636e+00  1.77411189e+00 -6.57272723e+00 -1.02984554e+01\n",
      "  1.48526875e+01 -1.12878362e+01  2.21024925e+00 -7.88715312e+00\n",
      " -4.24064402e+00  1.01756005e+01 -4.92258966e+00 -1.32708559e+00\n",
      "  2.33257733e+00  1.82997951e+00  2.24729853e+00  2.43087522e+00\n",
      "  4.32025116e-02  1.73253562e+00  7.86533775e+00  1.60464534e+00\n",
      " -1.28314806e+01 -4.66718999e+00  3.27649890e-01  3.76669857e+00\n",
      "  3.11709883e+00 -5.12547283e+00  1.96543917e+00  3.06446791e-01\n",
      "  4.46483884e+00 -6.11436129e+00  5.17930296e+00 -9.39534424e-02\n",
      "  1.12363397e-01 -1.45532654e+00 -1.56372403e+00  3.74359373e+00\n",
      "  2.65673719e+00  2.76909114e+00 -1.82699373e+00 -4.16227714e+00\n",
      "  3.58377107e+00  1.44787492e-01 -1.27363578e+00 -1.04600862e+00\n",
      " -1.92212635e+00  1.72250739e+00 -6.17710059e-01 -1.14034939e+00\n",
      " -1.19390845e+00  1.91149775e+00 -3.73523196e-02 -1.75914573e+00]\n",
      "(poly deg 5) linear model intercept (b): 2765491671462.154\n",
      "(poly deg 5) R-squared score (training): 0.279\n",
      "(poly deg 5) R-squared score (test): -30.740\n",
      "(poly deg 5) RMSE score (train): 25.513\n",
      "(poly deg 5) RMSE score (test): 165.622\n",
      "\n",
      "POLYNOMIAL DEGREE-10 W/ LINEAR REGRESSION\n",
      "(poly deg 10) linear model coeff (w):\n",
      "[ 9.49914285e-08  4.67242339e+00  3.25537860e+00  9.18778103e+00\n",
      "  1.62002881e+01 -1.54772028e+02 -5.04514114e+01  4.99897324e+01\n",
      " -4.51136017e+00  7.91284201e-01 -2.02202689e+01 -1.05121852e+02\n",
      " -3.29237485e+00  1.67431173e+02 -7.91627756e+01 -2.72664852e+01\n",
      " -5.17866946e+01 -8.51742005e+01  2.86744327e+02 -2.94012032e+01\n",
      "  4.17514541e+01  1.62809843e+02  6.51772115e+01 -4.05790601e+02\n",
      "  4.58969176e+02 -3.20401518e+02  4.70663756e+02 -1.42445733e+02\n",
      "  5.90165936e+02 -4.69419583e+00 -9.63857804e+01 -1.45154889e+02\n",
      "  7.59402144e+01 -9.71878745e+01  1.73672378e+01 -4.22816047e+01\n",
      "  1.67898293e+02  1.77094238e+01 -3.29469349e+02 -1.87141664e+02\n",
      "  3.03321077e+02  4.28712989e+02  2.25038170e+02 -1.91035757e+02\n",
      "  6.73500836e+01 -2.53291295e+02  3.38662772e+00 -1.12593744e+02\n",
      "  2.92026048e+02 -2.27860587e+01  8.55506671e+01  2.01777462e+02\n",
      " -5.91684048e+02  3.45419324e+02 -3.22440147e+02  7.56608575e+00\n",
      " -2.92066497e+01 -8.47120044e+01 -6.13467758e+01 -3.94190970e+01\n",
      " -2.35101109e+02  3.07360108e+02 -2.20914590e+02  4.55487901e+01\n",
      " -7.01007144e+02  2.66215707e+02  7.27664150e+02 -4.32230229e+02\n",
      "  1.02879573e+03 -7.87339175e+02  3.26305529e+02 -5.03676133e+02\n",
      "  6.21065331e+01 -1.20391870e+03  6.64318188e+02 -4.97288702e+02\n",
      " -4.63544403e+00  5.52578941e+01  1.96370210e+02 -1.47566803e+01\n",
      "  2.71340531e+02 -2.52762875e+02  1.54224367e+02 -1.68322536e+01\n",
      "  4.66725233e+01 -9.86640061e+01 -2.99784138e+01  1.96987640e+02\n",
      "  3.71599671e+02 -2.01047810e+02 -4.96965133e+02 -4.56058625e+02\n",
      "  2.98501138e+02  1.47856038e+01  8.12779849e+02 -4.28993413e+02\n",
      " -2.65975215e+02  1.84707467e+02 -1.32811540e+02 -6.22858011e+02\n",
      "  8.90602366e+02 -8.46859491e+01 -6.40919099e+02  1.60244598e+02\n",
      " -4.11421853e+01  2.00522496e+02 -3.40684013e+02 -5.31572094e+01\n",
      "  3.66794629e+02  2.63318758e+02 -3.30918775e+02  3.43739350e+01\n",
      " -3.88271716e+01 -9.70873960e+01  4.57772420e+02 -6.11650030e+02\n",
      "  5.30504654e+02 -2.03252580e+02  1.15031318e+02  7.67183512e+00\n",
      " -1.43760650e+01  3.73070071e+01  7.62218002e+01  1.19818570e+02\n",
      " -3.01688210e+02 -5.40141507e+01  1.45203862e+02  6.45452974e+02\n",
      "  2.96520690e+02 -2.71857368e+02 -4.90608622e+02 -2.78342851e+02\n",
      " -5.47807804e+02  9.41344320e+02 -2.47170420e+02  3.22012364e+02\n",
      "  6.08116474e+01  5.72495524e+02 -9.21112046e+02  4.68884264e+02\n",
      " -9.61071892e+01 -2.03296587e+02  1.92125045e+02 -7.85615747e+02\n",
      "  4.86569182e+02 -5.42826383e+02  1.98752586e+02 -9.37316068e+01\n",
      "  1.31961428e+02 -1.06503944e+02  7.44993734e+02 -3.11298020e+02\n",
      "  9.19359291e+02 -4.46527269e+02  1.65016662e+02  3.46334478e+00\n",
      " -1.03667640e+01 -2.25626180e+01 -9.74733431e+01 -1.01001451e+02\n",
      "  2.17396519e+01 -1.75307662e+02  1.69558556e+02 -6.84527726e+01\n",
      "  2.48435165e+00 -3.34184049e-02  8.73002209e+00 -3.73237212e+01\n",
      " -9.99238982e+01  1.21659196e+02  3.15342983e+01  2.49896521e+02\n",
      " -1.16663663e+02 -2.93640512e+02  8.87238335e+01 -3.88711832e+02\n",
      "  1.02048260e+02  1.00252951e+03 -6.25573743e+02  3.43939578e+01\n",
      "  4.56308407e+02 -4.58025983e+02 -1.05802732e+03  8.37944610e+02\n",
      "  2.00014541e+02 -6.39276643e+01 -4.23983059e+02  8.76815399e+02\n",
      "  2.77290522e+02  5.62393744e+00 -7.71980734e+02  2.64608047e+02\n",
      "  6.08207067e+00  2.32960578e+02 -7.08128314e+02  2.90832552e+02\n",
      " -3.56774486e+02  5.31604394e+02 -9.31279760e+01 -1.58331024e+01\n",
      "  7.78486013e+00 -5.81325847e+01  2.29090132e+02 -1.13212172e+02\n",
      "  1.19932386e+00 -2.52741556e+01 -3.50843653e+01 -6.03452949e+01\n",
      "  6.77005542e+01 -7.65562002e+00  7.96222418e+00 -2.35714477e+00\n",
      " -9.24890531e+01  1.81406138e+02 -2.31920804e+02  2.28193294e+02\n",
      " -1.42114557e+02  4.61402660e+01 -1.61354903e+01 -2.04408116e+00\n",
      "  4.28840345e-01 -1.90961394e+00  6.24221385e+00  1.42540633e+01\n",
      " -4.88175301e+00 -3.64276873e-02 -9.24778626e+01 -3.11544235e+01\n",
      "  8.70356287e+01  2.08802992e+00  1.48024025e+02 -4.69130613e+01\n",
      " -2.85396834e+02  4.37689548e+01  3.80413928e+01 -2.01150384e+02\n",
      "  1.27996811e+02  2.04265398e+02 -1.47216076e+01 -2.78794680e+02\n",
      "  5.97025115e+01  1.44918199e+02  2.17475809e+00 -1.66806278e+01\n",
      " -2.76651415e+02  4.76910827e+02 -1.09033300e+02  1.58273339e+01\n",
      "  2.07581026e+01 -3.18782226e+02  3.96401947e+02  3.19185861e+02\n",
      " -3.15788069e+02 -1.33865539e+02  8.77795014e+01 -1.19727145e+00\n",
      " -5.06204460e+01  2.40335834e+02 -4.26258525e+02 -4.51170935e+01\n",
      "  7.77059770e+01  2.71263357e+02 -7.84004189e+01 -3.61831278e+00\n",
      "  8.88255210e+00  4.53713356e+00 -2.34653435e+01  7.50379000e+01\n",
      " -9.84910621e+00 -2.72834436e+02  1.16254131e+02 -2.12754147e+02\n",
      "  1.26100598e+02 -3.30233349e+01  6.07271364e-01 -8.95556154e-01\n",
      "  5.76446523e-01  1.27970965e+01 -4.97367740e+01  1.05635510e+02\n",
      "  1.68481302e+01  4.39295401e-01  4.57476441e+00 -2.32440260e+01\n",
      "  9.75914611e+00  2.16943412e-01]\n",
      "(poly deg 10) linear model intercept (b): 81.572\n",
      "(poly deg 10) R-squared score (training): 0.762\n",
      "(poly deg 10) R-squared score (test): -19388973036.737\n",
      "(poly deg 10) RMSE score (train): 14.650\n",
      "(poly deg 10) RMSE score (test): 4093461.600\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_learning_curves' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-566-242b744f3f40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m          .format(degree, sqrt(mean_squared_error(y_test, linreg.predict(X_test)))))\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mplot_learning_curves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinreg\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_learning_curves' is not defined"
     ]
    }
   ],
   "source": [
    "for degree in [2, 3, 5,10]:\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_F1_poly = poly.fit_transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y, random_state=0)\n",
    "    linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "    print('POLYNOMIAL DEGREE-{} W/ LINEAR REGRESSION'.format(degree))\n",
    "    print('(poly deg {}) linear model coeff (w):\\n{}'\n",
    "         .format(degree, linreg.coef_))\n",
    "    print('(poly deg {}) linear model intercept (b): {:.3f}'\n",
    "         .format(degree, linreg.intercept_))\n",
    "    print('(poly deg {}) R-squared score (training): {:.3f}'\n",
    "         .format(degree, linreg.score(X_train, y_train)))\n",
    "    print('(poly deg {}) R-squared score (test): {:.3f}'\n",
    "         .format(degree, linreg.score(X_test, y_test)))\n",
    "    print('(poly deg {}) RMSE score (train): {:.3f}'\n",
    "         .format(degree, sqrt(mean_squared_error(y_train, linreg.predict(X_train)))))\n",
    "    print('(poly deg {}) RMSE score (test): {:.3f}\\n'\n",
    "         .format(degree, sqrt(mean_squared_error(y_test, linreg.predict(X_test)))))\n",
    "\n",
    "plot_learning_curves(linreg,  X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLYNOMIAL DEGREE-2 W/ RIDGE REGRESSION\n",
      "(poly deg 2) linear model coeff (w):\n",
      "[ 0.          9.26871585 14.85093921  6.26015403 -1.71335056  3.45958078\n",
      " -2.36101212 -2.06108392  2.2770491  -0.04612315]\n",
      "(poly deg 2) linear model intercept (b): 78.213\n",
      "(poly deg 2) R-squared score (training): 0.446\n",
      "(poly deg 2) R-squared score (test): 0.468\n",
      "(poly deg 2) RMSE score (train): 22.361\n",
      "(poly deg 2) RMSE score (test): 21.445\n",
      "\n",
      "POLYNOMIAL DEGREE-3 W/ RIDGE REGRESSION\n",
      "(poly deg 3) linear model coeff (w):\n",
      "[ 0.         10.00261414 16.38346877  3.84937049 -1.950071    4.57036623\n",
      " -3.55003185 -2.90750989  3.57237401 -0.60189014  0.31286385  0.70291667\n",
      "  1.71901529 -1.11820182  0.79729862 -0.7417496  -0.24382326 -0.48711814\n",
      " -0.47377904  0.37102814]\n",
      "(poly deg 3) linear model intercept (b): 79.311\n",
      "(poly deg 3) R-squared score (training): 0.460\n",
      "(poly deg 3) R-squared score (test): 0.420\n",
      "(poly deg 3) RMSE score (train): 22.074\n",
      "(poly deg 3) RMSE score (test): 22.384\n",
      "\n",
      "POLYNOMIAL DEGREE-5 W/ RIDGE REGRESSION\n",
      "(poly deg 5) linear model coeff (w):\n",
      "[ 0.          5.65189076 14.24383317  4.9049975  -4.45387024  1.32780139\n",
      " -3.4918604  -9.66990351  6.51031049 -8.51211033  2.56420553 -6.31288931\n",
      " -2.53387266  5.08082224  3.14180052  1.64242401  1.30871527  0.90089231\n",
      "  4.69234085  1.73970102 -1.25527243  5.15108241  3.4920569  -2.79129865\n",
      " -7.8190492   2.63650699 -0.86788673  2.86185943  3.29739938 -1.94578572\n",
      "  1.55601501  1.30145363  1.02629997 -0.73001324  1.5159934   0.25120527\n",
      " -0.88021405 -0.41710159  0.63723964  3.13103342 -0.40753271 -0.05755854\n",
      " -3.76549989  0.08928967  0.6820785  -0.69326973 -0.52585336  0.72094791\n",
      " -2.52332817  0.25843204 -0.37143705  0.04481272  0.7471299  -0.4368825\n",
      " -1.09310691 -0.55392969]\n",
      "(poly deg 5) linear model intercept (b): 85.929\n",
      "(poly deg 5) R-squared score (training): 0.537\n",
      "(poly deg 5) R-squared score (test): -25.199\n",
      "(poly deg 5) RMSE score (train): 20.448\n",
      "(poly deg 5) RMSE score (test): 150.472\n",
      "\n",
      "POLYNOMIAL DEGREE-10 W/ RIDGE REGRESSION\n",
      "(poly deg 10) linear model coeff (w):\n",
      "[ 0.00000000e+00  2.94967855e+00  1.63807816e+01  2.40274291e+00\n",
      " -9.69815445e+00 -1.55585026e+00  3.61397207e+00 -3.87987388e+00\n",
      " -3.42004067e+00 -1.99167793e+00 -6.18559408e+00  8.55688705e+00\n",
      "  4.93911184e+00  3.86398075e+00  1.33960840e+00  1.56950950e+01\n",
      " -5.50718186e+00  2.12313481e+00  9.84865811e+00  3.16043786e+00\n",
      "  1.38699970e+00 -2.87942683e+00  5.87547868e+00  2.88120412e+00\n",
      "  3.60203097e+00  2.13692799e+00  6.16184551e+00 -1.16178527e+01\n",
      "  1.64644275e+01  6.61442893e-01 -2.81131907e+00 -9.63273228e-02\n",
      " -3.56841035e+00  3.98563676e-01  3.90436675e+00  3.14160663e+00\n",
      " -5.05212415e+00 -5.20552785e+00 -2.83299109e+00 -3.12040804e+00\n",
      "  9.11396756e+00  5.37150264e+00 -5.92821100e+00 -4.57711430e-01\n",
      "  6.18660878e+00 -4.82688927e+00  5.04672329e+00  1.02729441e+01\n",
      "  1.21483106e+00 -3.47380062e+00  3.29872254e+00  8.03930844e+00\n",
      " -3.57596337e+00 -9.50416278e+00 -4.92143715e+00 -3.62552083e+00\n",
      "  2.27969435e+00 -1.01552760e+01 -5.84222876e+00  3.79216480e-02\n",
      "  4.16529544e-01  5.63698552e+00  2.03237261e-01 -9.28559820e+00\n",
      "  7.22350242e-01 -4.66052793e+00  8.52145650e+00 -7.03089376e+00\n",
      " -5.23960688e+00  4.68006479e+00  8.52671323e+00  3.03214387e+00\n",
      " -9.48217317e+00  1.07983206e+01  2.12983701e+00  6.59932274e+00\n",
      " -2.01170529e+00 -2.88638608e-01  1.42179806e+01 -4.91565839e+00\n",
      " -4.42093930e+00 -4.43029018e+00  1.95322433e+00 -5.02458049e+00\n",
      "  2.33817339e+00 -6.42723822e+00  3.53451240e+00  8.74123063e+00\n",
      " -6.44121638e+00 -1.71854681e+01 -7.59538239e-02 -3.19032125e+00\n",
      " -5.28649237e+00  1.80051429e+00 -7.73008267e+00 -8.84056040e-01\n",
      " -4.54849828e+00 -3.93581400e+00  1.04030284e+01  5.14349935e+00\n",
      "  2.14879066e+00 -3.59005230e+00 -6.55280535e+00 -1.56283872e+00\n",
      " -8.60317855e+00 -5.27140118e+00  1.84063662e+00  7.91982476e+00\n",
      "  3.99185124e+00  7.49221979e+00  3.12383633e-01 -7.39937685e+00\n",
      "  1.43227109e+00 -2.57592462e+00 -5.01550448e+00 -7.95554780e+00\n",
      "  5.01984281e+00  1.64293309e+00 -1.39790607e+00  3.63014289e+00\n",
      " -4.47909459e+00  1.14723511e+01  3.35846779e+00  1.06376733e+01\n",
      "  2.17903222e+00 -1.22624082e+00 -5.01455729e+00  1.14204886e+01\n",
      " -3.94437501e+00 -2.87550849e+00 -8.72284924e+00 -4.04718741e-01\n",
      " -3.93954982e+00  6.70739731e+00 -9.74150013e-01 -8.14832527e+00\n",
      " -7.34481810e+00  4.16925168e+00  2.10471952e-01  4.66194616e+00\n",
      " -2.58305957e+00 -7.65170951e-01  3.43634565e+00 -6.21841420e+00\n",
      "  8.02776583e-01 -3.43189494e+00 -1.12826972e+01 -6.91283713e+00\n",
      " -4.88653873e-01  6.33561312e+00  6.89613208e+00 -3.97977605e+00\n",
      "  8.62671934e+00  2.86728245e-01 -1.13522840e+01  1.38501839e+00\n",
      "  1.12792606e-01 -5.71401068e+00 -1.51969845e+00  1.26235928e+00\n",
      "  6.86999689e-01 -5.46167835e-01  5.81150735e+00  5.15041005e-01\n",
      "  7.52404403e-01  1.60709058e+00 -3.64164595e+00 -2.64887465e+00\n",
      " -3.02035044e+00  5.37986953e+00  4.13697770e+00  7.32129032e+00\n",
      "  5.00645320e+00  7.57147714e+00 -1.26378784e+00 -3.09583580e+00\n",
      " -7.20152313e-01 -1.85284847e+00 -5.11878183e-01 -7.13510100e+00\n",
      "  4.03049982e+00 -4.84536705e+00  3.00445503e+00  2.24844380e+00\n",
      "  2.31244786e+00  2.75140707e+00  9.53120779e-02 -1.52354517e-01\n",
      "  2.01992210e-01  4.08850185e+00 -1.51175781e+00 -6.53919751e+00\n",
      "  2.08613835e+00 -5.74398332e+00 -1.90674280e+00  4.24873530e+00\n",
      "  2.52960939e+00 -1.14201180e+01  2.42812615e-01  6.32523258e+00\n",
      "  4.39298493e-01  2.91412719e+00  2.17286862e+00 -3.16872890e+00\n",
      " -2.99679017e+00  4.33502620e-01  3.33941611e+00 -7.70214394e+00\n",
      "  1.26361659e+00  1.62402297e+00 -5.22412932e-01 -2.84697979e-01\n",
      " -9.30227984e-02  2.41459822e+00  9.94033950e+00 -6.07627669e+00\n",
      " -2.41370263e+00  3.10181952e+00  9.01004695e-03 -8.08778298e-01\n",
      " -1.60997963e-01  3.84359519e-01  5.33328213e-01 -2.34168950e-01\n",
      " -1.40507753e+00 -4.91838575e-01 -2.61043950e+00 -2.25299422e+00\n",
      "  8.74612765e-02  9.51320817e-01 -1.82581967e+00  1.84713357e-01\n",
      " -1.73073998e+00 -2.18608142e+00  3.56070109e+00 -2.53299748e+00\n",
      " -8.44745903e+00  1.13613257e+00  6.56868769e+00 -1.17393352e+01\n",
      " -2.22696400e-01  7.03387785e+00 -3.57740130e+00 -6.00585671e+00\n",
      "  1.45995178e+01  8.09543683e+00 -2.38396087e+00 -2.28161628e+00\n",
      "  2.03939119e+00  1.02919251e+01  5.98735143e+00 -5.29911621e+00\n",
      "  6.26746151e+00 -1.18636357e+01  7.01181085e+00  1.06487526e+00\n",
      " -1.10616027e+00  7.86970956e-01  5.71248151e+00 -5.88996211e+00\n",
      " -9.83046026e+00  4.51017085e+00  2.63253441e+00  2.35076722e+00\n",
      "  1.62138833e+00 -9.26593967e-01 -2.32016134e+00 -1.53066136e+00\n",
      "  5.89689688e+00 -2.35322750e+00 -7.03301849e+00 -5.23586089e+00\n",
      "  4.19890689e+00  1.09491628e+00 -3.12456821e-01  1.43340926e-01\n",
      "  9.91725700e-01  1.67408734e+00 -3.47652802e+00 -6.83103446e+00\n",
      "  9.27858376e+00  3.33150211e+00 -2.80838625e+00 -1.85492661e+00\n",
      "  1.60502856e-02  1.13249811e-01]\n",
      "(poly deg 10) linear model intercept (b): 84.643\n",
      "(poly deg 10) R-squared score (training): 0.667\n",
      "(poly deg 10) R-squared score (test): -689828690.366\n",
      "(poly deg 10) RMSE score (train): 17.326\n",
      "(poly deg 10) RMSE score (test): 772118.475\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.85983e-18): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n"
     ]
    }
   ],
   "source": [
    "for degree in [2, 3, 5, 10]:\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_F1_poly = poly.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y, random_state=0)\n",
    "    linreg = Ridge().fit(X_train, y_train)\n",
    "\n",
    "    print('POLYNOMIAL DEGREE-{} W/ RIDGE REGRESSION'.format(degree))\n",
    "    print('(poly deg {}) linear model coeff (w):\\n{}'\n",
    "         .format(degree, linreg.coef_))\n",
    "    print('(poly deg {}) linear model intercept (b): {:.3f}'\n",
    "         .format(degree, linreg.intercept_))\n",
    "    print('(poly deg {}) R-squared score (training): {:.3f}'\n",
    "         .format(degree, linreg.score(X_train, y_train)))\n",
    "    print('(poly deg {}) R-squared score (test): {:.3f}'\n",
    "         .format(degree, linreg.score(X_test, y_test)))\n",
    "    print('(poly deg {}) RMSE score (train): {:.3f}'\n",
    "         .format(degree, sqrt(mean_squared_error(y_train, linreg.predict(X_train)))))\n",
    "    print('(poly deg {}) RMSE score (test): {:.3f}\\n'\n",
    "         .format(degree, sqrt(mean_squared_error(y_test, linreg.predict(X_test)))))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DAP3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
