{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y2E1jbS4ZluC",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.23.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn) (1.16.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn) (0.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (1.16.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shaun\\appdata\\roaming\\python\\python37\\site-packages (from python-dateutil>=2.7.3->pandas) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (3.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (1.16.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (7.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shaun\\appdata\\roaming\\python\\python37\\site-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from seaborn) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from seaborn) (1.16.4)\n",
      "Requirement already satisfied: pandas>=0.22.0 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from seaborn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.0.1 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from seaborn) (1.5.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=2.1.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=2.1.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=2.1.2->seaborn) (1.2.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=2.1.2->seaborn) (7.2.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas>=0.22.0->seaborn) (2020.1)\n",
      "Requirement already satisfied: six in c:\\users\\shaun\\appdata\\roaming\\python\\python37\\site-packages (from cycler>=0.10->matplotlib>=2.1.2->seaborn) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.14.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-plot in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.3.7)\n",
      "Requirement already satisfied: joblib>=0.10 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-plot) (0.16.0)\n",
      "Requirement already satisfied: scipy>=0.9 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-plot) (1.5.2)\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-plot) (3.3.0)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-plot) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scipy>=0.9->scikit-plot) (1.16.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (7.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn>=0.18->scikit-plot) (2.1.0)\n",
      "Requirement already satisfied: six in c:\\users\\shaun\\appdata\\roaming\\python\\python37\\site-packages (from cycler>=0.10->matplotlib>=1.4.0->scikit-plot) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.11.1)\n",
      "Requirement already satisfied: numpy>=1.14 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from statsmodels) (1.16.4)\n",
      "Requirement already satisfied: pandas>=0.21 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from statsmodels) (1.1.0)\n",
      "Requirement already satisfied: patsy>=0.5 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from statsmodels) (0.5.1)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from statsmodels) (1.5.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas>=0.21->statsmodels) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas>=0.21->statsmodels) (2.8.1)\n",
      "Requirement already satisfied: six in c:\\users\\shaun\\appdata\\roaming\\python\\python37\\site-packages (from patsy>=0.5->statsmodels) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#If you need to install in modules in jupyter notebook \n",
    "%pip install scikit-learn\n",
    "%pip install pandas\n",
    "%pip install matplotlib \n",
    "%pip install seaborn \n",
    "%pip install graphviz\n",
    "%pip install scikit-plot   \n",
    "%pip install statsmodels   \n",
    "\n",
    "\n",
    "#Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "import datetime # Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import scikitplot as skplt\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.linear_model import LinearRegression\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"decision_trees\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IvsV2RSPapC6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#changes the output for the print statements\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vtW9LfInZ_x2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Pull \n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Rodrig79/Machine-Learning-Data-Analysis-Project/master/rawData/pokemon.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "#Data Cleaning\n",
    "df = df[~df.Name.str.contains('Mega')] #removed pokemon with \"Mega\" in it\n",
    "df = df.drop(columns = [\"Name\",\"Type 1\",\"Type 2\",\"#\",\"Generation\"]) #Removed columns with names\n",
    "\n",
    "#normalization\n",
    "# X = df.values #returns a numpy array\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# x_scaled = min_max_scaler.fit_transform(X)\n",
    "# df=pd.DataFrame(x_scaled, columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Splitting dataset to x and y\n",
    "# y = df.Attack\n",
    "# X = df[['Defense','HP','Sp. Atk','Sp. Def','Speed']]\n",
    "#seperating data\n",
    "df = df.dropna() \n",
    "y = df['Attack']\n",
    "X = df[['Defense','HP','Speed']] \n",
    "X.dropna()\n",
    "y.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Defense</th>\n",
       "      <th>HP</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>45</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>39</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>80</td>\n",
       "      <td>65</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>121</td>\n",
       "      <td>108</td>\n",
       "      <td>81</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>150</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>170</td>\n",
       "      <td>130</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>130</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>751 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Defense   HP  Sp. Atk  Sp. Def  Speed\n",
       "0    49       45   65       65       45   \n",
       "1    63       60   80       80       60   \n",
       "2    83       80   100      100      80   \n",
       "4    43       39   60       50       65   \n",
       "5    58       58   80       65       80   \n",
       "..   ..       ..   ..       ..       ..   \n",
       "794  121      108  81       95       95   \n",
       "795  150      50   100      150      50   \n",
       "797  60       80   150      130      70   \n",
       "798  60       80   170      130      80   \n",
       "799  120      80   130      90       70   \n",
       "\n",
       "[751 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiVariate  Regression with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.linear_model import LinearRegression\n",
    " from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wheat dataset\n",
      "linear model intercept: 75.61804921078154\n",
      "linear model coeff:\n",
      "[ 9.79637961 10.12057203  8.26066902]\n",
      "R-squared score (training): 0.370\n",
      "R-squared score (test): 0.374\n",
      "RMSE: 23.266\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('Wheat dataset')\n",
    "print('linear model intercept: {}'\n",
    "     .format(linreg.intercept_))\n",
    "print('linear model coeff:\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "# make predictions on the testing set\n",
    "y_pred = linreg.predict(X_test)\n",
    "print('RMSE: {:.3f}'.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlrd in c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\shaun\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "# excel_file = 'Folds5x2_pp.xlsx'\n",
    "# data = pd.read_excel(excel_file)\n",
    "# print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X, Y, B):\n",
    "    m = len(Y)\n",
    "    J = np.sum((X.dot(B) - Y) ** 2)/(2 * m)\n",
    "    return J\n",
    "\n",
    "def batch_gradient_descent(X, Y, B, alpha, iterations):\n",
    "    cost_history = [0] * iterations\n",
    "    m = len(Y)\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        #print(iteration)\n",
    "        # Hypothesis Values\n",
    "        h = X.dot(B)\n",
    "        # Difference b/w Hypothesis and Actual Y\n",
    "        loss = h - Y\n",
    "        # Gradient Calculation\n",
    "        gradient = X.T.dot(loss) / m\n",
    "        # Changing Values of B using Gradient\n",
    "        B = B - alpha * gradient\n",
    "        # New Cost Value\n",
    "        cost = cost_function(X, Y, B)\n",
    "        cost_history[iteration] = cost \n",
    "    return B, cost_history \n",
    "def pred(x_test, newB):\n",
    "    return x_test.dot(newB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Coefficients\n",
    "B = np.zeros(X_train.shape[1])\n",
    "alpha = 0.005\n",
    "iter_ = 2000\n",
    "newB, cost_history = batch_gradient_descent(X_train, y_train, B, alpha, iter_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = pred(X_test,newB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.571688375255779"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def r2(y_,y):\n",
    "    sst = np.sum((y-y.mean())**2)\n",
    "    ssr = np.sum((y_-y)**2)\n",
    "    r2 = 1-(ssr/sst)\n",
    "    return(r2)\n",
    "#----------------\n",
    "r2(y_,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-23.155180614367016"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_ = pred(X_test[3],newB)\n",
    "ans_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3290.7368925532273,\n",
       " 3288.8534498363492,\n",
       " 3286.993251925182,\n",
       " 3285.1560104156224,\n",
       " 3283.341440492039,\n",
       " 3281.5492608825552,\n",
       " 3279.7791938148885,\n",
       " 3278.0309649727424,\n",
       " 3276.3043034527404,\n",
       " 3274.5989417218984,\n",
       " 3272.9146155756252,\n",
       " 3271.25106409625,\n",
       " 3269.608029612064,\n",
       " 3267.985257656877,\n",
       " 3266.382496930072,\n",
       " 3264.799499257165,\n",
       " 3263.236019550852,\n",
       " 3261.6918157725404,\n",
       " 3260.166648894368,\n",
       " 3258.660282861688,\n",
       " 3257.1724845560225,\n",
       " 3255.7030237584827,\n",
       " 3254.2516731136416,\n",
       " 3252.818208093856,\n",
       " 3251.402406964039,\n",
       " 3250.0040507468652,\n",
       " 3248.622923188415,\n",
       " 3247.2588107242423,\n",
       " 3245.911502445873,\n",
       " 3244.580790067711,\n",
       " 3243.266467894366,\n",
       " 3241.9683327883845,\n",
       " 3240.6861841383816,\n",
       " 3239.4198238275753,\n",
       " 3238.1690562027093,\n",
       " 3236.933688043363,\n",
       " 3235.7135285316467,\n",
       " 3234.5083892222715,\n",
       " 3233.3180840129985,\n",
       " 3232.1424291154476,\n",
       " 3230.9812430262773,\n",
       " 3229.834346498721,\n",
       " 3228.7015625144804,\n",
       " 3227.582716255963,\n",
       " 3226.477635078878,\n",
       " 3225.386148485158,\n",
       " 3224.308088096233,\n",
       " 3223.243287626629,\n",
       " 3222.1915828578976,\n",
       " 3221.1528116128707,\n",
       " 3220.12681373024,\n",
       " 3219.113431039446,\n",
       " 3218.112507335886,\n",
       " 3217.1238883564306,\n",
       " 3216.1474217552413,\n",
       " 3215.182957079897,\n",
       " 3214.2303457478074,\n",
       " 3213.289441022932,\n",
       " 3212.3600979927833,\n",
       " 3211.4421735457154,\n",
       " 3210.5355263484994,\n",
       " 3209.6400168241776,\n",
       " 3208.7555071301895,\n",
       " 3207.8818611367806,\n",
       " 3207.018944405666,\n",
       " 3206.1666241689736,\n",
       " 3205.3247693084413,\n",
       " 3204.4932503348787,\n",
       " 3203.6719393678777,\n",
       " 3202.860710115787,\n",
       " 3202.0594378559244,\n",
       " 3201.267999415046,\n",
       " 3200.486273150049,\n",
       " 3199.7141389289263,\n",
       " 3198.9514781119483,\n",
       " 3198.198173533085,\n",
       " 3197.454109481657,\n",
       " 3196.719171684218,\n",
       " 3195.993247286661,\n",
       " 3195.2762248365484,\n",
       " 3194.5679942656607,\n",
       " 3193.8684468727665,\n",
       " 3193.1774753066015,\n",
       " 3192.494973549067,\n",
       " 3191.82083689863,\n",
       " 3191.1549619539387,\n",
       " 3190.4972465976325,\n",
       " 3189.8475899803616,\n",
       " 3189.2058925050037,\n",
       " 3188.572055811076,\n",
       " 3187.9459827593414,\n",
       " 3187.327577416608,\n",
       " 3186.7167450407196,\n",
       " 3186.113392065729,\n",
       " 3185.5174260872573,\n",
       " 3184.928755848039,\n",
       " 3184.347291223646,\n",
       " 3183.772943208384,\n",
       " 3183.205623901373,\n",
       " 3182.645246492796,\n",
       " 3182.091725250321,\n",
       " 3181.5449755056916,\n",
       " 3181.0049136414837,\n",
       " 3180.4714570780307,\n",
       " 3179.9445242605066,\n",
       " 3179.4240346461756,\n",
       " 3178.9099086917963,\n",
       " 3178.4020678411835,\n",
       " 3177.9004345129288,\n",
       " 3177.4049320882673,\n",
       " 3176.9154848991034,\n",
       " 3176.4320182161796,\n",
       " 3175.954458237395,\n",
       " 3175.4827320762715,\n",
       " 3175.016767750559,\n",
       " 3174.556494170988,\n",
       " 3174.1018411301566,\n",
       " 3173.6527392915614,\n",
       " 3173.2091201787616,\n",
       " 3172.770916164679,\n",
       " 3172.3380604610334,\n",
       " 3171.9104871079035,\n",
       " 3171.488130963428,\n",
       " 3171.0709276936236,\n",
       " 3170.6588137623394,\n",
       " 3170.2517264213293,\n",
       " 3169.8496037004547,\n",
       " 3169.4523843980037,\n",
       " 3169.060008071132,\n",
       " 3168.6724150264263,\n",
       " 3168.2895463105824,\n",
       " 3167.911343701199,\n",
       " 3167.537749697687,\n",
       " 3167.1687075122954,\n",
       " 3166.8041610612404,\n",
       " 3166.4440549559554,\n",
       " 3166.0883344944423,\n",
       " 3165.736945652735,\n",
       " 3165.389835076465,\n",
       " 3165.046950072537,\n",
       " 3164.708238600906,\n",
       " 3164.373649266455,\n",
       " 3164.043131310975,\n",
       " 3163.71663460525,\n",
       " 3163.394109641228,\n",
       " 3163.0755075243083,\n",
       " 3162.7607799657026,\n",
       " 3162.4498792749114,\n",
       " 3162.142758352282,\n",
       " 3161.8393706816623,\n",
       " 3161.539670323149,\n",
       " 3161.2436119059203,\n",
       " 3160.9511506211647,\n",
       " 3160.6622422150913,\n",
       " 3160.3768429820325,\n",
       " 3160.094909757628,\n",
       " 3159.8163999120998,\n",
       " 3159.541271343604,\n",
       " 3159.2694824716673,\n",
       " 3159.0009922307136,\n",
       " 3158.735760063655,\n",
       " 3158.473745915578,\n",
       " 3158.214910227501,\n",
       " 3157.9592139302076,\n",
       " 3157.706618438166,\n",
       " 3157.4570856435103,\n",
       " 3157.2105779101103,\n",
       " 3156.9670580677052,\n",
       " 3156.72648940612,\n",
       " 3156.48883566954,\n",
       " 3156.2540610508718,\n",
       " 3156.022130186165,\n",
       " 3155.793008149105,\n",
       " 3155.5666604455782,\n",
       " 3155.3430530082983,\n",
       " 3155.122152191506,\n",
       " 3154.903924765732,\n",
       " 3154.688337912623,\n",
       " 3154.475359219836,\n",
       " 3154.2649566759933,\n",
       " 3154.057098665703,\n",
       " 3153.851753964638,\n",
       " 3153.6488917346796,\n",
       " 3153.44848151912,\n",
       " 3153.2504932379225,\n",
       " 3153.0548971830485,\n",
       " 3152.861664013829,\n",
       " 3152.67076475241,\n",
       " 3152.48217077924,\n",
       " 3152.2958538286234,\n",
       " 3152.1117859843266,\n",
       " 3151.9299396752335,\n",
       " 3151.750287671066,\n",
       " 3151.5728030781474,\n",
       " 3151.3974593352227,\n",
       " 3151.2242302093337,\n",
       " 3151.053089791739,\n",
       " 3150.8840124938906,\n",
       " 3150.7169730434584,\n",
       " 3150.551946480403,\n",
       " 3150.3889081530983,\n",
       " 3150.227833714506,\n",
       " 3150.0686991183884,\n",
       " 3149.9114806155803,\n",
       " 3149.756154750298,\n",
       " 3149.602698356495,\n",
       " 3149.4510885542722,\n",
       " 3149.301302746322,\n",
       " 3149.1533186144184,\n",
       " 3149.007114115959,\n",
       " 3148.8626674805396,\n",
       " 3148.719957206577,\n",
       " 3148.5789620579744,\n",
       " 3148.4396610608237,\n",
       " 3148.302033500158,\n",
       " 3148.166058916729,\n",
       " 3148.031717103843,\n",
       " 3147.8989881042203,\n",
       " 3147.7678522069054,\n",
       " 3147.6382899442074,\n",
       " 3147.510282088685,\n",
       " 3147.383809650161,\n",
       " 3147.2588538727846,\n",
       " 3147.1353962321205,\n",
       " 3147.013418432279,\n",
       " 3146.892902403082,\n",
       " 3146.773830297263,\n",
       " 3146.6561844876987,\n",
       " 3146.5399475646846,\n",
       " 3146.425102333234,\n",
       " 3146.3116318104167,\n",
       " 3146.1995192227273,\n",
       " 3146.0887480034894,\n",
       " 3145.9793017902925,\n",
       " 3145.871164422456,\n",
       " 3145.7643199385275,\n",
       " 3145.6587525738155,\n",
       " 3145.554446757947,\n",
       " 3145.4513871124605,\n",
       " 3145.349558448424,\n",
       " 3145.248945764088,\n",
       " 3145.149534242563,\n",
       " 3145.051309249528,\n",
       " 3144.954256330969,\n",
       " 3144.858361210942,\n",
       " 3144.763609789368,\n",
       " 3144.66998813985,\n",
       " 3144.5774825075237,\n",
       " 3144.4860793069292,\n",
       " 3144.395765119915,\n",
       " 3144.3065266935578,\n",
       " 3144.2183509381234,\n",
       " 3144.131224925039,\n",
       " 3144.045135884898,\n",
       " 3143.960071205486,\n",
       " 3143.8760184298376,\n",
       " 3143.7929652543085,\n",
       " 3143.710899526677,\n",
       " 3143.629809244272,\n",
       " 3143.5496825521136,\n",
       " 3143.4705077410895,\n",
       " 3143.392273246146,\n",
       " 3143.3149676445023,\n",
       " 3143.2385796538915,\n",
       " 3143.1630981308176,\n",
       " 3143.088512068838,\n",
       " 3143.0148105968665,\n",
       " 3142.941982977497,\n",
       " 3142.870018605348,\n",
       " 3142.798907005427,\n",
       " 3142.728637831517,\n",
       " 3142.6592008645853,\n",
       " 3142.590586011201,\n",
       " 3142.522783301989,\n",
       " 3142.4557828900874,\n",
       " 3142.389575049635,\n",
       " 3142.3241501742723,\n",
       " 3142.259498775662,\n",
       " 3142.1956114820296,\n",
       " 3142.1324790367175,\n",
       " 3142.0700922967667,\n",
       " 3142.0084422315017,\n",
       " 3141.947519921148,\n",
       " 3141.887316555457,\n",
       " 3141.827823432349,\n",
       " 3141.7690319565795,\n",
       " 3141.7109336384124,\n",
       " 3141.6535200923195,\n",
       " 3141.5967830356867,\n",
       " 3141.5407142875456,\n",
       " 3141.4853057673126,\n",
       " 3141.43054949355,\n",
       " 3141.376437582737,\n",
       " 3141.3229622480612,\n",
       " 3141.270115798221,\n",
       " 3141.217890636248,\n",
       " 3141.166279258333,\n",
       " 3141.1152742526833,\n",
       " 3141.0648682983797,\n",
       " 3141.0150541642524,\n",
       " 3140.9658247077746,\n",
       " 3140.9171728739652,\n",
       " 3140.869091694307,\n",
       " 3140.821574285676,\n",
       " 3140.7746138492907,\n",
       " 3140.728203669664,\n",
       " 3140.6823371135774,\n",
       " 3140.637007629065,\n",
       " 3140.5922087444064,\n",
       " 3140.5479340671372,\n",
       " 3140.5041772830705,\n",
       " 3140.4609321553266,\n",
       " 3140.418192523382,\n",
       " 3140.375952302124,\n",
       " 3140.3342054809195,\n",
       " 3140.292946122694,\n",
       " 3140.2521683630266,\n",
       " 3140.211866409248,\n",
       " 3140.172034539558,\n",
       " 3140.132667102148,\n",
       " 3140.0937585143397,\n",
       " 3140.0553032617286,\n",
       " 3140.017295897342,\n",
       " 3139.979731040808,\n",
       " 3139.9426033775317,\n",
       " 3139.9059076578833,\n",
       " 3139.869638696397,\n",
       " 3139.833791370978,\n",
       " 3139.7983606221233,\n",
       " 3139.7633414521447,\n",
       " 3139.72872892441,\n",
       " 3139.6945181625883,\n",
       " 3139.660704349905,\n",
       " 3139.627282728408,\n",
       " 3139.5942485982428,\n",
       " 3139.5615973169347,\n",
       " 3139.529324298681,\n",
       " 3139.497425013652,\n",
       " 3139.465894987303,\n",
       " 3139.4347297996883,\n",
       " 3139.403925084791,\n",
       " 3139.3734765298577,\n",
       " 3139.343379874742,\n",
       " 3139.3136309112538,\n",
       " 3139.284225482521,\n",
       " 3139.2551594823562,\n",
       " 3139.2264288546326,\n",
       " 3139.1980295926646,\n",
       " 3139.1699577386034,\n",
       " 3139.142209382829,\n",
       " 3139.114780663361,\n",
       " 3139.0876677652686,\n",
       " 3139.0608669200924,\n",
       " 3139.034374405271,\n",
       " 3139.0081865435764,\n",
       " 3138.9822997025553,\n",
       " 3138.956710293978,\n",
       " 3138.931414773293,\n",
       " 3138.9064096390894,\n",
       " 3138.8816914325653,\n",
       " 3138.8572567370043,\n",
       " 3138.833102177256,\n",
       " 3138.8092244192267,\n",
       " 3138.7856201693708,\n",
       " 3138.762286174193,\n",
       " 3138.7392192197563,\n",
       " 3138.716416131194,\n",
       " 3138.6938737722303,\n",
       " 3138.6715890447026,\n",
       " 3138.6495588880966,\n",
       " 3138.6277802790787,\n",
       " 3138.6062502310438,\n",
       " 3138.584965793658,\n",
       " 3138.563924052416,\n",
       " 3138.5431221281997,\n",
       " 3138.5225571768437,\n",
       " 3138.5022263887026,\n",
       " 3138.482126988231,\n",
       " 3138.4622562335594,\n",
       " 3138.4426114160856,\n",
       " 3138.4231898600606,\n",
       " 3138.403988922189,\n",
       " 3138.385005991226,\n",
       " 3138.3662384875884,\n",
       " 3138.347683862961,\n",
       " 3138.329339599915,\n",
       " 3138.3112032115273,\n",
       " 3138.2932722410087,\n",
       " 3138.2755442613293,\n",
       " 3138.258016874855,\n",
       " 3138.2406877129883,\n",
       " 3138.223554435808,\n",
       " 3138.206614731722,\n",
       " 3138.1898663171132,\n",
       " 3138.173306936003,\n",
       " 3138.1569343597052,\n",
       " 3138.1407463864975,\n",
       " 3138.124740841285,\n",
       " 3138.108915575279,\n",
       " 3138.093268465667,\n",
       " 3138.077797415301,\n",
       " 3138.06250035238,\n",
       " 3138.0473752301373,\n",
       " 3138.0324200265354,\n",
       " 3138.017632743963,\n",
       " 3138.0030114089345,\n",
       " 3137.9885540717946,\n",
       " 3137.974258806426,\n",
       " 3137.9601237099596,\n",
       " 3137.946146902493,\n",
       " 3137.932326526806,\n",
       " 3137.918660748082,\n",
       " 3137.9051477536364,\n",
       " 3137.891785752644,\n",
       " 3137.87857297587,\n",
       " 3137.865507675408,\n",
       " 3137.852588124416,\n",
       " 3137.839812616861,\n",
       " 3137.827179467262,\n",
       " 3137.8146870104397,\n",
       " 3137.8023336012684,\n",
       " 3137.790117614427,\n",
       " 3137.778037444163,\n",
       " 3137.766091504047,\n",
       " 3137.7542782267387,\n",
       " 3137.742596063752,\n",
       " 3137.731043485229,\n",
       " 3137.719618979704,\n",
       " 3137.7083210538863,\n",
       " 3137.697148232434,\n",
       " 3137.686099057733,\n",
       " 3137.6751720896855,\n",
       " 3137.6643659054907,\n",
       " 3137.653679099436,\n",
       " 3137.643110282687,\n",
       " 3137.632658083083,\n",
       " 3137.6223211449305,\n",
       " 3137.612098128804,\n",
       " 3137.6019877113463,\n",
       " 3137.591988585073,\n",
       " 3137.582099458178,\n",
       " 3137.5723190543436,\n",
       " 3137.5626461125457,\n",
       " 3137.553079386877,\n",
       " 3137.5436176463522,\n",
       " 3137.534259674734,\n",
       " 3137.5250042703465,\n",
       " 3137.515850245902,\n",
       " 3137.5067964283235,\n",
       " 3137.4978416585727,\n",
       " 3137.4889847914774,\n",
       " 3137.4802246955614,\n",
       " 3137.4715602528818,\n",
       " 3137.462990358857,\n",
       " 3137.454513922112,\n",
       " 3137.44612986431,\n",
       " 3137.437837119998,\n",
       " 3137.4296346364495,\n",
       " 3137.4215213735065,\n",
       " 3137.4134963034294,\n",
       " 3137.405558410744,\n",
       " 3137.397706692093,\n",
       " 3137.389940156088,\n",
       " 3137.3822578231634,\n",
       " 3137.3746587254336,\n",
       " 3137.36714190655,\n",
       " 3137.3597064215596,\n",
       " 3137.35235133677,\n",
       " 3137.345075729606,\n",
       " 3137.337878688481,\n",
       " 3137.3307593126588,\n",
       " 3137.3237167121247,\n",
       " 3137.3167500074514,\n",
       " 3137.309858329675,\n",
       " 3137.303040820164,\n",
       " 3137.2962966304954,\n",
       " 3137.289624922328,\n",
       " 3137.2830248672863,\n",
       " 3137.276495646831,\n",
       " 3137.2700364521465,\n",
       " 3137.2636464840193,\n",
       " 3137.257324952721,\n",
       " 3137.2510710778943,\n",
       " 3137.2448840884413,\n",
       " 3137.238763222406,\n",
       " 3137.2327077268683,\n",
       " 3137.226716857831,\n",
       " 3137.220789880114,\n",
       " 3137.214926067245,\n",
       " 3137.209124701355,\n",
       " 3137.2033850730754,\n",
       " 3137.1977064814323,\n",
       " 3137.1920882337463,\n",
       " 3137.186529645531,\n",
       " 3137.1810300403954,\n",
       " 3137.1755887499444,\n",
       " 3137.1702051136813,\n",
       " 3137.164878478914,\n",
       " 3137.15960820066,\n",
       " 3137.1543936415496,\n",
       " 3137.1492341717417,\n",
       " 3137.144129168824,\n",
       " 3137.1390780177258,\n",
       " 3137.134080110635,\n",
       " 3137.1291348468994,\n",
       " 3137.124241632951,\n",
       " 3137.1193998822127,\n",
       " 3137.114609015018,\n",
       " 3137.109868458524,\n",
       " 3137.105177646632,\n",
       " 3137.100536019905,\n",
       " 3137.0959430254875,\n",
       " 3137.0913981170233,\n",
       " 3137.086900754582,\n",
       " 3137.0824504045763,\n",
       " 3137.0780465396897,\n",
       " 3137.073688638796,\n",
       " 3137.069376186891,\n",
       " 3137.0651086750127,\n",
       " 3137.06088560017,\n",
       " 3137.0567064652732,\n",
       " 3137.052570779061,\n",
       " 3137.0484780560278,\n",
       " 3137.0444278163604,\n",
       " 3137.0404195858623,\n",
       " 3137.036452895891,\n",
       " 3137.0325272832906,\n",
       " 3137.0286422903214,\n",
       " 3137.024797464601,\n",
       " 3137.0209923590355,\n",
       " 3137.0172265317556,\n",
       " 3137.0134995460576,\n",
       " 3137.009810970337,\n",
       " 3137.0061603780296,\n",
       " 3137.0025473475503,\n",
       " 3136.998971462231,\n",
       " 3136.995432310267,\n",
       " 3136.9919294846522,\n",
       " 3136.9884625831255,\n",
       " 3136.9850312081126,\n",
       " 3136.981634966669,\n",
       " 3136.978273470425,\n",
       " 3136.974946335532,\n",
       " 3136.971653182606,\n",
       " 3136.9683936366755,\n",
       " 3136.9651673271273,\n",
       " 3136.961973887655,\n",
       " 3136.9588129562085,\n",
       " 3136.955684174941,\n",
       " 3136.9525871901587,\n",
       " 3136.949521652273,\n",
       " 3136.946487215748,\n",
       " 3136.943483539056,\n",
       " 3136.940510284628,\n",
       " 3136.9375671188013,\n",
       " 3136.9346537117813,\n",
       " 3136.9317697375905,\n",
       " 3136.9289148740204,\n",
       " 3136.9260888025924,\n",
       " 3136.9232912085095,\n",
       " 3136.9205217806116,\n",
       " 3136.9177802113336,\n",
       " 3136.915066196663,\n",
       " 3136.9123794360953,\n",
       " 3136.9097196325943,\n",
       " 3136.9070864925493,\n",
       " 3136.904479725734,\n",
       " 3136.901899045267,\n",
       " 3136.8993441675725,\n",
       " 3136.8968148123377,\n",
       " 3136.8943107024784,\n",
       " 3136.8918315640963,\n",
       " 3136.889377126443,\n",
       " 3136.8869471218836,\n",
       " 3136.884541285857,\n",
       " 3136.882159356841,\n",
       " 3136.8798010763144,\n",
       " 3136.877466188725,\n",
       " 3136.8751544414495,\n",
       " 3136.872865584762,\n",
       " 3136.8705993717977,\n",
       " 3136.8683555585194,\n",
       " 3136.8661339036853,\n",
       " 3136.8639341688117,\n",
       " 3136.861756118147,\n",
       " 3136.859599518632,\n",
       " 3136.857464139872,\n",
       " 3136.855349754104,\n",
       " 3136.8532561361662,\n",
       " 3136.8511830634666,\n",
       " 3136.849130315952,\n",
       " 3136.8470976760796,\n",
       " 3136.845084928784,\n",
       " 3136.8430918614517,\n",
       " 3136.841118263889,\n",
       " 3136.839163928295,\n",
       " 3136.837228649232,\n",
       " 3136.8353122235994,\n",
       " 3136.8334144506043,\n",
       " 3136.8315351317347,\n",
       " 3136.8296740707324,\n",
       " 3136.8278310735664,\n",
       " 3136.8260059484073,\n",
       " 3136.8241985055997,\n",
       " 3136.822408557638,\n",
       " 3136.82063591914,\n",
       " 3136.8188804068222,\n",
       " 3136.817141839476,\n",
       " 3136.8154200379413,\n",
       " 3136.813714825082,\n",
       " 3136.812026025766,\n",
       " 3136.810353466838,\n",
       " 3136.808696977095,\n",
       " 3136.8070563872684,\n",
       " 3136.805431529996,\n",
       " 3136.8038222398036,\n",
       " 3136.8022283530786,\n",
       " 3136.8006497080523,\n",
       " 3136.7990861447756,\n",
       " 3136.797537505098,\n",
       " 3136.7960036326467,\n",
       " 3136.7944843728064,\n",
       " 3136.792979572697,\n",
       " 3136.7914890811544,\n",
       " 3136.790012748711,\n",
       " 3136.788550427573,\n",
       " 3136.787101971604,\n",
       " 3136.7856672363037,\n",
       " 3136.7842460787892,\n",
       " 3136.782838357776,\n",
       " 3136.7814439335602,\n",
       " 3136.780062667996,\n",
       " 3136.7786944244845,\n",
       " 3136.777339067948,\n",
       " 3136.775996464818,\n",
       " 3136.7746664830142,\n",
       " 3136.773348991929,\n",
       " 3136.7720438624087,\n",
       " 3136.770750966736,\n",
       " 3136.769470178618,\n",
       " 3136.7682013731624,\n",
       " 3136.7669444268663,\n",
       " 3136.7656992175985,\n",
       " 3136.764465624583,\n",
       " 3136.763243528383,\n",
       " 3136.7620328108883,\n",
       " 3136.760833355294,\n",
       " 3136.7596450460906,\n",
       " 3136.7584677690475,\n",
       " 3136.7573014111963,\n",
       " 3136.756145860818,\n",
       " 3136.7550010074274,\n",
       " 3136.7538667417602,\n",
       " 3136.7527429557563,\n",
       " 3136.7516295425494,\n",
       " 3136.750526396449,\n",
       " 3136.7494334129296,\n",
       " 3136.748350488617,\n",
       " 3136.7472775212727,\n",
       " 3136.746214409783,\n",
       " 3136.745161054147,\n",
       " 3136.74411735546,\n",
       " 3136.743083215902,\n",
       " 3136.7420585387285,\n",
       " 3136.741043228253,\n",
       " 3136.7400371898398,\n",
       " 3136.739040329886,\n",
       " 3136.7380525558146,\n",
       " 3136.7370737760616,\n",
       " 3136.736103900062,\n",
       " 3136.7351428382412,\n",
       " 3136.7341905020007,\n",
       " 3136.733246803709,\n",
       " 3136.7323116566886,\n",
       " 3136.731384975208,\n",
       " 3136.730466674469,\n",
       " 3136.729556670593,\n",
       " 3136.728654880617,\n",
       " 3136.727761222476,\n",
       " 3136.726875614998,\n",
       " 3136.725997977891,\n",
       " 3136.725128231734,\n",
       " 3136.724266297966,\n",
       " 3136.723412098877,\n",
       " 3136.7225655575985,\n",
       " 3136.721726598092,\n",
       " 3136.720895145141,\n",
       " 3136.720071124344,\n",
       " 3136.719254462098,\n",
       " 3136.718445085596,\n",
       " 3136.717642922817,\n",
       " 3136.7168479025136,\n",
       " 3136.716059954207,\n",
       " 3136.7152790081736,\n",
       " 3136.7145049954433,\n",
       " 3136.7137378477832,\n",
       " 3136.7129774976947,\n",
       " 3136.712223878403,\n",
       " 3136.7114769238487,\n",
       " 3136.710736568679,\n",
       " 3136.7100027482434,\n",
       " 3136.7092753985794,\n",
       " 3136.7085544564115,\n",
       " 3136.7078398591375,\n",
       " 3136.707131544825,\n",
       " 3136.7064294522015,\n",
       " 3136.7057335206473,\n",
       " 3136.705043690189,\n",
       " 3136.7043599014914,\n",
       " 3136.7036820958497,\n",
       " 3136.7030102151834,\n",
       " 3136.7023442020295,\n",
       " 3136.7016839995345,\n",
       " 3136.7010295514465,\n",
       " 3136.7003808021113,\n",
       " 3136.699737696464,\n",
       " 3136.6991001800216,\n",
       " 3136.6984681988774,\n",
       " 3136.697841699694,\n",
       " 3136.6972206296973,\n",
       " 3136.6966049366715,\n",
       " 3136.695994568948,\n",
       " 3136.6953894754065,\n",
       " 3136.6947896054617,\n",
       " 3136.694194909062,\n",
       " 3136.69360533668,\n",
       " 3136.693020839311,\n",
       " 3136.692441368464,\n",
       " 3136.6918668761546,\n",
       " 3136.6912973149037,\n",
       " 3136.690732637728,\n",
       " 3136.6901727981353,\n",
       " 3136.689617750121,\n",
       " 3136.6890674481606,\n",
       " 3136.6885218472034,\n",
       " 3136.687980902671,\n",
       " 3136.687444570447,\n",
       " 3136.6869128068765,\n",
       " 3136.686385568759,\n",
       " 3136.685862813341,\n",
       " 3136.6853444983158,\n",
       " 3136.684830581814,\n",
       " 3136.684321022402,\n",
       " 3136.683815779074,\n",
       " 3136.683314811251,\n",
       " 3136.6828180787716,\n",
       " 3136.6823255418926,\n",
       " 3136.6818371612785,\n",
       " 3136.6813528980015,\n",
       " 3136.680872713535,\n",
       " 3136.6803965697504,\n",
       " 3136.67992442891,\n",
       " 3136.679456253666,\n",
       " 3136.678992007054,\n",
       " 3136.6785316524893,\n",
       " 3136.6780751537635,\n",
       " 3136.677622475039,\n",
       " 3136.677173580845,\n",
       " 3136.6767284360753,\n",
       " 3136.6762870059815,\n",
       " 3136.675849256171,\n",
       " 3136.675415152604,\n",
       " 3136.6749846615858,\n",
       " 3136.674557749766,\n",
       " 3136.6741343841354,\n",
       " 3136.673714532019,\n",
       " 3136.6732981610758,\n",
       " 3136.672885239292,\n",
       " 3136.6724757349807,\n",
       " 3136.6720696167754,\n",
       " 3136.671666853627,\n",
       " 3136.671267414803,\n",
       " 3136.6708712698796,\n",
       " 3136.6704783887426,\n",
       " 3136.670088741581,\n",
       " 3136.6697022988856,\n",
       " 3136.669319031443,\n",
       " 3136.668938910337,\n",
       " 3136.668561906941,\n",
       " 3136.668187992916,\n",
       " 3136.667817140209,\n",
       " 3136.6674493210476,\n",
       " 3136.6670845079393,\n",
       " 3136.6667226736668,\n",
       " 3136.6663637912857,\n",
       " 3136.66600783412,\n",
       " 3136.665654775762,\n",
       " 3136.6653045900675,\n",
       " 3136.6649572511533,\n",
       " 3136.6646127333934,\n",
       " 3136.664271011419,\n",
       " 3136.663932060112,\n",
       " 3136.663595854606,\n",
       " 3136.6632623702794,\n",
       " 3136.662931582758,\n",
       " 3136.662603467908,\n",
       " 3136.6622780018333,\n",
       " 3136.661955160877,\n",
       " 3136.661634921615,\n",
       " 3136.661317260856,\n",
       " 3136.6610021556353,\n",
       " 3136.660689583217,\n",
       " 3136.6603795210895,\n",
       " 3136.660071946961,\n",
       " 3136.6597668387617,\n",
       " 3136.6594641746356,\n",
       " 3136.6591639329445,\n",
       " 3136.6588660922616,\n",
       " 3136.658570631369,\n",
       " 3136.6582775292586,\n",
       " 3136.657986765127,\n",
       " 3136.657698318373,\n",
       " 3136.6574121685994,\n",
       " 3136.657128295606,\n",
       " 3136.6568466793906,\n",
       " 3136.656567300146,\n",
       " 3136.6562901382567,\n",
       " 3136.656015174299,\n",
       " 3136.655742389038,\n",
       " 3136.6554717634253,\n",
       " 3136.655203278597,\n",
       " 3136.6549369158724,\n",
       " 3136.654672656751,\n",
       " 3136.6544104829127,\n",
       " 3136.654150376213,\n",
       " 3136.6538923186827,\n",
       " 3136.653636292527,\n",
       " 3136.653382280122,\n",
       " 3136.653130264013,\n",
       " 3136.6528802269136,\n",
       " 3136.6526321517035,\n",
       " 3136.6523860214265,\n",
       " 3136.65214181929,\n",
       " 3136.651899528662,\n",
       " 3136.651659133069,\n",
       " 3136.651420616195,\n",
       " 3136.6511839618806,\n",
       " 3136.650949154121,\n",
       " 3136.6507161770633,\n",
       " 3136.6504850150063,\n",
       " 3136.650255652397,\n",
       " 3136.650028073832,\n",
       " 3136.649802264053,\n",
       " 3136.6495782079473,\n",
       " 3136.6493558905463,\n",
       " 3136.6491352970206,\n",
       " 3136.648916412684,\n",
       " 3136.648699222987,\n",
       " 3136.648483713519,\n",
       " 3136.648269870005,\n",
       " 3136.648057678305,\n",
       " 3136.647847124412,\n",
       " 3136.647638194449,\n",
       " 3136.647430874674,\n",
       " 3136.647225151469,\n",
       " 3136.6470210113466,\n",
       " 3136.6468184409455,\n",
       " 3136.64661742703,\n",
       " 3136.6464179564864,\n",
       " 3136.6462200163264,\n",
       " 3136.646023593681,\n",
       " 3136.6458286758007,\n",
       " 3136.6456352500577,\n",
       " 3136.645443303939,\n",
       " 3136.645252825049,\n",
       " 3136.645063801108,\n",
       " 3136.6448762199493,\n",
       " 3136.6446900695205,\n",
       " 3136.6445053378784,\n",
       " 3136.644322013193,\n",
       " 3136.644140083743,\n",
       " 3136.643959537915,\n",
       " 3136.643780364202,\n",
       " 3136.6436025512053,\n",
       " 3136.6434260876285,\n",
       " 3136.643250962283,\n",
       " 3136.6430771640794,\n",
       " 3136.6429046820317,\n",
       " 3136.6427335052545,\n",
       " 3136.642563622963,\n",
       " 3136.6423950244707,\n",
       " 3136.642227699189,\n",
       " 3136.6420616366254,\n",
       " 3136.641896826384,\n",
       " 3136.6417332581636,\n",
       " 3136.6415709217567,\n",
       " 3136.6414098070504,\n",
       " 3136.6412499040202,\n",
       " 3136.641091202737,\n",
       " 3136.6409336933584,\n",
       " 3136.640777366133,\n",
       " 3136.640622211399,\n",
       " 3136.64046821958,\n",
       " 3136.640315381185,\n",
       " 3136.6401636868122,\n",
       " 3136.6400131271434,\n",
       " 3136.639863692944,\n",
       " 3136.639715375063,\n",
       " 3136.6395681644312,\n",
       " 3136.639422052061,\n",
       " 3136.6392770290463,\n",
       " 3136.6391330865617,\n",
       " 3136.638990215858,\n",
       " 3136.6388484082677,\n",
       " 3136.6387076551982,\n",
       " 3136.6385679481364,\n",
       " 3136.6384292786424,\n",
       " 3136.6382916383536,\n",
       " 3136.6381550189817,\n",
       " 3136.6380194123108,\n",
       " 3136.6378848102004,\n",
       " 3136.637751204581,\n",
       " 3136.637618587455,\n",
       " 3136.637486950895,\n",
       " 3136.6373562870444,\n",
       " 3136.6372265881178,\n",
       " 3136.6370978463965,\n",
       " 3136.63697005423,\n",
       " 3136.636843204037,\n",
       " 3136.6367172883015,\n",
       " 3136.6365922995756,\n",
       " 3136.6364682304747,\n",
       " 3136.636345073681,\n",
       " 3136.636222821939,\n",
       " 3136.636101468059,\n",
       " 3136.6359810049144,\n",
       " 3136.635861425438,\n",
       " 3136.6357427226276,\n",
       " 3136.635624889542,\n",
       " 3136.6355079192995,\n",
       " 3136.6353918050777,\n",
       " 3136.635276540116,\n",
       " 3136.6351621177114,\n",
       " 3136.6350485312187,\n",
       " 3136.6349357740514,\n",
       " 3136.6348238396804,\n",
       " 3136.6347127216322,\n",
       " 3136.6346024134905,\n",
       " 3136.634492908893,\n",
       " 3136.6343842015344,\n",
       " 3136.634276285163,\n",
       " 3136.6341691535813,\n",
       " 3136.6340628006446,\n",
       " 3136.633957220262,\n",
       " 3136.633852406394,\n",
       " 3136.6337483530547,\n",
       " 3136.6336450543085,\n",
       " 3136.6335425042703,\n",
       " 3136.633440697107,\n",
       " 3136.633339627034,\n",
       " 3136.633239288317,\n",
       " 3136.633139675271,\n",
       " 3136.6330407822584,\n",
       " 3136.6329426036923,\n",
       " 3136.6328451340296,\n",
       " 3136.632748367778,\n",
       " 3136.6326522994896,\n",
       " 3136.6325569237642,\n",
       " 3136.632462235247,\n",
       " 3136.6323682286284,\n",
       " 3136.6322748986445,\n",
       " 3136.632182240075,\n",
       " 3136.6320902477455,\n",
       " 3136.6319989165236,\n",
       " 3136.63190824132,\n",
       " 3136.631818217091,\n",
       " 3136.631728838833,\n",
       " 3136.631640101585,\n",
       " 3136.631552000429,\n",
       " 3136.6314645304865,\n",
       " 3136.631377686921,\n",
       " 3136.631291464937,\n",
       " 3136.631205859779,\n",
       " 3136.6311208667294,\n",
       " 3136.631036481114,\n",
       " 3136.6309526982945,\n",
       " 3136.6308695136713,\n",
       " 3136.6307869226857,\n",
       " 3136.630704920815,\n",
       " 3136.630623503575,\n",
       " 3136.630542666518,\n",
       " 3136.6304624052345,\n",
       " 3136.630382715351,\n",
       " 3136.63030359253,\n",
       " 3136.6302250324698,\n",
       " 3136.6301470309063,\n",
       " 3136.6300695836076,\n",
       " 3136.6299926863794,\n",
       " 3136.6299163350614,\n",
       " 3136.6298405255266,\n",
       " 3136.6297652536828,\n",
       " 3136.6296905154722,\n",
       " 3136.6296163068696,\n",
       " 3136.6295426238826,\n",
       " 3136.629469462553,\n",
       " 3136.629396818954,\n",
       " 3136.629324689189,\n",
       " 3136.629253069399,\n",
       " 3136.6291819557514,\n",
       " 3136.6291113444468,\n",
       " ...]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Regression Model\n",
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge regression linear model intercept: 7.009233212820845\n",
      "ridge regression linear model coeff:\n",
      "[ 0.41417311  0.40321748  0.19917968 -0.29035188  0.26331594]\n",
      "R-squared score (training): 0.417\n",
      "R-squared score (test): 0.443\n",
      "Number of non-zero features: 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "linridge = Ridge(alpha=25.0).fit(X_train, y_train)\n",
    "\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test, y_test)))\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-352-d71bc87e92fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX_train_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX_test_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlinridge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRidge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         X = check_array(X, copy=self.copy, dtype=FLOAT_DTYPES,\n\u001b[0m\u001b[1;32m    408\u001b[0m                         force_all_finite=\"allow-nan\")\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m             raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n\u001b[0m\u001b[1;32m    651\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                              % (n_samples, array.shape, ensure_min_samples,\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "# Ridge regression with feature normalization\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linridge = Ridge(alpha=20.0).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Wheat dataset')\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test_scaled, y_test)))\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression: effect of alpha regularization parameter\n",
      "\n",
      "Alpha = 0.00\n",
      "num abs(coeff) > 1.0: 5, r-squared training: 0.42, r-squared test: 0.44\n",
      "\n",
      "Alpha = 1.00\n",
      "num abs(coeff) > 1.0: 5, r-squared training: 0.41, r-squared test: 0.44\n",
      "\n",
      "Alpha = 10.00\n",
      "num abs(coeff) > 1.0: 5, r-squared training: 0.32, r-squared test: 0.33\n",
      "\n",
      "Alpha = 20.00\n",
      "num abs(coeff) > 1.0: 5, r-squared training: 0.26, r-squared test: 0.27\n",
      "\n",
      "Alpha = 50.00\n",
      "num abs(coeff) > 1.0: 5, r-squared training: 0.18, r-squared test: 0.18\n",
      "\n",
      "Alpha = 100.00\n",
      "num abs(coeff) > 1.0: 5, r-squared training: 0.11, r-squared test: 0.11\n",
      "\n",
      "Alpha = 1000.00\n",
      "num abs(coeff) > 1.0: 2, r-squared training: 0.02, r-squared test: 0.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ridge regression with regularization parameter: alpha\n",
    "\n",
    "print('Ridge regression: effect of alpha regularization parameter\\n')\n",
    "for this_alpha in [0, 1, 10, 20, 50, 100, 1000]:\n",
    "    linridge = Ridge(alpha = this_alpha).fit(X_train_scaled, y_train)\n",
    "    r2_train = linridge.score(X_train_scaled, y_train)\n",
    "    r2_test = linridge.score(X_test_scaled, y_test)\n",
    "    num_coeff_bigger = np.sum(abs(linridge.coef_) > 1.0)\n",
    "    print('Alpha = {:.2f}\\nnum abs(coeff) > 1.0: {}, \\\n",
    "r-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n",
    "         .format(this_alpha, num_coeff_bigger, r2_train, r2_test))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso regression linear model intercept: 66.20384241687611\n",
      "lasso regression linear model coeff:\n",
      "[34.28143912]\n",
      "Non-zero features: 1\n",
      "R-squared score (training): 0.089\n",
      "R-squared score (test): 0.084\n",
      "\n",
      "Features with non-zero weight (sorted by absolute magnitude):\n",
      "\tHP, 34.281\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linlasso = Lasso(alpha=1, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('lasso regression linear model intercept: {}'\n",
    "     .format(linlasso.intercept_))\n",
    "print('lasso regression linear model coeff:\\n{}'\n",
    "     .format(linlasso.coef_))\n",
    "print('Non-zero features: {}'\n",
    "     .format(np.sum(linlasso.coef_ != 0)))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linlasso.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}\\n'\n",
    "     .format(linlasso.score(X_test_scaled, y_test)))\n",
    "print('Features with non-zero weight (sorted by absolute magnitude):')\n",
    "\n",
    "for e in sorted (list(zip(list(X), linlasso.coef_)),\n",
    "                key = lambda e: -abs(e[1])):\n",
    "    if e[1] != 0:\n",
    "        print('\\t{}, {:.3f}'.format(e[0], e[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso regression: effect of alpha regularization\n",
      "parameter on number of features kept in final model\n",
      "\n",
      "Alpha = 0.50\n",
      "Features kept: 1, r-squared training: 0.17, r-squared test: 0.14\n",
      "\n",
      "Alpha = 1.00\n",
      "Features kept: 1, r-squared training: 0.09, r-squared test: 0.08\n",
      "\n",
      "Alpha = 2.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n",
      "Alpha = 3.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n",
      "Alpha = 5.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n",
      "Alpha = 10.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n",
      "Alpha = 20.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n",
      "Alpha = 50.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Lasso regression: effect of alpha regularization\\n\\\n",
    "parameter on number of features kept in final model\\n')\n",
    "\n",
    "for alpha in [0.5, 1, 2, 3, 5, 10, 20, 50]:\n",
    "    linlasso = Lasso(alpha, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "    r2_train = linlasso.score(X_train_scaled, y_train)\n",
    "    r2_test = linlasso.score(X_test_scaled, y_test)\n",
    "    \n",
    "    print('Alpha = {:.2f}\\nFeatures kept: {}, r-squared training: {:.2f}, \\\n",
    "r-squared test: {:.2f}\\n'\n",
    "         .format(alpha, np.sum(linlasso.coef_ != 0), r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import KFold # import k-fold validation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLYNOMIAL DEGREE-2 W/ LINEAR REGRESSION\n",
      "(poly deg 2) linear model coeff (w):\n",
      "[ 0.00000000e+00  7.83632395e-01  5.81316973e-01  8.36628758e-02\n",
      " -3.03589224e-01  2.17935028e-01 -2.64092610e-03  4.12878549e-03\n",
      " -2.05705057e-03 -1.36610177e-03 -5.91249232e-06 -3.96493171e-03\n",
      " -2.46874919e-03  2.19326197e-03  4.71140219e-03  5.30018156e-03\n",
      " -4.63437500e-03 -2.32196045e-03  2.37022152e-03 -8.05809773e-04\n",
      " -6.17839096e-04]\n",
      "(poly deg 2) linear model intercept (b): -6.663\n",
      "(poly deg 2) R-squared score (training): 0.514\n",
      "(poly deg 2) R-squared score (test): 0.477\n",
      "(poly deg 2) RMSE score (train): 20.939\n",
      "(poly deg 2) RMSE score (test): 21.262\n",
      "\n",
      "POLYNOMIAL DEGREE-3 W/ LINEAR REGRESSION\n",
      "(poly deg 3) linear model coeff (w):\n",
      "[ 1.09582330e-10  9.63693242e-01 -1.68716025e+00  2.32220783e+00\n",
      " -1.47109972e+00 -7.42478680e-02 -9.34907237e-03  1.87006590e-02\n",
      " -2.78074030e-02  1.70931737e-02  4.16698973e-03  1.27650489e-02\n",
      " -5.55729820e-03  1.19546395e-02  1.94395528e-02 -8.89151315e-03\n",
      " -8.15594151e-04 -5.39787789e-03  2.30581295e-03 -6.37891267e-03\n",
      " -2.27424818e-03  1.09719466e-05 -3.46605342e-05  1.01164047e-04\n",
      "  4.54683442e-07 -2.38523633e-05 -4.23440735e-05  1.91696677e-04\n",
      " -1.53060933e-04 -4.38951706e-05 -2.40531362e-04  1.54108782e-04\n",
      "  2.85242718e-04 -7.13653918e-05 -8.80119423e-05 -7.63471778e-05\n",
      " -4.69359841e-05 -3.44315685e-05  3.61804724e-05 -6.76359005e-05\n",
      "  4.60722453e-05 -2.82363629e-04  1.90551047e-04  1.03285955e-04\n",
      " -9.94710864e-05 -6.23269954e-05  2.38384891e-05  1.66829933e-04\n",
      "  9.79938105e-05  4.91061040e-06 -3.34109916e-04 -1.44307008e-04\n",
      " -2.27800627e-05  9.08953659e-05  2.27827441e-04  1.89342535e-05]\n",
      "(poly deg 3) linear model intercept (b): 19.873\n",
      "(poly deg 3) R-squared score (training): 0.609\n",
      "(poly deg 3) R-squared score (test): -1.122\n",
      "(poly deg 3) RMSE score (train): 18.796\n",
      "(poly deg 3) RMSE score (test): 42.820\n",
      "\n",
      "POLYNOMIAL DEGREE-5 W/ LINEAR REGRESSION\n",
      "(poly deg 5) linear model coeff (w):\n",
      "[ 2.80817175e+03 -7.67431645e+00  3.53508781e+00  2.66288734e+01\n",
      "  3.66818142e+01 -2.00835710e+01 -2.15536924e-02  4.92799198e-01\n",
      "  3.76759674e-01 -5.11396640e-01  2.06149314e-01  1.02829334e-01\n",
      " -2.87443219e-01 -8.66152664e-01  1.06589272e-01 -5.97216748e-01\n",
      " -4.25401258e-01  9.17776323e-02 -3.37565700e-01 -4.58518810e-02\n",
      "  4.39097245e-01  1.63688866e-03 -7.24823623e-03 -1.86092436e-03\n",
      "  4.11333251e-03 -1.92415017e-04 -6.91686293e-03 -5.81807999e-03\n",
      "  1.22808831e-02 -1.65418982e-03  5.52265724e-04 -1.89868912e-03\n",
      " -5.29225647e-03  3.38378940e-03 -7.05089033e-04 -1.05955300e-03\n",
      " -2.03011707e-03 -1.37852717e-03  1.04703567e-02  3.64974667e-03\n",
      "  7.87859937e-03  1.00444448e-02 -8.44189611e-03 -1.01898818e-03\n",
      "  2.35885588e-04 -2.88834126e-04  4.97537647e-03 -6.08704291e-03\n",
      "  6.96644787e-03  9.98325217e-03  5.07435660e-03 -5.44667365e-03\n",
      "  9.43091358e-05  3.35471491e-03 -4.24236557e-03 -2.91354605e-03\n",
      " -1.70644357e-05  5.65695166e-05  4.46820391e-06 -1.32617236e-05\n",
      " -3.40772175e-06  1.01980315e-04 -8.26119227e-05 -7.70069435e-05\n",
      " -3.99653760e-05 -2.20168252e-05  1.24536126e-04  2.45151478e-05\n",
      " -4.31487769e-05 -3.28409601e-05  2.94287818e-05 -2.30506674e-05\n",
      "  2.25280530e-05  7.27764342e-06  3.91652322e-05  2.08142127e-05\n",
      "  1.05757355e-04  1.86024681e-04 -6.22085550e-05 -1.30953816e-04\n",
      "  1.14946773e-05  7.23401556e-05 -1.04375332e-04 -1.36618837e-04\n",
      " -1.39015504e-04  2.27100406e-04 -3.49000532e-05  5.32421571e-05\n",
      " -5.54313627e-05  6.88730153e-05 -1.66297358e-05  9.87042542e-06\n",
      "  5.34633709e-05 -3.37439191e-05  5.64895807e-06 -7.90749837e-05\n",
      "  5.54352155e-06 -3.81092536e-05 -7.59118790e-05 -7.97083346e-05\n",
      " -3.21835388e-05  1.83515800e-07 -5.36969488e-05  1.60032192e-05\n",
      " -1.19269443e-04 -5.77026460e-05  9.19486732e-05  5.12226678e-05\n",
      "  1.93648130e-04 -5.56698530e-05  6.87369038e-06 -3.45548176e-05\n",
      " -3.35646119e-05 -1.66768423e-05  1.89161033e-04  2.65581361e-05\n",
      " -2.83953137e-05 -6.61039284e-05 -2.52372973e-04  9.00796301e-05\n",
      "  2.29027219e-05 -1.27864538e-05  5.31083861e-05 -8.20124689e-05\n",
      "  6.03348378e-05  2.45345520e-06  6.04021153e-08 -2.91984579e-08\n",
      " -1.56104066e-07 -4.55378046e-08  3.49725626e-08 -5.72845449e-07\n",
      "  6.87841821e-07 -1.55999473e-07  1.98899713e-07 -4.35439249e-08\n",
      " -3.56385890e-07  4.19249886e-07  5.11648632e-07 -2.69485630e-07\n",
      " -1.72559155e-07 -7.19265485e-08 -3.35200149e-07  1.08832516e-06\n",
      " -2.45186935e-07  2.78258364e-07 -5.59163676e-07 -2.60742886e-07\n",
      " -2.48189168e-07  2.99646269e-07 -4.45244752e-08  6.90715289e-08\n",
      " -9.67132827e-08  8.97366235e-08  1.08507267e-06 -2.18597733e-06\n",
      "  3.76312288e-07 -8.30386007e-07  1.05540929e-06  4.34158152e-07\n",
      " -1.74188651e-07  3.92904204e-07  5.76573669e-08 -8.14428125e-07\n",
      " -5.29140200e-07  6.96156092e-07 -6.03597586e-07 -8.95918339e-07\n",
      " -6.37654079e-07  2.29582595e-06  2.65212198e-07 -1.51021843e-06\n",
      "  1.40835017e-06  1.17120723e-06 -2.57244917e-07 -1.16584688e-06\n",
      " -1.01530162e-06  9.39564599e-07 -5.79691676e-07 -3.00843558e-07\n",
      "  2.10347609e-07 -5.90901283e-09  1.21548016e-07  2.65694676e-07\n",
      " -4.44313923e-08 -1.05461992e-07 -5.67871368e-08 -5.58576429e-07\n",
      "  1.62310354e-06 -2.27250481e-07  3.61037141e-07  3.74171960e-07\n",
      " -7.77070272e-07  7.04077152e-09 -5.32933750e-07  1.11690268e-07\n",
      " -6.21778611e-08 -2.97650233e-08  2.92635551e-08 -1.25529501e-07\n",
      " -1.75146853e-07 -9.27235618e-07  8.17434434e-07  6.68213747e-07\n",
      "  6.78169158e-07 -2.80449921e-07  6.44585188e-07  4.50012316e-08\n",
      " -1.34283204e-06  1.83208754e-06 -1.47024466e-07  1.01680127e-06\n",
      " -1.61267701e-07 -1.73058216e-06  1.92833836e-07 -6.46719294e-08\n",
      " -2.26684145e-07  6.75944675e-07  5.55630146e-07 -1.90651093e-06\n",
      "  1.01546178e-06 -8.10527448e-07  4.01995205e-07  5.62451142e-07\n",
      " -5.98052346e-07  4.30256719e-10 -3.43544693e-07 -5.52374919e-08\n",
      "  5.87175651e-07  1.04381696e-07 -6.96546381e-08  1.51492376e-07\n",
      " -3.23784901e-08 -2.04684400e-07 -9.71172218e-08  4.51696112e-08\n",
      "  1.79548483e-07  1.31667855e-07 -6.87250472e-07  5.64855780e-08\n",
      "  1.73708856e-07  1.28824384e-07  4.93795492e-07  5.16889389e-07\n",
      " -5.38109483e-07 -1.05741331e-07 -3.31211627e-08 -5.75150505e-09\n",
      " -2.02596021e-07  2.33843242e-07 -9.04279784e-08  2.34385968e-08]\n",
      "(poly deg 5) linear model intercept (b): -3229.763\n",
      "(poly deg 5) R-squared score (training): 0.792\n",
      "(poly deg 5) R-squared score (test): -2495.770\n",
      "(poly deg 5) RMSE score (train): 13.716\n",
      "(poly deg 5) RMSE score (test): 1468.935\n",
      "\n",
      "POLYNOMIAL DEGREE-10 W/ LINEAR REGRESSION\n",
      "(poly deg 10) linear model coeff (w):\n",
      "[ 2.49200575e-12 -1.24200888e-11  1.24983611e-11 ...  4.48849851e-14\n",
      " -1.30415844e-13 -1.00614254e-13]\n",
      "(poly deg 10) linear model intercept (b): 23.903\n",
      "(poly deg 10) R-squared score (training): 1.000\n",
      "(poly deg 10) R-squared score (test): -12398793083561872.000\n",
      "(poly deg 10) RMSE score (train): 0.020\n",
      "(poly deg 10) RMSE score (test): 3273429738.344\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for degree in [2, 3, 5,10]:\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_F1_poly = poly.fit_transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y, random_state=0)\n",
    "    linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "    print('POLYNOMIAL DEGREE-{} W/ LINEAR REGRESSION'.format(degree))\n",
    "    print('(poly deg {}) linear model coeff (w):\\n{}'\n",
    "         .format(degree, linreg.coef_))\n",
    "    print('(poly deg {}) linear model intercept (b): {:.3f}'\n",
    "         .format(degree, linreg.intercept_))\n",
    "    print('(poly deg {}) R-squared score (training): {:.3f}'\n",
    "         .format(degree, linreg.score(X_train, y_train)))\n",
    "    print('(poly deg {}) R-squared score (test): {:.3f}'\n",
    "         .format(degree, linreg.score(X_test, y_test)))\n",
    "    print('(poly deg {}) RMSE score (train): {:.3f}'\n",
    "         .format(degree, sqrt(mean_squared_error(y_train, linreg.predict(X_train)))))\n",
    "    print('(poly deg {}) RMSE score (test): {:.3f}\\n'\n",
    "         .format(degree, sqrt(mean_squared_error(y_test, linreg.predict(X_test)))))\n",
    "\n",
    "plot_learning_curves(linreg,  X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLYNOMIAL DEGREE-2 W/ RIDGE REGRESSION\n",
      "(poly deg 2) linear model coeff (w):\n",
      "[ 0.00000000e+00  7.83561256e-01  5.81248959e-01  8.36606865e-02\n",
      " -3.03533540e-01  2.17897801e-01 -2.64075722e-03  4.12917773e-03\n",
      " -2.05708348e-03 -1.36616736e-03 -5.60359635e-06 -3.96475273e-03\n",
      " -2.46871137e-03  2.19313880e-03  4.71163080e-03  5.30016917e-03\n",
      " -4.63433789e-03 -2.32194092e-03  2.37009827e-03 -8.06078647e-04\n",
      " -6.17730005e-04]\n",
      "(poly deg 2) linear model intercept (b): -6.659\n",
      "(poly deg 2) R-squared score (training): 0.514\n",
      "(poly deg 2) R-squared score (test): 0.477\n",
      "(poly deg 2) RMSE score (train): 20.939\n",
      "(poly deg 2) RMSE score (test): 21.262\n",
      "\n",
      "POLYNOMIAL DEGREE-3 W/ RIDGE REGRESSION\n",
      "(poly deg 3) linear model coeff (w):\n",
      "[ 0.00000000e+00  9.62069563e-01 -1.68358847e+00  2.31876270e+00\n",
      " -1.46747506e+00 -7.31761141e-02 -9.33646847e-03  1.86938416e-02\n",
      " -2.77955470e-02  1.70822169e-02  4.19307280e-03  1.27382279e-02\n",
      " -5.53520793e-03  1.19199550e-02  1.94080912e-02 -8.88527363e-03\n",
      " -8.00337144e-04 -5.36569520e-03  2.28899218e-03 -6.41651992e-03\n",
      " -2.28464225e-03  1.09450454e-05 -3.46712910e-05  1.01175577e-04\n",
      "  4.60659942e-07 -2.39465657e-05 -4.22856630e-05  1.91555001e-04\n",
      " -1.52904925e-04 -4.39060630e-05 -2.40488300e-04  1.54002169e-04\n",
      "  2.85199945e-04 -7.13331527e-05 -8.79852738e-05 -7.64079188e-05\n",
      " -4.68869056e-05 -3.44658935e-05  3.62910481e-05 -6.75496007e-05\n",
      "  4.60506413e-05 -2.82316725e-04  1.90478717e-04  1.03239802e-04\n",
      " -9.93243463e-05 -6.22361387e-05  2.38197342e-05  1.66805199e-04\n",
      "  9.79920579e-05  4.93306394e-06 -3.34246577e-04 -1.44394795e-04\n",
      " -2.27501270e-05  9.10029938e-05  2.27954872e-04  1.89586572e-05]\n",
      "(poly deg 3) linear model intercept (b): 19.808\n",
      "(poly deg 3) R-squared score (training): 0.609\n",
      "(poly deg 3) R-squared score (test): -1.121\n",
      "(poly deg 3) RMSE score (train): 18.796\n",
      "(poly deg 3) RMSE score (test): 42.812\n",
      "\n",
      "POLYNOMIAL DEGREE-5 W/ RIDGE REGRESSION\n",
      "(poly deg 5) linear model coeff (w):\n",
      "[ 0.00000000e+00 -1.58495107e+00  4.63260318e+00  1.93542423e+01\n",
      "  2.16510859e+01 -1.28797807e+01 -1.26766698e-01  3.34272404e-01\n",
      "  4.46082208e-01 -3.88849402e-01 -9.33766889e-03 -1.26661523e-02\n",
      " -5.59569257e-02 -5.01149549e-01 -1.45898769e-01 -5.71348395e-01\n",
      " -3.82618428e-01  1.50323714e-01 -1.91978281e-01  7.18044283e-02\n",
      "  3.47331751e-01  2.23053148e-03 -4.77254402e-03 -2.75645051e-03\n",
      "  3.88030610e-03  1.76120317e-03 -5.68326478e-03 -5.55282995e-03\n",
      "  8.19347465e-03  1.09361934e-03  6.37856867e-04 -2.04577777e-03\n",
      " -6.54540851e-03  2.35273093e-03  4.46351560e-04  6.85755435e-04\n",
      " -6.20115525e-04 -3.90495704e-03  8.40083433e-03  7.73358740e-03\n",
      "  6.96962069e-03  6.71611281e-03 -9.04837188e-03 -2.42162689e-03\n",
      " -1.10416923e-03  8.65500610e-04  4.99270705e-03 -5.93010294e-03\n",
      "  6.41324174e-03  9.96118410e-03  5.81269558e-03 -5.58879885e-03\n",
      " -4.56803614e-04  7.78418344e-04 -4.54697659e-03 -2.21921108e-03\n",
      " -1.83425546e-05  4.76361611e-05  5.94811692e-06 -1.44695104e-05\n",
      " -8.56347276e-06  8.17152290e-05 -7.63700872e-05 -6.43629099e-05\n",
      " -6.03279668e-05 -2.17470652e-05  1.21435918e-04  4.24633345e-05\n",
      " -3.71514284e-05 -4.96953548e-05  1.70232185e-05 -2.21703902e-05\n",
      "  1.53630038e-05  2.99508060e-05  3.44796353e-05  2.25325771e-05\n",
      "  1.01071221e-04  1.76336741e-04 -4.10086473e-05 -1.17279145e-04\n",
      " -2.67288526e-06  7.36000864e-05 -1.07796456e-04 -1.37829364e-04\n",
      " -1.23005224e-04  2.03630455e-04 -1.72050361e-05  4.95180164e-05\n",
      " -5.22878332e-05  7.04628952e-05 -2.83184218e-05  3.91649879e-06\n",
      "  6.26284907e-05 -3.75747850e-05 -1.79976328e-05 -6.61569128e-05\n",
      "  3.50972335e-05 -3.76140715e-05 -6.36393542e-05 -9.50257198e-05\n",
      " -4.46677519e-05 -4.09270460e-07 -5.52249356e-05  1.90452252e-05\n",
      " -9.80410865e-05 -4.88791804e-05  9.50698675e-05  3.70554186e-05\n",
      "  2.06900121e-04 -4.77679182e-05  4.95436466e-06 -3.18524505e-05\n",
      " -4.64473143e-05 -1.74787147e-05  1.99669581e-04  4.46280269e-05\n",
      " -2.86745177e-05 -7.84495416e-05 -2.60534769e-04  7.58946999e-05\n",
      "  2.27037148e-05 -7.54592065e-06  7.29394118e-05 -7.77301023e-05\n",
      "  6.19004414e-05 -1.17642906e-07  5.81759638e-08 -3.45788211e-08\n",
      " -1.28195982e-07 -1.94604355e-08  2.74526012e-08 -4.52544424e-07\n",
      "  5.62243511e-07 -1.95172203e-07  2.39421001e-07 -2.23136447e-08\n",
      " -3.07137815e-07  2.98851944e-07  4.10959765e-07 -1.55311145e-07\n",
      " -1.27850769e-07 -9.14183716e-08 -2.46630621e-07  9.14478066e-07\n",
      " -2.02103809e-07  3.28782920e-07 -4.52741634e-07 -3.43545210e-07\n",
      " -9.60150647e-08  2.18210986e-07  9.36463296e-08 -5.06661830e-09\n",
      " -2.84835705e-08  1.08188502e-07  7.64763861e-07 -1.81126532e-06\n",
      "  2.80586430e-07 -6.84448844e-07  9.96029359e-07  2.66560817e-07\n",
      " -1.26988030e-07  3.88933030e-07  7.42276870e-08 -7.56889588e-07\n",
      " -5.83721361e-07  5.95963205e-07 -5.88920648e-07 -7.94457391e-07\n",
      " -7.01726200e-07  2.27291296e-06  2.61384990e-07 -1.43372881e-06\n",
      "  1.09805822e-06  1.29805402e-06 -4.56715614e-08 -1.17840532e-06\n",
      " -1.04707301e-06  6.76824957e-07 -4.71112359e-07 -4.40704557e-07\n",
      "  2.43841311e-07  7.77559926e-09  1.53803831e-07  2.61423553e-07\n",
      "  1.34345665e-08 -1.90476727e-07 -8.08831455e-08 -4.03444678e-07\n",
      "  1.31487405e-06 -3.33828819e-08  2.93696133e-07  3.24525433e-07\n",
      " -7.70215720e-07  1.37032037e-07 -5.00602904e-07  1.31801494e-07\n",
      " -4.99951454e-08 -4.68236930e-08  2.39468588e-08 -8.25121771e-08\n",
      " -1.51624162e-07 -9.65455805e-07  7.00046688e-07  6.52464303e-07\n",
      "  8.19853272e-07 -1.67120107e-07  6.27211786e-07 -1.27512411e-08\n",
      " -1.38503765e-06  1.62941904e-06  6.08263508e-08  1.07801014e-06\n",
      " -7.13242620e-08 -1.86838851e-06  1.40078497e-07 -1.27902784e-07\n",
      " -2.60933565e-07  7.07509532e-07  6.32875698e-07 -1.66262659e-06\n",
      "  8.84614346e-07 -8.99966940e-07  1.95748117e-07  5.73389723e-07\n",
      " -7.62303901e-07  6.94478952e-08 -2.09919628e-07 -1.17118695e-07\n",
      "  6.96910407e-07  1.31381736e-07 -6.12759510e-08  1.40236723e-07\n",
      "  1.37928843e-08 -1.96888498e-07 -9.12156324e-08 -8.57999311e-08\n",
      "  1.94013267e-07 -1.83969009e-08 -5.92088426e-07  1.37541467e-07\n",
      "  1.69555450e-07  2.09466068e-07  5.88214164e-07  4.66294584e-07\n",
      " -4.82323239e-07 -1.19078241e-07 -5.92587980e-08 -6.96252725e-08\n",
      " -2.12402196e-07  1.35560142e-07 -7.51561088e-08  2.56451267e-08]\n",
      "(poly deg 5) linear model intercept (b): -322.214\n",
      "(poly deg 5) R-squared score (training): 0.792\n",
      "(poly deg 5) R-squared score (test): -2601.783\n",
      "(poly deg 5) RMSE score (train): 13.713\n",
      "(poly deg 5) RMSE score (test): 1499.796\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=3.04438e-25): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLYNOMIAL DEGREE-10 W/ RIDGE REGRESSION\n",
      "(poly deg 10) linear model coeff (w):\n",
      "[ 0.00000000e+00  7.68787830e-30  8.38795854e-30 ...  2.72464543e-16\n",
      " -3.14148365e-17 -1.09156796e-17]\n",
      "(poly deg 10) linear model intercept (b): 55.069\n",
      "(poly deg 10) R-squared score (training): 0.846\n",
      "(poly deg 10) R-squared score (test): -11380044339.484\n",
      "(poly deg 10) RMSE score (train): 11.782\n",
      "(poly deg 10) RMSE score (test): 3136066.725\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n"
     ]
    }
   ],
   "source": [
    "for degree in [2, 3, 5, 10]:\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_F1_poly = poly.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y, random_state=0)\n",
    "    linreg = Ridge().fit(X_train, y_train)\n",
    "\n",
    "    print('POLYNOMIAL DEGREE-{} W/ RIDGE REGRESSION'.format(degree))\n",
    "    print('(poly deg {}) linear model coeff (w):\\n{}'\n",
    "         .format(degree, linreg.coef_))\n",
    "    print('(poly deg {}) linear model intercept (b): {:.3f}'\n",
    "         .format(degree, linreg.intercept_))\n",
    "    print('(poly deg {}) R-squared score (training): {:.3f}'\n",
    "         .format(degree, linreg.score(X_train, y_train)))\n",
    "    print('(poly deg {}) R-squared score (test): {:.3f}'\n",
    "         .format(degree, linreg.score(X_test, y_test)))\n",
    "    print('(poly deg {}) RMSE score (train): {:.3f}'\n",
    "         .format(degree, sqrt(mean_squared_error(y_train, linreg.predict(X_train)))))\n",
    "    print('(poly deg {}) RMSE score (test): {:.3f}\\n'\n",
    "         .format(degree, sqrt(mean_squared_error(y_test, linreg.predict(X_test)))))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DAP3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
