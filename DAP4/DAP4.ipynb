{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y2E1jbS4ZluC",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (0.23.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-learn) (1.19.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-learn) (0.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from pandas) (1.19.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (3.3.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib) (7.2.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib) (1.19.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: six in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: seaborn in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from seaborn) (1.19.1)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from seaborn) (1.1.1)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from seaborn) (1.5.2)\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from seaborn) (3.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from pandas>=0.22.0->seaborn) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from pandas>=0.22.0->seaborn) (2020.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=2.1.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=2.1.2->seaborn) (1.2.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=2.1.2->seaborn) (2020.6.20)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=2.1.2->seaborn) (7.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=0.22.0->seaborn) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: graphviz in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (0.14.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-plot in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (0.3.7)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-plot) (0.23.2)\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-plot) (3.3.1)\n",
      "Requirement already satisfied: joblib>=0.10 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-plot) (0.16.0)\n",
      "Requirement already satisfied: scipy>=0.9 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-plot) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-learn>=0.18->scikit-plot) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from scikit-learn>=0.18->scikit-plot) (1.19.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=1.4.0->scikit-plot) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=1.4.0->scikit-plot) (2020.6.20)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=1.4.0->scikit-plot) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=1.4.0->scikit-plot) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from matplotlib>=1.4.0->scikit-plot) (7.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from python-dateutil>=2.1->matplotlib>=1.4.0->scikit-plot) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: statsmodels in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (0.11.1)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from statsmodels) (1.5.2)\n",
      "Requirement already satisfied: pandas>=0.21 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from statsmodels) (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.14 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from statsmodels) (1.19.1)\n",
      "Requirement already satisfied: patsy>=0.5 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from statsmodels) (0.5.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from pandas>=0.21->statsmodels) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from pandas>=0.21->statsmodels) (2.8.1)\n",
      "Requirement already satisfied: six in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (from patsy>=0.5->statsmodels) (1.15.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#If you need to install in modules in jupyter notebook \n",
    "%pip install scikit-learn\n",
    "%pip install pandas\n",
    "%pip install matplotlib \n",
    "%pip install seaborn \n",
    "%pip install graphviz\n",
    "%pip install scikit-plot   \n",
    "%pip install statsmodels   \n",
    "\n",
    "\n",
    "#Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "import datetime # Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import scikitplot as skplt\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.linear_model import LinearRegression\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"decision_trees\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IvsV2RSPapC6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#changes the output for the print statements\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vtW9LfInZ_x2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Pull \n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Rodrig79/Machine-Learning-Data-Analysis-Project/master/rawData/pokemon.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "#Data Cleaning\n",
    "df = df[~df.Name.str.contains('Mega')] #removed pokemon with \"Mega\" in it\n",
    "df = df.drop(columns = [\"Name\",\"Type 1\",\"Type 2\",\"#\",\"Generation\"]) #Removed columns with names\n",
    "\n",
    "#normalization\n",
    "# X = df.values #returns a numpy array\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# x_scaled = min_max_scaler.fit_transform(X)\n",
    "# df=pd.DataFrame(x_scaled, columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      49 \n",
       "1      62 \n",
       "2      82 \n",
       "4      52 \n",
       "5      64 \n",
       "       .. \n",
       "794    100\n",
       "795    100\n",
       "797    110\n",
       "798    160\n",
       "799    110\n",
       "Name: Attack, Length: 751, dtype: int64"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #Splitting dataset to x and y\n",
    "# y = df.Attack\n",
    "# X = df[['Defense','HP','Sp. Atk','Sp. Def','Speed']]\n",
    "#seperating data\n",
    "df = df.dropna() \n",
    "y = df['Attack']\n",
    "X = df[['Defense','HP','Speed']] \n",
    "X.dropna()\n",
    "y.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Defense</th>\n",
       "      <th>HP</th>\n",
       "      <th>Speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>39</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>121</td>\n",
       "      <td>108</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>751 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Defense   HP  Speed\n",
       "0    49       45   45   \n",
       "1    63       60   60   \n",
       "2    83       80   80   \n",
       "4    43       39   65   \n",
       "5    58       58   80   \n",
       "..   ..       ..   ..   \n",
       "794  121      108  95   \n",
       "795  150      50   50   \n",
       "797  60       80   70   \n",
       "798  60       80   80   \n",
       "799  120      80   70   \n",
       "\n",
       "[751 rows x 3 columns]"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiVariate  Regression with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.linear_model import LinearRegression\n",
    " from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wheat dataset\n",
      "linear model intercept: 5.305692824017925\n",
      "linear model coeff:\n",
      "[0.33084547 0.39032057 0.29685746]\n",
      "R-squared score (training): 0.370\n",
      "R-squared score (test): 0.374\n",
      "RMSE: 23.266\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('Wheat dataset')\n",
    "print('linear model intercept: {}'\n",
    "     .format(linreg.intercept_))\n",
    "print('linear model coeff:\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "# make predictions on the testing set\n",
    "y_pred = linreg.predict(X_test)\n",
    "print('RMSE: {:.3f}'.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlrd in /usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "# excel_file = 'Folds5x2_pp.xlsx'\n",
    "# data = pd.read_excel(excel_file)\n",
    "# print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X, Y, B):\n",
    "    m = len(Y)\n",
    "    J = np.sum((X.dot(B) - Y) ** 2)/(2 * m)\n",
    "    return J\n",
    "\n",
    "def batch_gradient_descent(X, Y, B, alpha, iterations):\n",
    "    cost_history = [0] * iterations\n",
    "    m = len(Y)\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        #print(iteration)\n",
    "        # Hypothesis Values\n",
    "        h = X.dot(B)\n",
    "        # Difference b/w Hypothesis and Actual Y\n",
    "        loss = h - Y\n",
    "        # Gradient Calculation\n",
    "        gradient = X.T.dot(loss) / m\n",
    "        # Changing Values of B using Gradient\n",
    "        B = B - alpha * gradient\n",
    "        # New Cost Value\n",
    "        cost = cost_function(X, Y, B)\n",
    "        cost_history[iteration] = cost \n",
    "    return B, cost_history \n",
    "def pred(x_test, newB):\n",
    "    return x_test.dot(newB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Coefficients\n",
    "B = np.zeros(X_train.shape[1])\n",
    "alpha = 0.005\n",
    "iter_ = 2000\n",
    "newB, cost_history = batch_gradient_descent(X_train, y_train, B, alpha, iter_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = pred(X_test,newB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.571688375255779"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def r2(y_,y):\n",
    "    sst = np.sum((y-y.mean())**2)\n",
    "    ssr = np.sum((y_-y)**2)\n",
    "    r2 = 1-(ssr/sst)\n",
    "    return(r2)\n",
    "#----------------\n",
    "r2(y_,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-23.39517167666285"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_ = pred(X_test[3],newB)\n",
    "ans_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3287.6149556223395,\n",
       " 3282.7351975065467,\n",
       " 3277.999917326219,\n",
       " 3273.404582548157,\n",
       " 3268.9448038670666,\n",
       " 3264.616330674631,\n",
       " 3260.4150466719334,\n",
       " 3256.3369656207087,\n",
       " 3252.378227229018,\n",
       " 3248.535093167105,\n",
       " 3244.803943209304,\n",
       " 3241.181271498023,\n",
       " 3237.6636829259246,\n",
       " 3234.2478896325824,\n",
       " 3230.930707611975,\n",
       " 3227.7090534273184,\n",
       " 3224.5799410298423,\n",
       " 3221.540478678216,\n",
       " 3218.587865955442,\n",
       " 3215.7193908801346,\n",
       " 3212.9324271091928,\n",
       " 3210.2244312289827,\n",
       " 3207.592940132222,\n",
       " 3205.0355684778565,\n",
       " 3202.550006231309,\n",
       " 3200.134016282548,\n",
       " 3197.78543213952,\n",
       " 3195.502155694557,\n",
       " 3193.282155061459,\n",
       " 3191.123462481001,\n",
       " 3189.0241722927117,\n",
       " 3186.982438970823,\n",
       " 3184.9964752223577,\n",
       " 3183.064550145391,\n",
       " 3181.184987445586,\n",
       " 3179.3561637091484,\n",
       " 3177.5765067304287,\n",
       " 3175.844493892434,\n",
       " 3174.15865059858,\n",
       " 3172.5175487540528,\n",
       " 3170.919805295226,\n",
       " 3169.3640807656006,\n",
       " 3167.849077936799,\n",
       " 3166.3735404731897,\n",
       " 3164.936251638758,\n",
       " 3163.5360330448916,\n",
       " 3162.1717434377833,\n",
       " 3160.8422775241916,\n",
       " 3159.546564834359,\n",
       " 3158.2835686209046,\n",
       " 3157.0522847925454,\n",
       " 3155.8517408815565,\n",
       " 3154.6809950439037,\n",
       " 3153.5391350909954,\n",
       " 3152.4252775520818,\n",
       " 3151.338566766306,\n",
       " 3150.278174003486,\n",
       " 3149.243296612708,\n",
       " 3148.2331571978593,\n",
       " 3147.247002819243,\n",
       " 3146.2841042204464,\n",
       " 3145.3437550796775,\n",
       " 3144.4252712847783,\n",
       " 3143.5279902311763,\n",
       " 3142.6512701420475,\n",
       " 3141.7944894099837,\n",
       " 3140.95704595949,\n",
       " 3140.1383566296468,\n",
       " 3139.3378565763023,\n",
       " 3138.5549986931787,\n",
       " 3137.7892530512895,\n",
       " 3137.0401063560917,\n",
       " 3136.3070614218086,\n",
       " 3135.589636662388,\n",
       " 3134.8873655985544,\n",
       " 3134.1997963804683,\n",
       " 3133.5264913254687,\n",
       " 3132.8670264704533,\n",
       " 3132.220991138415,\n",
       " 3131.587987518681,\n",
       " 3130.9676302604444,\n",
       " 3130.3595460791435,\n",
       " 3129.7633733752973,\n",
       " 3129.1787618653943,\n",
       " 3128.605372224463,\n",
       " 3128.042875739941,\n",
       " 3127.4909539765017,\n",
       " 3126.949298451475,\n",
       " 3126.4176103205386,\n",
       " 3125.8956000733524,\n",
       " 3125.3829872388173,\n",
       " 3124.8795000996606,\n",
       " 3124.3848754160426,\n",
       " 3123.898858157916,\n",
       " 3123.421201245839,\n",
       " 3122.951665299992,\n",
       " 3122.4900183971386,\n",
       " 3122.0360358352605,\n",
       " 3121.5894999056477,\n",
       " 3121.1501996721904,\n",
       " 3120.7179307576575,\n",
       " 3120.29249513673,\n",
       " 3119.8737009355755,\n",
       " 3119.4613622377783,\n",
       " 3119.055298896386,\n",
       " 3118.655336351918,\n",
       " 3118.261305456115,\n",
       " 3117.873042301277,\n",
       " 3117.4903880549837,\n",
       " 3117.1131888000564,\n",
       " 3116.741295379565,\n",
       " 3116.374563246753,\n",
       " 3116.012852319696,\n",
       " 3115.656026840559,\n",
       " 3115.3039552393097,\n",
       " 3114.956510001735,\n",
       " 3114.613567541633,\n",
       " 3114.2750080770484,\n",
       " 3113.9407155104163,\n",
       " 3113.6105773125023,\n",
       " 3113.2844844100077,\n",
       " 3112.9623310767274,\n",
       " 3112.644014828156,\n",
       " 3112.329436319416,\n",
       " 3112.0184992464215,\n",
       " 3111.711110250164,\n",
       " 3111.407178824022,\n",
       " 3111.106617224004,\n",
       " 3110.809340381826,\n",
       " 3110.5152658207417,\n",
       " 3110.2243135740255,\n",
       " 3109.936406106043,\n",
       " 3109.651468235805,\n",
       " 3109.3694270629467,\n",
       " 3109.090211896041,\n",
       " 3108.8137541831848,\n",
       " 3108.5399874447676,\n",
       " 3108.2688472083764,\n",
       " 3108.000270945752,\n",
       " 3107.734198011739,\n",
       " 3107.4705695851653,\n",
       " 3107.2093286115896,\n",
       " 3106.9504197478577,\n",
       " 3106.6937893084128,\n",
       " 3106.439385213299,\n",
       " 3106.1871569378113,\n",
       " 3105.9370554637335,\n",
       " 3105.68903323212,\n",
       " 3105.443044097563,\n",
       " 3105.1990432839148,\n",
       " 3104.956987341398,\n",
       " 3104.7168341050783,\n",
       " 3104.478542654645,\n",
       " 3104.242073275464,\n",
       " 3104.0073874208592,\n",
       " 3103.7744476755875,\n",
       " 3103.543217720464,\n",
       " 3103.3136622981096,\n",
       " 3103.085747179772,\n",
       " 3102.8594391332044,\n",
       " 3102.6347058915517,\n",
       " 3102.4115161232176,\n",
       " 3102.1898394026925,\n",
       " 3101.9696461822914,\n",
       " 3101.750907764789,\n",
       " 3101.5335962769213,\n",
       " 3101.3176846437173,\n",
       " 3101.103146563647,\n",
       " 3100.889956484548,\n",
       " 3100.6780895803167,\n",
       " 3100.4675217283334,\n",
       " 3100.258229487597,\n",
       " 3100.0501900775544,\n",
       " 3099.8433813575944,\n",
       " 3099.6377818071874,\n",
       " 3099.4333705066574,\n",
       " 3099.230127118553,\n",
       " 3099.0280318696127,\n",
       " 3098.8270655332917,\n",
       " 3098.627209412848,\n",
       " 3098.4284453249543,\n",
       " 3098.230755583831,\n",
       " 3098.0341229858786,\n",
       " 3097.8385307947933,\n",
       " 3097.643962727151,\n",
       " 3097.4504029384507,\n",
       " 3097.25783600959,\n",
       " 3097.0662469337726,\n",
       " 3096.8756211038262,\n",
       " 3096.6859442999207,\n",
       " 3096.49720267767,\n",
       " 3096.3093827566126,\n",
       " 3096.122471409055,\n",
       " 3095.9364558492603,\n",
       " 3095.7513236229856,\n",
       " 3095.567062597341,\n",
       " 3095.3836609509763,\n",
       " 3095.2011071645647,\n",
       " 3095.0193900115983,\n",
       " 3094.8384985494636,\n",
       " 3094.6584221108046,\n",
       " 3094.4791502951516,\n",
       " 3094.300672960819,\n",
       " 3094.122980217051,\n",
       " 3093.9460624164212,\n",
       " 3093.769910147466,\n",
       " 3093.594514227552,\n",
       " 3093.419865695965,\n",
       " 3093.245955807221,\n",
       " 3093.0727760245795,\n",
       " 3092.900318013769,\n",
       " 3092.7285736369013,\n",
       " 3092.5575349465826,\n",
       " 3092.3871941802095,\n",
       " 3092.217543754436,\n",
       " 3092.0485762598278,\n",
       " 3091.8802844556667,\n",
       " 3091.7126612649367,\n",
       " 3091.54569976945,\n",
       " 3091.3793932051394,\n",
       " 3091.2137349574873,\n",
       " 3091.0487185571064,\n",
       " 3090.8843376754526,\n",
       " 3090.7205861206753,\n",
       " 3090.5574578335973,\n",
       " 3090.394946883818,\n",
       " 3090.233047465941,\n",
       " 3090.0717538959193,\n",
       " 3089.911060607513,\n",
       " 3089.7509621488593,\n",
       " 3089.591453179148,\n",
       " 3089.432528465404,\n",
       " 3089.2741828793646,\n",
       " 3089.116411394459,\n",
       " 3088.9592090828814,\n",
       " 3088.802571112752,\n",
       " 3088.6464927453703,\n",
       " 3088.4909693325517,\n",
       " 3088.3359963140465,\n",
       " 3088.1815692150426,\n",
       " 3088.02768364374,\n",
       " 3087.874335289008,\n",
       " 3087.7215199181073,\n",
       " 3087.5692333744896,\n",
       " 3087.4174715756612,\n",
       " 3087.2662305111144,\n",
       " 3087.115506240323,\n",
       " 3086.9652948908006,\n",
       " 3086.815592656215,\n",
       " 3086.666395794568,\n",
       " 3086.517700626426,\n",
       " 3086.3695035332053,\n",
       " 3086.2218009555145,\n",
       " 3086.0745893915428,\n",
       " 3085.927865395501,\n",
       " 3085.781625576114,\n",
       " 3085.63586659515,\n",
       " 3085.4905851660046,\n",
       " 3085.3457780523245,\n",
       " 3085.201442066671,\n",
       " 3085.0575740692307,\n",
       " 3084.9141709665596,\n",
       " 3084.77122971037,\n",
       " 3084.628747296351,\n",
       " 3084.486720763029,\n",
       " 3084.34514719066,\n",
       " 3084.204023700157,\n",
       " 3084.063347452049,\n",
       " 3083.9231156454734,\n",
       " 3083.783325517199,\n",
       " 3083.6439743406763,\n",
       " 3083.5050594251193,\n",
       " 3083.366578114614,\n",
       " 3083.228527787255,\n",
       " 3083.0909058543066,\n",
       " 3082.9537097593884,\n",
       " 3082.816936977692,\n",
       " 3082.6805850152114,\n",
       " 3082.544651408005,\n",
       " 3082.409133721474,\n",
       " 3082.2740295496683,\n",
       " 3082.139336514607,\n",
       " 3082.0050522656243,\n",
       " 3081.8711744787324,\n",
       " 3081.737700856002,\n",
       " 3081.6046291249663,\n",
       " 3081.4719570380394,\n",
       " 3081.3396823719504,\n",
       " 3081.207802927196,\n",
       " 3081.0763165275134,\n",
       " 3080.945221019362,\n",
       " 3080.814514271423,\n",
       " 3080.6841941741154,\n",
       " 3080.554258639127,\n",
       " 3080.4247055989517,\n",
       " 3080.2955330064524,\n",
       " 3080.166738834426,\n",
       " 3080.0383210751856,\n",
       " 3079.910277740158,\n",
       " 3079.7826068594854,\n",
       " 3079.6553064816444,\n",
       " 3079.528374673076,\n",
       " 3079.4018095178226,\n",
       " 3079.2756091171773,\n",
       " 3079.1497715893465,\n",
       " 3079.0242950691168,\n",
       " 3078.8991777075344,\n",
       " 3078.7744176715933,\n",
       " 3078.650013143932,\n",
       " 3078.5259623225415,\n",
       " 3078.4022634204753,\n",
       " 3078.2789146655728,\n",
       " 3078.155914300191,\n",
       " 3078.03326058094,\n",
       " 3077.910951778428,\n",
       " 3077.788986177014,\n",
       " 3077.6673620745664,\n",
       " 3077.546077782227,\n",
       " 3077.4251316241844,\n",
       " 3077.304521937453,\n",
       " 3077.184247071656,\n",
       " 3077.064305388815,\n",
       " 3076.9446952631483,\n",
       " 3076.8254150808707,\n",
       " 3076.70646324,\n",
       " 3076.5878381501716,\n",
       " 3076.4695382324526,\n",
       " 3076.3515619191644,\n",
       " 3076.233907653713,\n",
       " 3076.1165738904137,\n",
       " 3075.9995590943345,\n",
       " 3075.8828617411295,\n",
       " 3075.7664803168896,\n",
       " 3075.6504133179847,\n",
       " 3075.5346592509186,\n",
       " 3075.4192166321877,\n",
       " 3075.3040839881373,\n",
       " 3075.189259854826,\n",
       " 3075.0747427778924,\n",
       " 3074.9605313124266,\n",
       " 3074.8466240228427,\n",
       " 3074.7330194827546,\n",
       " 3074.619716274858,\n",
       " 3074.506712990811,\n",
       " 3074.394008231122,\n",
       " 3074.281600605034,\n",
       " 3074.169488730422,\n",
       " 3074.05767123368,\n",
       " 3073.9461467496235,\n",
       " 3073.834913921383,\n",
       " 3073.7239714003076,\n",
       " 3073.6133178458713,\n",
       " 3073.5029519255727,\n",
       " 3073.392872314848,\n",
       " 3073.2830776969795,\n",
       " 3073.173566763007,\n",
       " 3073.064338211643,\n",
       " 3072.9553907491895,\n",
       " 3072.8467230894535,\n",
       " 3072.738333953669,\n",
       " 3072.630222070419,\n",
       " 3072.522386175557,\n",
       " 3072.414825012134,\n",
       " 3072.3075373303213,\n",
       " 3072.200521887346,\n",
       " 3072.0937774474114,\n",
       " 3071.987302781637,\n",
       " 3071.881096667986,\n",
       " 3071.7751578912003,\n",
       " 3071.6694852427368,\n",
       " 3071.5640775207034,\n",
       " 3071.4589335297974,\n",
       " 3071.3540520812453,\n",
       " 3071.2494319927428,\n",
       " 3071.145072088395,\n",
       " 3071.040971198663,\n",
       " 3070.9371281603035,\n",
       " 3070.8335418163165,\n",
       " 3070.7302110158903,\n",
       " 3070.6271346143503,\n",
       " 3070.524311473103,\n",
       " 3070.42174045959,\n",
       " 3070.3194204472347,\n",
       " 3070.217350315394,\n",
       " 3070.11552894931,\n",
       " 3070.0139552400624,\n",
       " 3069.9126280845226,\n",
       " 3069.811546385308,\n",
       " 3069.7107090507347,\n",
       " 3069.6101149947767,\n",
       " 3069.509763137021,\n",
       " 3069.409652402623,\n",
       " 3069.3097817222674,\n",
       " 3069.2101500321246,\n",
       " 3069.1107562738125,\n",
       " 3069.0115993943523,\n",
       " 3068.9126783461343,\n",
       " 3068.813992086875,\n",
       " 3068.7155395795803,\n",
       " 3068.617319792509,\n",
       " 3068.519331699134,\n",
       " 3068.4215742781057,\n",
       " 3068.3240465132185,\n",
       " 3068.2267473933725,\n",
       " 3068.12967591254,\n",
       " 3068.0328310697305,\n",
       " 3067.9362118689587,\n",
       " 3067.8398173192063,\n",
       " 3067.743646434396,\n",
       " 3067.647698233351,\n",
       " 3067.5519717397706,\n",
       " 3067.4564659821917,\n",
       " 3067.3611799939636,\n",
       " 3067.2661128132104,\n",
       " 3067.1712634828077,\n",
       " 3067.0766310503477,\n",
       " 3066.9822145681105,\n",
       " 3066.888013093035,\n",
       " 3066.794025686691,\n",
       " 3066.700251415248,\n",
       " 3066.6066893494494,\n",
       " 3066.5133385645845,\n",
       " 3066.4201981404576,\n",
       " 3066.327267161364,\n",
       " 3066.2345447160624,\n",
       " 3066.1420298977478,\n",
       " 3066.049721804024,\n",
       " 3065.957619536879,\n",
       " 3065.865722202659,\n",
       " 3065.774028912043,\n",
       " 3065.6825387800145,\n",
       " 3065.591250925842,\n",
       " 3065.5001644730487,\n",
       " 3065.409278549393,\n",
       " 3065.3185922868397,\n",
       " 3065.228104821538,\n",
       " 3065.1378152937996,\n",
       " 3065.0477228480704,\n",
       " 3064.9578266329127,\n",
       " 3064.8681258009783,\n",
       " 3064.7786195089866,\n",
       " 3064.6893069177013,\n",
       " 3064.6001871919107,\n",
       " 3064.511259500402,\n",
       " 3064.422523015942,\n",
       " 3064.3339769152512,\n",
       " 3064.245620378989,\n",
       " 3064.157452591724,\n",
       " 3064.06947274192,\n",
       " 3063.98168002191,\n",
       " 3063.8940736278782,\n",
       " 3063.8066527598376,\n",
       " 3063.719416621609,\n",
       " 3063.6323644208037,\n",
       " 3063.545495368799,\n",
       " 3063.458808680722,\n",
       " 3063.3723035754265,\n",
       " 3063.2859792754757,\n",
       " 3063.1998350071212,\n",
       " 3063.113870000284,\n",
       " 3063.0280834885357,\n",
       " 3062.9424747090784,\n",
       " 3062.857042902727,\n",
       " 3062.7717873138868,\n",
       " 3062.6867071905417,\n",
       " 3062.6018017842293,\n",
       " 3062.517070350024,\n",
       " 3062.4325121465204,\n",
       " 3062.348126435814,\n",
       " 3062.2639124834823,\n",
       " 3062.1798695585703,\n",
       " 3062.0959969335677,\n",
       " 3062.0122938843947,\n",
       " 3061.9287596903837,\n",
       " 3061.845393634262,\n",
       " 3061.762195002135,\n",
       " 3061.6791630834655,\n",
       " 3061.596297171063,\n",
       " 3061.5135965610602,\n",
       " 3061.431060552903,\n",
       " 3061.348688449326,\n",
       " 3061.266479556342,\n",
       " 3061.1844331832235,\n",
       " 3061.1025486424846,\n",
       " 3061.0208252498683,\n",
       " 3060.939262324328,\n",
       " 3060.85785918801,\n",
       " 3060.7766151662418,\n",
       " 3060.6955295875105,\n",
       " 3060.6146017834544,\n",
       " 3060.53383108884,\n",
       " 3060.4532168415512,\n",
       " 3060.3727583825707,\n",
       " 3060.292455055968,\n",
       " 3060.212306208879,\n",
       " 3060.132311191499,\n",
       " 3060.0524693570583,\n",
       " 3059.972780061813,\n",
       " 3059.89324266503,\n",
       " 3059.8138565289664,\n",
       " 3059.7346210188643,\n",
       " 3059.655535502928,\n",
       " 3059.5765993523123,\n",
       " 3059.4978119411094,\n",
       " 3059.4191726463314,\n",
       " 3059.340680847898,\n",
       " 3059.2623359286235,\n",
       " 3059.184137274199,\n",
       " 3059.1060842731827,\n",
       " 3059.02817631698,\n",
       " 3058.950412799837,\n",
       " 3058.87279311882,\n",
       " 3058.795316673806,\n",
       " 3058.717982867467,\n",
       " 3058.6407911052574,\n",
       " 3058.5637407953986,\n",
       " 3058.4868313488696,\n",
       " 3058.4100621793873,\n",
       " 3058.333432703399,\n",
       " 3058.256942340068,\n",
       " 3058.180590511256,\n",
       " 3058.104376641517,\n",
       " 3058.0283001580783,\n",
       " 3057.952360490831,\n",
       " 3057.8765570723144,\n",
       " 3057.800889337708,\n",
       " 3057.7253567248126,\n",
       " 3057.6499586740406,\n",
       " 3057.574694628406,\n",
       " 3057.499564033508,\n",
       " 3057.4245663375164,\n",
       " 3057.349700991167,\n",
       " 3057.2749674477427,\n",
       " 3057.2003651630644,\n",
       " 3057.125893595476,\n",
       " 3057.051552205836,\n",
       " 3056.977340457501,\n",
       " 3056.903257816318,\n",
       " 3056.8293037506087,\n",
       " 3056.7554777311607,\n",
       " 3056.6817792312145,\n",
       " 3056.60820772645,\n",
       " 3056.5347626949774,\n",
       " 3056.4614436173247,\n",
       " 3056.388249976425,\n",
       " 3056.315181257606,\n",
       " 3056.2422369485794,\n",
       " 3056.1694165394265,\n",
       " 3056.096719522591,\n",
       " 3056.0241453928634,\n",
       " 3055.951693647374,\n",
       " 3055.8793637855765,\n",
       " 3055.807155309243,\n",
       " 3055.7350677224495,\n",
       " 3055.663100531563,\n",
       " 3055.5912532452344,\n",
       " 3055.519525374386,\n",
       " 3055.4479164322,\n",
       " 3055.376425934109,\n",
       " 3055.305053397785,\n",
       " 3055.2337983431257,\n",
       " 3055.16266029225,\n",
       " 3055.0916387694824,\n",
       " 3055.0207333013436,\n",
       " 3054.9499434165405,\n",
       " 3054.8792686459556,\n",
       " 3054.8087085226384,\n",
       " 3054.738262581791,\n",
       " 3054.6679303607616,\n",
       " 3054.597711399033,\n",
       " 3054.527605238212,\n",
       " 3054.457611422019,\n",
       " 3054.387729496281,\n",
       " 3054.317959008916,\n",
       " 3054.2482995099303,\n",
       " 3054.1787505513994,\n",
       " 3054.109311687467,\n",
       " 3054.039982474332,\n",
       " 3053.970762470235,\n",
       " 3053.9016512354538,\n",
       " 3053.832648332292,\n",
       " 3053.763753325068,\n",
       " 3053.694965780108,\n",
       " 3053.6262852657333,\n",
       " 3053.557711352254,\n",
       " 3053.489243611957,\n",
       " 3053.420881619099,\n",
       " 3053.3526249498946,\n",
       " 3053.2844731825085,\n",
       " 3053.216425897048,\n",
       " 3053.1484826755486,\n",
       " 3053.08064310197,\n",
       " 3053.0129067621865,\n",
       " 3052.9452732439727,\n",
       " 3052.8777421370028,\n",
       " 3052.810313032833,\n",
       " 3052.7429855249,\n",
       " 3052.675759208507,\n",
       " 3052.608633680816,\n",
       " 3052.5416085408424,\n",
       " 3052.4746833894405,\n",
       " 3052.4078578293006,\n",
       " 3052.3411314649347,\n",
       " 3052.2745039026727,\n",
       " 3052.2079747506514,\n",
       " 3052.1415436188054,\n",
       " 3052.0752101188614,\n",
       " 3052.0089738643255,\n",
       " 3051.9428344704797,\n",
       " 3051.8767915543685,\n",
       " 3051.810844734795,\n",
       " 3051.74499363231,\n",
       " 3051.679237869204,\n",
       " 3051.6135770694996,\n",
       " 3051.5480108589427,\n",
       " 3051.4825388649956,\n",
       " 3051.417160716828,\n",
       " 3051.3518760453085,\n",
       " 3051.286684482997,\n",
       " 3051.2215856641365,\n",
       " 3051.156579224646,\n",
       " 3051.091664802112,\n",
       " 3051.02684203578,\n",
       " 3050.9621105665487,\n",
       " 3050.89747003696,\n",
       " 3050.832920091192,\n",
       " 3050.768460375051,\n",
       " 3050.704090535964,\n",
       " 3050.6398102229737,\n",
       " 3050.575619086725,\n",
       " 3050.511516779463,\n",
       " 3050.4475029550235,\n",
       " 3050.383577268824,\n",
       " 3050.319739377858,\n",
       " 3050.2559889406875,\n",
       " 3050.1923256174355,\n",
       " 3050.128749069778,\n",
       " 3050.0652589609376,\n",
       " 3050.0018549556753,\n",
       " 3049.9385367202835,\n",
       " 3049.8753039225785,\n",
       " 3049.812156231895,\n",
       " 3049.7490933190784,\n",
       " 3049.6861148564744,\n",
       " 3049.623220517927,\n",
       " 3049.5604099787683,\n",
       " 3049.497682915811,\n",
       " 3049.4350390073446,\n",
       " 3049.372477933125,\n",
       " 3049.309999374371,\n",
       " 3049.2476030137527,\n",
       " 3049.1852885353906,\n",
       " 3049.1230556248433,\n",
       " 3049.0609039691044,\n",
       " 3048.9988332565945,\n",
       " 3048.936843177154,\n",
       " 3048.8749334220365,\n",
       " 3048.813103683904,\n",
       " 3048.7513536568176,\n",
       " 3048.689683036233,\n",
       " 3048.6280915189927,\n",
       " 3048.5665788033193,\n",
       " 3048.505144588811,\n",
       " 3048.4437885764337,\n",
       " 3048.3825104685125,\n",
       " 3048.3213099687305,\n",
       " 3048.2601867821168,\n",
       " 3048.199140615044,\n",
       " 3048.138171175222,\n",
       " 3048.077278171687,\n",
       " 3048.0164613148027,\n",
       " 3047.9557203162467,\n",
       " 3047.895054889009,\n",
       " 3047.834464747385,\n",
       " 3047.7739496069685,\n",
       " 3047.7135091846458,\n",
       " 3047.65314319859,\n",
       " 3047.5928513682557,\n",
       " 3047.532633414369,\n",
       " 3047.4724890589287,\n",
       " 3047.4124180251943,\n",
       " 3047.352420037682,\n",
       " 3047.292494822159,\n",
       " 3047.2326421056373,\n",
       " 3047.1728616163696,\n",
       " 3047.11315308384,\n",
       " 3047.0535162387614,\n",
       " 3046.9939508130674,\n",
       " 3046.9344565399097,\n",
       " 3046.875033153648,\n",
       " 3046.815680389849,\n",
       " 3046.7563979852766,\n",
       " 3046.6971856778896,\n",
       " 3046.6380432068354,\n",
       " 3046.57897031244,\n",
       " 3046.519966736211,\n",
       " 3046.461032220824,\n",
       " 3046.402166510122,\n",
       " 3046.3433693491083,\n",
       " 3046.2846404839397,\n",
       " 3046.225979661925,\n",
       " 3046.167386631515,\n",
       " 3046.1088611423006,\n",
       " 3046.0504029450053,\n",
       " 3045.9920117914808,\n",
       " 3045.9336874347027,\n",
       " 3045.8754296287643,\n",
       " 3045.8172381288696,\n",
       " 3045.759112691332,\n",
       " 3045.7010530735647,\n",
       " 3045.64305903408,\n",
       " 3045.585130332481,\n",
       " 3045.5272667294566,\n",
       " 3045.469467986779,\n",
       " 3045.4117338672963,\n",
       " 3045.3540641349273,\n",
       " 3045.2964585546583,\n",
       " 3045.238916892537,\n",
       " 3045.181438915667,\n",
       " 3045.124024392205,\n",
       " 3045.0666730913517,\n",
       " 3045.009384783353,\n",
       " 3044.9521592394904,\n",
       " 3044.894996232076,\n",
       " 3044.837895534452,\n",
       " 3044.78085692098,\n",
       " 3044.7238801670424,\n",
       " 3044.666965049032,\n",
       " 3044.6101113443533,\n",
       " 3044.5533188314103,\n",
       " 3044.496587289608,\n",
       " 3044.4399164993456,\n",
       " 3044.3833062420126,\n",
       " 3044.3267562999813,\n",
       " 3044.2702664566064,\n",
       " 3044.213836496218,\n",
       " 3044.1574662041166,\n",
       " 3044.1011553665708,\n",
       " 3044.0449037708086,\n",
       " 3043.988711205021,\n",
       " 3043.9325774583463,\n",
       " 3043.8765023208757,\n",
       " 3043.820485583644,\n",
       " 3043.764527038624,\n",
       " 3043.708626478727,\n",
       " 3043.6527836977943,\n",
       " 3043.596998490594,\n",
       " 3043.5412706528164,\n",
       " 3043.4855999810716,\n",
       " 3043.429986272882,\n",
       " 3043.3744293266814,\n",
       " 3043.3189289418083,\n",
       " 3043.2634849185024,\n",
       " 3043.208097057901,\n",
       " 3043.152765162034,\n",
       " 3043.0974890338207,\n",
       " 3043.042268477064,\n",
       " 3042.9871032964497,\n",
       " 3042.9319932975363,\n",
       " 3042.8769382867586,\n",
       " 3042.821938071417,\n",
       " 3042.766992459678,\n",
       " 3042.7121012605667,\n",
       " 3042.657264283966,\n",
       " 3042.6024813406107,\n",
       " 3042.5477522420824,\n",
       " 3042.4930768008094,\n",
       " 3042.4384548300595,\n",
       " 3042.3838861439367,\n",
       " 3042.3293705573774,\n",
       " 3042.2749078861484,\n",
       " 3042.2204979468397,\n",
       " 3042.166140556864,\n",
       " 3042.1118355344493,\n",
       " 3042.057582698639,\n",
       " 3042.003381869285,\n",
       " 3041.9492328670453,\n",
       " 3041.8951355133822,\n",
       " 3041.841089630553,\n",
       " 3041.787095041612,\n",
       " 3041.733151570404,\n",
       " 3041.6792590415625,\n",
       " 3041.625417280502,\n",
       " 3041.57162611342,\n",
       " 3041.517885367288,\n",
       " 3041.464194869853,\n",
       " 3041.4105544496288,\n",
       " 3041.356963935898,\n",
       " 3041.303423158701,\n",
       " 3041.249931948842,\n",
       " 3041.1964901378765,\n",
       " 3041.1430975581125,\n",
       " 3041.089754042607,\n",
       " 3041.0364594251623,\n",
       " 3040.9832135403194,\n",
       " 3040.930016223359,\n",
       " 3040.876867310297,\n",
       " 3040.8237666378773,\n",
       " 3040.7707140435737,\n",
       " 3040.7177093655846,\n",
       " 3040.6647524428267,\n",
       " 3040.6118431149375,\n",
       " 3040.5589812222665,\n",
       " 3040.5061666058737,\n",
       " 3040.453399107529,\n",
       " 3040.4006785697043,\n",
       " 3040.3480048355736,\n",
       " 3040.2953777490093,\n",
       " 3040.242797154577,\n",
       " 3040.190262897535,\n",
       " 3040.1377748238283,\n",
       " 3040.0853327800874,\n",
       " 3040.032936613626,\n",
       " 3039.980586172435,\n",
       " 3039.928281305181,\n",
       " 3039.876021861203,\n",
       " 3039.8238076905095,\n",
       " 3039.7716386437746,\n",
       " 3039.719514572337,\n",
       " 3039.6674353281933,\n",
       " 3039.6154007639984,\n",
       " 3039.563410733062,\n",
       " 3039.511465089343,\n",
       " 3039.4595636874474,\n",
       " 3039.4077063826294,\n",
       " 3039.3558930307827,\n",
       " 3039.30412348844,\n",
       " 3039.2523976127713,\n",
       " 3039.200715261578,\n",
       " 3039.149076293293,\n",
       " 3039.097480566976,\n",
       " 3039.045927942311,\n",
       " 3038.994418279603,\n",
       " 3038.9429514397766,\n",
       " 3038.891527284371,\n",
       " 3038.840145675539,\n",
       " 3038.788806476043,\n",
       " 3038.737509549253,\n",
       " 3038.686254759143,\n",
       " 3038.63504197029,\n",
       " 3038.583871047868,\n",
       " 3038.5327418576476,\n",
       " 3038.4816542659933,\n",
       " 3038.4306081398595,\n",
       " 3038.3796033467897,\n",
       " 3038.328639754911,\n",
       " 3038.2777172329347,\n",
       " 3038.2268356501495,\n",
       " 3038.175994876424,\n",
       " 3038.125194782199,\n",
       " 3038.0744352384872,\n",
       " 3038.0237161168734,\n",
       " 3037.973037289504,\n",
       " 3037.922398629095,\n",
       " 3037.871800008918,\n",
       " 3037.8212413028086,\n",
       " 3037.770722385154,\n",
       " 3037.720243130898,\n",
       " 3037.6698034155343,\n",
       " 3037.6194031151053,\n",
       " 3037.5690421061995,\n",
       " 3037.518720265948,\n",
       " 3037.468437472025,\n",
       " 3037.4181936026407,\n",
       " 3037.367988536543,\n",
       " 3037.3178221530125,\n",
       " 3037.26769433186,\n",
       " 3037.217604953428,\n",
       " 3037.1675538985814,\n",
       " 3037.117541048711,\n",
       " 3037.0675662857293,\n",
       " 3037.017629492066,\n",
       " 3036.967730550669,\n",
       " 3036.917869345,\n",
       " 3036.8680457590317,\n",
       " 3036.8182596772467,\n",
       " 3036.768510984635,\n",
       " 3036.7187995666923,\n",
       " 3036.669125309414,\n",
       " 3036.6194880992994,\n",
       " 3036.569887823341,\n",
       " 3036.520324369032,\n",
       " 3036.4707976243562,\n",
       " 3036.4213074777867,\n",
       " 3036.371853818289,\n",
       " 3036.3224365353126,\n",
       " 3036.2730555187923,\n",
       " 3036.223710659145,\n",
       " 3036.1744018472677,\n",
       " 3036.1251289745333,\n",
       " 3036.075891932792,\n",
       " 3036.0266906143665,\n",
       " 3035.977524912051,\n",
       " 3035.9283947191084,\n",
       " 3035.8792999292677,\n",
       " 3035.830240436724,\n",
       " 3035.7812161361335,\n",
       " 3035.7322269226142,\n",
       " 3035.68327269174,\n",
       " 3035.634353339544,\n",
       " 3035.5854687625106,\n",
       " 3035.5366188575777,\n",
       " 3035.4878035221327,\n",
       " 3035.4390226540118,\n",
       " 3035.390276151494,\n",
       " 3035.341563913307,\n",
       " 3035.292885838616,\n",
       " 3035.244241827026,\n",
       " 3035.195631778583,\n",
       " 3035.147055593766,\n",
       " 3035.0985131734874,\n",
       " 3035.050004419092,\n",
       " 3035.0015292323556,\n",
       " 3034.95308751548,\n",
       " 3034.904679171093,\n",
       " 3034.856304102246,\n",
       " 3034.8079622124146,\n",
       " 3034.75965340549,\n",
       " 3034.711377585786,\n",
       " 3034.6631346580293,\n",
       " 3034.6149245273623,\n",
       " 3034.566747099339,\n",
       " 3034.5186022799244,\n",
       " 3034.470489975492,\n",
       " 3034.422410092821,\n",
       " 3034.3743625390957,\n",
       " 3034.3263472219055,\n",
       " 3034.278364049237,\n",
       " 3034.2304129294803,\n",
       " 3034.1824937714196,\n",
       " 3034.134606484236,\n",
       " 3034.086750977505,\n",
       " 3034.0389271611925,\n",
       " 3033.9911349456565,\n",
       " 3033.943374241642,\n",
       " 3033.895644960281,\n",
       " 3033.8479470130915,\n",
       " 3033.800280311973,\n",
       " 3033.752644769206,\n",
       " 3033.7050402974546,\n",
       " 3033.657466809755,\n",
       " 3033.6099242195237,\n",
       " 3033.562412440551,\n",
       " 3033.514931386998,\n",
       " 3033.467480973399,\n",
       " 3033.420061114656,\n",
       " 3033.372671726041,\n",
       " 3033.3253127231887,\n",
       " 3033.2779840221015,\n",
       " 3033.2306855391416,\n",
       " 3033.1834171910336,\n",
       " 3033.1361788948616,\n",
       " 3033.0889705680665,\n",
       " 3033.041792128446,\n",
       " 3032.994643494152,\n",
       " 3032.9475245836893,\n",
       " 3032.9004353159135,\n",
       " 3032.8533756100305,\n",
       " 3032.8063453855934,\n",
       " 3032.7593445625025,\n",
       " 3032.7123730610024,\n",
       " 3032.665430801681,\n",
       " 3032.6185177054685,\n",
       " 3032.571633693635,\n",
       " 3032.524778687788,\n",
       " 3032.4779526098737,\n",
       " 3032.4311553821735,\n",
       " 3032.384386927302,\n",
       " 3032.3376471682063,\n",
       " 3032.290936028166,\n",
       " 3032.244253430788,\n",
       " 3032.1975993000087,\n",
       " 3032.15097356009,\n",
       " 3032.104376135619,\n",
       " 3032.0578069515072,\n",
       " 3032.0112659329866,\n",
       " 3031.9647530056113,\n",
       " 3031.9182680952526,\n",
       " 3031.8718111281005,\n",
       " 3031.8253820306627,\n",
       " 3031.7789807297577,\n",
       " 3031.7326071525213,\n",
       " 3031.686261226399,\n",
       " 3031.6399428791474,\n",
       " 3031.5936520388323,\n",
       " 3031.5473886338264,\n",
       " 3031.5011525928094,\n",
       " 3031.4549438447657,\n",
       " 3031.408762318984,\n",
       " 3031.3626079450514,\n",
       " 3031.316480652861,\n",
       " 3031.270380372602,\n",
       " 3031.2243070347618,\n",
       " 3031.178260570125,\n",
       " 3031.1322409097725,\n",
       " 3031.086247985076,\n",
       " 3031.040281727703,\n",
       " 3030.9943420696113,\n",
       " 3030.9484289430484,\n",
       " 3030.9025422805503,\n",
       " ...]"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Regression Model\n",
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge regression linear model intercept: 75.74640911421972\n",
      "ridge regression linear model coeff:\n",
      "[ 48.25205691  -5.12955315  -5.71560173  -9.46624393 -15.56213121\n",
      "  -7.54832619]\n",
      "R-squared score (training): 0.808\n",
      "R-squared score (test): 0.815\n",
      "Number of non-zero features: 6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "linridge = Ridge(alpha=25.0).fit(X_train, y_train)\n",
    "\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test, y_test)))\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-352-d71bc87e92fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX_train_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX_test_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlinridge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRidge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         X = check_array(X, copy=self.copy, dtype=FLOAT_DTYPES,\n\u001b[0m\u001b[1;32m    408\u001b[0m                         force_all_finite=\"allow-nan\")\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m             raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n\u001b[0m\u001b[1;32m    651\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                              % (n_samples, array.shape, ensure_min_samples,\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "# Ridge regression with feature normalization\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linridge = Ridge(alpha=20.0).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Wheat dataset')\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test_scaled, y_test)))\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression: effect of alpha regularization parameter\n",
      "\n",
      "Alpha = 0.00\n",
      "num abs(coeff) > 1.0: 5, r-squared training: 0.42, r-squared test: 0.44\n",
      "\n",
      "Alpha = 1.00\n",
      "num abs(coeff) > 1.0: 5, r-squared training: 0.41, r-squared test: 0.44\n",
      "\n",
      "Alpha = 10.00\n",
      "num abs(coeff) > 1.0: 5, r-squared training: 0.32, r-squared test: 0.33\n",
      "\n",
      "Alpha = 20.00\n",
      "num abs(coeff) > 1.0: 5, r-squared training: 0.26, r-squared test: 0.27\n",
      "\n",
      "Alpha = 50.00\n",
      "num abs(coeff) > 1.0: 5, r-squared training: 0.18, r-squared test: 0.18\n",
      "\n",
      "Alpha = 100.00\n",
      "num abs(coeff) > 1.0: 5, r-squared training: 0.11, r-squared test: 0.11\n",
      "\n",
      "Alpha = 1000.00\n",
      "num abs(coeff) > 1.0: 2, r-squared training: 0.02, r-squared test: 0.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ridge regression with regularization parameter: alpha\n",
    "\n",
    "print('Ridge regression: effect of alpha regularization parameter\\n')\n",
    "for this_alpha in [0, 1, 10, 20, 50, 100, 1000]:\n",
    "    linridge = Ridge(alpha = this_alpha).fit(X_train_scaled, y_train)\n",
    "    r2_train = linridge.score(X_train_scaled, y_train)\n",
    "    r2_test = linridge.score(X_test_scaled, y_test)\n",
    "    num_coeff_bigger = np.sum(abs(linridge.coef_) > 1.0)\n",
    "    print('Alpha = {:.2f}\\nnum abs(coeff) > 1.0: {}, \\\n",
    "r-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n",
    "         .format(this_alpha, num_coeff_bigger, r2_train, r2_test))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso regression linear model intercept: 66.20384241687611\n",
      "lasso regression linear model coeff:\n",
      "[34.28143912]\n",
      "Non-zero features: 1\n",
      "R-squared score (training): 0.089\n",
      "R-squared score (test): 0.084\n",
      "\n",
      "Features with non-zero weight (sorted by absolute magnitude):\n",
      "\tHP, 34.281\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linlasso = Lasso(alpha=1, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('lasso regression linear model intercept: {}'\n",
    "     .format(linlasso.intercept_))\n",
    "print('lasso regression linear model coeff:\\n{}'\n",
    "     .format(linlasso.coef_))\n",
    "print('Non-zero features: {}'\n",
    "     .format(np.sum(linlasso.coef_ != 0)))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linlasso.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}\\n'\n",
    "     .format(linlasso.score(X_test_scaled, y_test)))\n",
    "print('Features with non-zero weight (sorted by absolute magnitude):')\n",
    "\n",
    "for e in sorted (list(zip(list(X), linlasso.coef_)),\n",
    "                key = lambda e: -abs(e[1])):\n",
    "    if e[1] != 0:\n",
    "        print('\\t{}, {:.3f}'.format(e[0], e[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso regression: effect of alpha regularization\n",
      "parameter on number of features kept in final model\n",
      "\n",
      "Alpha = 0.50\n",
      "Features kept: 1, r-squared training: 0.17, r-squared test: 0.14\n",
      "\n",
      "Alpha = 1.00\n",
      "Features kept: 1, r-squared training: 0.09, r-squared test: 0.08\n",
      "\n",
      "Alpha = 2.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n",
      "Alpha = 3.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n",
      "Alpha = 5.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n",
      "Alpha = 10.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n",
      "Alpha = 20.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n",
      "Alpha = 50.00\n",
      "Features kept: 0, r-squared training: 0.00, r-squared test: -0.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Lasso regression: effect of alpha regularization\\n\\\n",
    "parameter on number of features kept in final model\\n')\n",
    "\n",
    "for alpha in [0.5, 1, 2, 3, 5, 10, 20, 50]:\n",
    "    linlasso = Lasso(alpha, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "    r2_train = linlasso.score(X_train_scaled, y_train)\n",
    "    r2_test = linlasso.score(X_test_scaled, y_test)\n",
    "    \n",
    "    print('Alpha = {:.2f}\\nFeatures kept: {}, r-squared training: {:.2f}, \\\n",
    "r-squared test: {:.2f}\\n'\n",
    "         .format(alpha, np.sum(linlasso.coef_ != 0), r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import KFold # import k-fold validation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLYNOMIAL DEGREE-2 W/ LINEAR REGRESSION\n",
      "(poly deg 2) linear model coeff (w):\n",
      "[ 0.00000000e+00  7.83632395e-01  5.81316973e-01  8.36628758e-02\n",
      " -3.03589224e-01  2.17935028e-01 -2.64092610e-03  4.12878549e-03\n",
      " -2.05705057e-03 -1.36610177e-03 -5.91249232e-06 -3.96493171e-03\n",
      " -2.46874919e-03  2.19326197e-03  4.71140219e-03  5.30018156e-03\n",
      " -4.63437500e-03 -2.32196045e-03  2.37022152e-03 -8.05809773e-04\n",
      " -6.17839096e-04]\n",
      "(poly deg 2) linear model intercept (b): -6.663\n",
      "(poly deg 2) R-squared score (training): 0.514\n",
      "(poly deg 2) R-squared score (test): 0.477\n",
      "(poly deg 2) RMSE score (train): 20.939\n",
      "(poly deg 2) RMSE score (test): 21.262\n",
      "\n",
      "POLYNOMIAL DEGREE-3 W/ LINEAR REGRESSION\n",
      "(poly deg 3) linear model coeff (w):\n",
      "[ 1.09582330e-10  9.63693242e-01 -1.68716025e+00  2.32220783e+00\n",
      " -1.47109972e+00 -7.42478680e-02 -9.34907237e-03  1.87006590e-02\n",
      " -2.78074030e-02  1.70931737e-02  4.16698973e-03  1.27650489e-02\n",
      " -5.55729820e-03  1.19546395e-02  1.94395528e-02 -8.89151315e-03\n",
      " -8.15594151e-04 -5.39787789e-03  2.30581295e-03 -6.37891267e-03\n",
      " -2.27424818e-03  1.09719466e-05 -3.46605342e-05  1.01164047e-04\n",
      "  4.54683442e-07 -2.38523633e-05 -4.23440735e-05  1.91696677e-04\n",
      " -1.53060933e-04 -4.38951706e-05 -2.40531362e-04  1.54108782e-04\n",
      "  2.85242718e-04 -7.13653918e-05 -8.80119423e-05 -7.63471778e-05\n",
      " -4.69359841e-05 -3.44315685e-05  3.61804724e-05 -6.76359005e-05\n",
      "  4.60722453e-05 -2.82363629e-04  1.90551047e-04  1.03285955e-04\n",
      " -9.94710864e-05 -6.23269954e-05  2.38384891e-05  1.66829933e-04\n",
      "  9.79938105e-05  4.91061040e-06 -3.34109916e-04 -1.44307008e-04\n",
      " -2.27800627e-05  9.08953659e-05  2.27827441e-04  1.89342535e-05]\n",
      "(poly deg 3) linear model intercept (b): 19.873\n",
      "(poly deg 3) R-squared score (training): 0.609\n",
      "(poly deg 3) R-squared score (test): -1.122\n",
      "(poly deg 3) RMSE score (train): 18.796\n",
      "(poly deg 3) RMSE score (test): 42.820\n",
      "\n",
      "POLYNOMIAL DEGREE-5 W/ LINEAR REGRESSION\n",
      "(poly deg 5) linear model coeff (w):\n",
      "[ 2.80817175e+03 -7.67431645e+00  3.53508781e+00  2.66288734e+01\n",
      "  3.66818142e+01 -2.00835710e+01 -2.15536924e-02  4.92799198e-01\n",
      "  3.76759674e-01 -5.11396640e-01  2.06149314e-01  1.02829334e-01\n",
      " -2.87443219e-01 -8.66152664e-01  1.06589272e-01 -5.97216748e-01\n",
      " -4.25401258e-01  9.17776323e-02 -3.37565700e-01 -4.58518810e-02\n",
      "  4.39097245e-01  1.63688866e-03 -7.24823623e-03 -1.86092436e-03\n",
      "  4.11333251e-03 -1.92415017e-04 -6.91686293e-03 -5.81807999e-03\n",
      "  1.22808831e-02 -1.65418982e-03  5.52265724e-04 -1.89868912e-03\n",
      " -5.29225647e-03  3.38378940e-03 -7.05089033e-04 -1.05955300e-03\n",
      " -2.03011707e-03 -1.37852717e-03  1.04703567e-02  3.64974667e-03\n",
      "  7.87859937e-03  1.00444448e-02 -8.44189611e-03 -1.01898818e-03\n",
      "  2.35885588e-04 -2.88834126e-04  4.97537647e-03 -6.08704291e-03\n",
      "  6.96644787e-03  9.98325217e-03  5.07435660e-03 -5.44667365e-03\n",
      "  9.43091358e-05  3.35471491e-03 -4.24236557e-03 -2.91354605e-03\n",
      " -1.70644357e-05  5.65695166e-05  4.46820391e-06 -1.32617236e-05\n",
      " -3.40772175e-06  1.01980315e-04 -8.26119227e-05 -7.70069435e-05\n",
      " -3.99653760e-05 -2.20168252e-05  1.24536126e-04  2.45151478e-05\n",
      " -4.31487769e-05 -3.28409601e-05  2.94287818e-05 -2.30506674e-05\n",
      "  2.25280530e-05  7.27764342e-06  3.91652322e-05  2.08142127e-05\n",
      "  1.05757355e-04  1.86024681e-04 -6.22085550e-05 -1.30953816e-04\n",
      "  1.14946773e-05  7.23401556e-05 -1.04375332e-04 -1.36618837e-04\n",
      " -1.39015504e-04  2.27100406e-04 -3.49000532e-05  5.32421571e-05\n",
      " -5.54313627e-05  6.88730153e-05 -1.66297358e-05  9.87042542e-06\n",
      "  5.34633709e-05 -3.37439191e-05  5.64895807e-06 -7.90749837e-05\n",
      "  5.54352155e-06 -3.81092536e-05 -7.59118790e-05 -7.97083346e-05\n",
      " -3.21835388e-05  1.83515800e-07 -5.36969488e-05  1.60032192e-05\n",
      " -1.19269443e-04 -5.77026460e-05  9.19486732e-05  5.12226678e-05\n",
      "  1.93648130e-04 -5.56698530e-05  6.87369038e-06 -3.45548176e-05\n",
      " -3.35646119e-05 -1.66768423e-05  1.89161033e-04  2.65581361e-05\n",
      " -2.83953137e-05 -6.61039284e-05 -2.52372973e-04  9.00796301e-05\n",
      "  2.29027219e-05 -1.27864538e-05  5.31083861e-05 -8.20124689e-05\n",
      "  6.03348378e-05  2.45345520e-06  6.04021153e-08 -2.91984579e-08\n",
      " -1.56104066e-07 -4.55378046e-08  3.49725626e-08 -5.72845449e-07\n",
      "  6.87841821e-07 -1.55999473e-07  1.98899713e-07 -4.35439249e-08\n",
      " -3.56385890e-07  4.19249886e-07  5.11648632e-07 -2.69485630e-07\n",
      " -1.72559155e-07 -7.19265485e-08 -3.35200149e-07  1.08832516e-06\n",
      " -2.45186935e-07  2.78258364e-07 -5.59163676e-07 -2.60742886e-07\n",
      " -2.48189168e-07  2.99646269e-07 -4.45244752e-08  6.90715289e-08\n",
      " -9.67132827e-08  8.97366235e-08  1.08507267e-06 -2.18597733e-06\n",
      "  3.76312288e-07 -8.30386007e-07  1.05540929e-06  4.34158152e-07\n",
      " -1.74188651e-07  3.92904204e-07  5.76573669e-08 -8.14428125e-07\n",
      " -5.29140200e-07  6.96156092e-07 -6.03597586e-07 -8.95918339e-07\n",
      " -6.37654079e-07  2.29582595e-06  2.65212198e-07 -1.51021843e-06\n",
      "  1.40835017e-06  1.17120723e-06 -2.57244917e-07 -1.16584688e-06\n",
      " -1.01530162e-06  9.39564599e-07 -5.79691676e-07 -3.00843558e-07\n",
      "  2.10347609e-07 -5.90901283e-09  1.21548016e-07  2.65694676e-07\n",
      " -4.44313923e-08 -1.05461992e-07 -5.67871368e-08 -5.58576429e-07\n",
      "  1.62310354e-06 -2.27250481e-07  3.61037141e-07  3.74171960e-07\n",
      " -7.77070272e-07  7.04077152e-09 -5.32933750e-07  1.11690268e-07\n",
      " -6.21778611e-08 -2.97650233e-08  2.92635551e-08 -1.25529501e-07\n",
      " -1.75146853e-07 -9.27235618e-07  8.17434434e-07  6.68213747e-07\n",
      "  6.78169158e-07 -2.80449921e-07  6.44585188e-07  4.50012316e-08\n",
      " -1.34283204e-06  1.83208754e-06 -1.47024466e-07  1.01680127e-06\n",
      " -1.61267701e-07 -1.73058216e-06  1.92833836e-07 -6.46719294e-08\n",
      " -2.26684145e-07  6.75944675e-07  5.55630146e-07 -1.90651093e-06\n",
      "  1.01546178e-06 -8.10527448e-07  4.01995205e-07  5.62451142e-07\n",
      " -5.98052346e-07  4.30256719e-10 -3.43544693e-07 -5.52374919e-08\n",
      "  5.87175651e-07  1.04381696e-07 -6.96546381e-08  1.51492376e-07\n",
      " -3.23784901e-08 -2.04684400e-07 -9.71172218e-08  4.51696112e-08\n",
      "  1.79548483e-07  1.31667855e-07 -6.87250472e-07  5.64855780e-08\n",
      "  1.73708856e-07  1.28824384e-07  4.93795492e-07  5.16889389e-07\n",
      " -5.38109483e-07 -1.05741331e-07 -3.31211627e-08 -5.75150505e-09\n",
      " -2.02596021e-07  2.33843242e-07 -9.04279784e-08  2.34385968e-08]\n",
      "(poly deg 5) linear model intercept (b): -3229.763\n",
      "(poly deg 5) R-squared score (training): 0.792\n",
      "(poly deg 5) R-squared score (test): -2495.770\n",
      "(poly deg 5) RMSE score (train): 13.716\n",
      "(poly deg 5) RMSE score (test): 1468.935\n",
      "\n",
      "POLYNOMIAL DEGREE-10 W/ LINEAR REGRESSION\n",
      "(poly deg 10) linear model coeff (w):\n",
      "[ 2.49200575e-12 -1.24200888e-11  1.24983611e-11 ...  4.48849851e-14\n",
      " -1.30415844e-13 -1.00614254e-13]\n",
      "(poly deg 10) linear model intercept (b): 23.903\n",
      "(poly deg 10) R-squared score (training): 1.000\n",
      "(poly deg 10) R-squared score (test): -12398793083561872.000\n",
      "(poly deg 10) RMSE score (train): 0.020\n",
      "(poly deg 10) RMSE score (test): 3273429738.344\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for degree in [2, 3, 5,10]:\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_F1_poly = poly.fit_transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y, random_state=0)\n",
    "    linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "    print('POLYNOMIAL DEGREE-{} W/ LINEAR REGRESSION'.format(degree))\n",
    "    print('(poly deg {}) linear model coeff (w):\\n{}'\n",
    "         .format(degree, linreg.coef_))\n",
    "    print('(poly deg {}) linear model intercept (b): {:.3f}'\n",
    "         .format(degree, linreg.intercept_))\n",
    "    print('(poly deg {}) R-squared score (training): {:.3f}'\n",
    "         .format(degree, linreg.score(X_train, y_train)))\n",
    "    print('(poly deg {}) R-squared score (test): {:.3f}'\n",
    "         .format(degree, linreg.score(X_test, y_test)))\n",
    "    print('(poly deg {}) RMSE score (train): {:.3f}'\n",
    "         .format(degree, sqrt(mean_squared_error(y_train, linreg.predict(X_train)))))\n",
    "    print('(poly deg {}) RMSE score (test): {:.3f}\\n'\n",
    "         .format(degree, sqrt(mean_squared_error(y_test, linreg.predict(X_test)))))\n",
    "\n",
    "plot_learning_curves(linreg,  X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLYNOMIAL DEGREE-2 W/ RIDGE REGRESSION\n",
      "(poly deg 2) linear model coeff (w):\n",
      "[ 0.00000000e+00  7.83561256e-01  5.81248959e-01  8.36606865e-02\n",
      " -3.03533540e-01  2.17897801e-01 -2.64075722e-03  4.12917773e-03\n",
      " -2.05708348e-03 -1.36616736e-03 -5.60359635e-06 -3.96475273e-03\n",
      " -2.46871137e-03  2.19313880e-03  4.71163080e-03  5.30016917e-03\n",
      " -4.63433789e-03 -2.32194092e-03  2.37009827e-03 -8.06078647e-04\n",
      " -6.17730005e-04]\n",
      "(poly deg 2) linear model intercept (b): -6.659\n",
      "(poly deg 2) R-squared score (training): 0.514\n",
      "(poly deg 2) R-squared score (test): 0.477\n",
      "(poly deg 2) RMSE score (train): 20.939\n",
      "(poly deg 2) RMSE score (test): 21.262\n",
      "\n",
      "POLYNOMIAL DEGREE-3 W/ RIDGE REGRESSION\n",
      "(poly deg 3) linear model coeff (w):\n",
      "[ 0.00000000e+00  9.62069563e-01 -1.68358847e+00  2.31876270e+00\n",
      " -1.46747506e+00 -7.31761141e-02 -9.33646847e-03  1.86938416e-02\n",
      " -2.77955470e-02  1.70822169e-02  4.19307280e-03  1.27382279e-02\n",
      " -5.53520793e-03  1.19199550e-02  1.94080912e-02 -8.88527363e-03\n",
      " -8.00337144e-04 -5.36569520e-03  2.28899218e-03 -6.41651992e-03\n",
      " -2.28464225e-03  1.09450454e-05 -3.46712910e-05  1.01175577e-04\n",
      "  4.60659942e-07 -2.39465657e-05 -4.22856630e-05  1.91555001e-04\n",
      " -1.52904925e-04 -4.39060630e-05 -2.40488300e-04  1.54002169e-04\n",
      "  2.85199945e-04 -7.13331527e-05 -8.79852738e-05 -7.64079188e-05\n",
      " -4.68869056e-05 -3.44658935e-05  3.62910481e-05 -6.75496007e-05\n",
      "  4.60506413e-05 -2.82316725e-04  1.90478717e-04  1.03239802e-04\n",
      " -9.93243463e-05 -6.22361387e-05  2.38197342e-05  1.66805199e-04\n",
      "  9.79920579e-05  4.93306394e-06 -3.34246577e-04 -1.44394795e-04\n",
      " -2.27501270e-05  9.10029938e-05  2.27954872e-04  1.89586572e-05]\n",
      "(poly deg 3) linear model intercept (b): 19.808\n",
      "(poly deg 3) R-squared score (training): 0.609\n",
      "(poly deg 3) R-squared score (test): -1.121\n",
      "(poly deg 3) RMSE score (train): 18.796\n",
      "(poly deg 3) RMSE score (test): 42.812\n",
      "\n",
      "POLYNOMIAL DEGREE-5 W/ RIDGE REGRESSION\n",
      "(poly deg 5) linear model coeff (w):\n",
      "[ 0.00000000e+00 -1.58495107e+00  4.63260318e+00  1.93542423e+01\n",
      "  2.16510859e+01 -1.28797807e+01 -1.26766698e-01  3.34272404e-01\n",
      "  4.46082208e-01 -3.88849402e-01 -9.33766889e-03 -1.26661523e-02\n",
      " -5.59569257e-02 -5.01149549e-01 -1.45898769e-01 -5.71348395e-01\n",
      " -3.82618428e-01  1.50323714e-01 -1.91978281e-01  7.18044283e-02\n",
      "  3.47331751e-01  2.23053148e-03 -4.77254402e-03 -2.75645051e-03\n",
      "  3.88030610e-03  1.76120317e-03 -5.68326478e-03 -5.55282995e-03\n",
      "  8.19347465e-03  1.09361934e-03  6.37856867e-04 -2.04577777e-03\n",
      " -6.54540851e-03  2.35273093e-03  4.46351560e-04  6.85755435e-04\n",
      " -6.20115525e-04 -3.90495704e-03  8.40083433e-03  7.73358740e-03\n",
      "  6.96962069e-03  6.71611281e-03 -9.04837188e-03 -2.42162689e-03\n",
      " -1.10416923e-03  8.65500610e-04  4.99270705e-03 -5.93010294e-03\n",
      "  6.41324174e-03  9.96118410e-03  5.81269558e-03 -5.58879885e-03\n",
      " -4.56803614e-04  7.78418344e-04 -4.54697659e-03 -2.21921108e-03\n",
      " -1.83425546e-05  4.76361611e-05  5.94811692e-06 -1.44695104e-05\n",
      " -8.56347276e-06  8.17152290e-05 -7.63700872e-05 -6.43629099e-05\n",
      " -6.03279668e-05 -2.17470652e-05  1.21435918e-04  4.24633345e-05\n",
      " -3.71514284e-05 -4.96953548e-05  1.70232185e-05 -2.21703902e-05\n",
      "  1.53630038e-05  2.99508060e-05  3.44796353e-05  2.25325771e-05\n",
      "  1.01071221e-04  1.76336741e-04 -4.10086473e-05 -1.17279145e-04\n",
      " -2.67288526e-06  7.36000864e-05 -1.07796456e-04 -1.37829364e-04\n",
      " -1.23005224e-04  2.03630455e-04 -1.72050361e-05  4.95180164e-05\n",
      " -5.22878332e-05  7.04628952e-05 -2.83184218e-05  3.91649879e-06\n",
      "  6.26284907e-05 -3.75747850e-05 -1.79976328e-05 -6.61569128e-05\n",
      "  3.50972335e-05 -3.76140715e-05 -6.36393542e-05 -9.50257198e-05\n",
      " -4.46677519e-05 -4.09270460e-07 -5.52249356e-05  1.90452252e-05\n",
      " -9.80410865e-05 -4.88791804e-05  9.50698675e-05  3.70554186e-05\n",
      "  2.06900121e-04 -4.77679182e-05  4.95436466e-06 -3.18524505e-05\n",
      " -4.64473143e-05 -1.74787147e-05  1.99669581e-04  4.46280269e-05\n",
      " -2.86745177e-05 -7.84495416e-05 -2.60534769e-04  7.58946999e-05\n",
      "  2.27037148e-05 -7.54592065e-06  7.29394118e-05 -7.77301023e-05\n",
      "  6.19004414e-05 -1.17642906e-07  5.81759638e-08 -3.45788211e-08\n",
      " -1.28195982e-07 -1.94604355e-08  2.74526012e-08 -4.52544424e-07\n",
      "  5.62243511e-07 -1.95172203e-07  2.39421001e-07 -2.23136447e-08\n",
      " -3.07137815e-07  2.98851944e-07  4.10959765e-07 -1.55311145e-07\n",
      " -1.27850769e-07 -9.14183716e-08 -2.46630621e-07  9.14478066e-07\n",
      " -2.02103809e-07  3.28782920e-07 -4.52741634e-07 -3.43545210e-07\n",
      " -9.60150647e-08  2.18210986e-07  9.36463296e-08 -5.06661830e-09\n",
      " -2.84835705e-08  1.08188502e-07  7.64763861e-07 -1.81126532e-06\n",
      "  2.80586430e-07 -6.84448844e-07  9.96029359e-07  2.66560817e-07\n",
      " -1.26988030e-07  3.88933030e-07  7.42276870e-08 -7.56889588e-07\n",
      " -5.83721361e-07  5.95963205e-07 -5.88920648e-07 -7.94457391e-07\n",
      " -7.01726200e-07  2.27291296e-06  2.61384990e-07 -1.43372881e-06\n",
      "  1.09805822e-06  1.29805402e-06 -4.56715614e-08 -1.17840532e-06\n",
      " -1.04707301e-06  6.76824957e-07 -4.71112359e-07 -4.40704557e-07\n",
      "  2.43841311e-07  7.77559926e-09  1.53803831e-07  2.61423553e-07\n",
      "  1.34345665e-08 -1.90476727e-07 -8.08831455e-08 -4.03444678e-07\n",
      "  1.31487405e-06 -3.33828819e-08  2.93696133e-07  3.24525433e-07\n",
      " -7.70215720e-07  1.37032037e-07 -5.00602904e-07  1.31801494e-07\n",
      " -4.99951454e-08 -4.68236930e-08  2.39468588e-08 -8.25121771e-08\n",
      " -1.51624162e-07 -9.65455805e-07  7.00046688e-07  6.52464303e-07\n",
      "  8.19853272e-07 -1.67120107e-07  6.27211786e-07 -1.27512411e-08\n",
      " -1.38503765e-06  1.62941904e-06  6.08263508e-08  1.07801014e-06\n",
      " -7.13242620e-08 -1.86838851e-06  1.40078497e-07 -1.27902784e-07\n",
      " -2.60933565e-07  7.07509532e-07  6.32875698e-07 -1.66262659e-06\n",
      "  8.84614346e-07 -8.99966940e-07  1.95748117e-07  5.73389723e-07\n",
      " -7.62303901e-07  6.94478952e-08 -2.09919628e-07 -1.17118695e-07\n",
      "  6.96910407e-07  1.31381736e-07 -6.12759510e-08  1.40236723e-07\n",
      "  1.37928843e-08 -1.96888498e-07 -9.12156324e-08 -8.57999311e-08\n",
      "  1.94013267e-07 -1.83969009e-08 -5.92088426e-07  1.37541467e-07\n",
      "  1.69555450e-07  2.09466068e-07  5.88214164e-07  4.66294584e-07\n",
      " -4.82323239e-07 -1.19078241e-07 -5.92587980e-08 -6.96252725e-08\n",
      " -2.12402196e-07  1.35560142e-07 -7.51561088e-08  2.56451267e-08]\n",
      "(poly deg 5) linear model intercept (b): -322.214\n",
      "(poly deg 5) R-squared score (training): 0.792\n",
      "(poly deg 5) R-squared score (test): -2601.783\n",
      "(poly deg 5) RMSE score (train): 13.713\n",
      "(poly deg 5) RMSE score (test): 1499.796\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=3.04438e-25): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLYNOMIAL DEGREE-10 W/ RIDGE REGRESSION\n",
      "(poly deg 10) linear model coeff (w):\n",
      "[ 0.00000000e+00  7.68787830e-30  8.38795854e-30 ...  2.72464543e-16\n",
      " -3.14148365e-17 -1.09156796e-17]\n",
      "(poly deg 10) linear model intercept (b): 55.069\n",
      "(poly deg 10) R-squared score (training): 0.846\n",
      "(poly deg 10) R-squared score (test): -11380044339.484\n",
      "(poly deg 10) RMSE score (train): 11.782\n",
      "(poly deg 10) RMSE score (test): 3136066.725\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/2.2.5/libexec/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n"
     ]
    }
   ],
   "source": [
    "for degree in [2, 3, 5, 10]:\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_F1_poly = poly.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y, random_state=0)\n",
    "    linreg = Ridge().fit(X_train, y_train)\n",
    "\n",
    "    print('POLYNOMIAL DEGREE-{} W/ RIDGE REGRESSION'.format(degree))\n",
    "    print('(poly deg {}) linear model coeff (w):\\n{}'\n",
    "         .format(degree, linreg.coef_))\n",
    "    print('(poly deg {}) linear model intercept (b): {:.3f}'\n",
    "         .format(degree, linreg.intercept_))\n",
    "    print('(poly deg {}) R-squared score (training): {:.3f}'\n",
    "         .format(degree, linreg.score(X_train, y_train)))\n",
    "    print('(poly deg {}) R-squared score (test): {:.3f}'\n",
    "         .format(degree, linreg.score(X_test, y_test)))\n",
    "    print('(poly deg {}) RMSE score (train): {:.3f}'\n",
    "         .format(degree, sqrt(mean_squared_error(y_train, linreg.predict(X_train)))))\n",
    "    print('(poly deg {}) RMSE score (test): {:.3f}\\n'\n",
    "         .format(degree, sqrt(mean_squared_error(y_test, linreg.predict(X_test)))))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DAP3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
